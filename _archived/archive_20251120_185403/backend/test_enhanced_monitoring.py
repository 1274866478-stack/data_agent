#!/usr/bin/env python3
"""
å¢å¼ºæ€§èƒ½ç›‘æ§æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°å¢çš„æŸ¥è¯¢æ€§èƒ½ç›‘æ§åŠŸèƒ½
"""

import asyncio
import logging
import sys
import os
import time
import random
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_enhanced_monitoring_features():
    """æµ‹è¯•å¢å¼ºçš„ç›‘æ§åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•å¢å¼ºç›‘æ§åŠŸèƒ½ ===")

    try:
        from services.query_performance_monitor import QueryPerformanceMonitor, QueryMetrics

        # åˆ›å»ºå¢å¼ºç›‘æ§å™¨
        monitor = QueryPerformanceMonitor(max_history=1000, slow_query_threshold=2.0)
        monitor.start_monitoring()

        logger.info("âœ… å¢å¼ºç›‘æ§å™¨åˆ›å»ºæˆåŠŸ")

        # æ¨¡æ‹Ÿä¸€äº›æŸ¥è¯¢æ•°æ®
        query_types = ["SELECT", "INSERT", "UPDATE", "JOIN", "AGGREGATE"]
        tenant_ids = ["tenant1", "tenant2", "tenant3"]

        for i in range(50):
            query_id = f"test_query_{i}"
            tenant_id = random.choice(tenant_ids)
            query_type = random.choice(query_types)

            # æ¨¡æ‹ŸæŸ¥è¯¢æ‰§è¡Œ
            async with monitor.monitor_query(query_id, tenant_id, query_type) as metrics:
                # æ¨¡æ‹Ÿå„é˜¶æ®µè€—æ—¶
                await asyncio.sleep(random.uniform(0.1, 1.5))  # æ¨¡æ‹Ÿæ€»æ‰§è¡Œæ—¶é—´

                # è®¾ç½®å„ç§æŒ‡æ ‡
                metrics.sql_generation_time = random.uniform(0.05, 0.3)
                metrics.sql_validation_time = random.uniform(0.02, 0.1)
                metrics.result_processing_time = random.uniform(0.01, 0.2)
                metrics.row_count = random.randint(1, 1000)

                # æ¨¡æ‹Ÿä¸€äº›ç¼“å­˜å‘½ä¸­å’Œé”™è¯¯
                if random.random() < 0.3:  # 30% ç¼“å­˜å‘½ä¸­
                    metrics.cache_hit = True

                if random.random() < 0.05:  # 5% é”™è¯¯ç‡
                    metrics.error = True
                    metrics.error_message = "Simulated error for testing"

        # æµ‹è¯•æ–°å¢çš„åŠŸèƒ½
        logger.info("\nğŸ“Š æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åˆ†æ...")
        trends = monitor.get_performance_trends(hours=24)
        logger.info(f"âœ… è¶‹åŠ¿åˆ†æ: {trends.get('trend_direction', 'unknown')} ({trends.get('total_queries', 0)} æŸ¥è¯¢)")

        logger.info("\nğŸ” æµ‹è¯•æŸ¥è¯¢æ¨¡å¼åˆ†æ...")
        patterns = monitor.get_query_patterns_analysis()
        cache_hit_rate = patterns.get('cache_analysis', {}).get('hit_rate_percent', 0)
        logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
        logger.info(f"âœ… ä¼˜åŒ–å»ºè®®æ•°é‡: {len(patterns.get('optimization_suggestions', []))}")

        logger.info("\ï¿½ï¿½ æµ‹è¯•ç§Ÿæˆ·æ€§èƒ½å¯¹æ¯”...")
        tenant_comparison = monitor.get_tenant_performance_comparison()
        logger.info(f"âœ… åˆ†æäº† {tenant_comparison.get('total_tenants', 0)} ä¸ªç§Ÿæˆ·")

        logger.info("\ğŸ“„ æµ‹è¯•æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ...")
        report = await monitor.generate_performance_report(hours=1)
        report_lines = report.split('\n')
        logger.info(f"âœ… ç”Ÿæˆäº† {len(report_lines)} è¡Œçš„æŠ¥å‘Š")

        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        logger.info("âœ… ç›‘æ§å™¨å·²åœæ­¢")

        return True

    except Exception as e:
        logger.error(f"å¢å¼ºç›‘æ§åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_performance_trends():
    """ä¸“é—¨æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åŠŸèƒ½ ===")

    try:
        from services.query_performance_monitor import QueryPerformanceMonitor

        monitor = QueryPerformanceMonitor(slow_query_threshold=1.0)

        # æ¨¡æ‹Ÿä¸åŒæ—¶é—´æ®µçš„æŸ¥è¯¢æ•°æ®
        now = datetime.utcnow()

        for i in range(100):
            # åˆ›å»ºä¸åŒæ—¶é—´çš„æŸ¥è¯¢
            query_time = now - timedelta(hours=random.randint(0, 23))
            query_type = random.choice(["SELECT", "JOIN", "AGGREGATE"])

            # æ‰‹åŠ¨æ·»åŠ æŸ¥è¯¢æŒ‡æ ‡ï¼ˆæ¨¡æ‹Ÿå†å²æ•°æ®ï¼‰
            metrics = QueryMetrics(
                query_id=f"trend_test_{i}",
                tenant_id="trend_tenant",
                query_type=query_type,
                query_hash="test_hash",
                execution_time=random.uniform(0.1, 3.0),
                sql_generation_time=0.1,
                sql_validation_time=0.05,
                result_processing_time=0.02,
                total_time=random.uniform(0.2, 3.5),
                row_count=random.randint(1, 500),
                cache_hit=random.random() < 0.4,
                error=random.random() < 0.1,
                timestamp=query_time
            )

            monitor._record_query_metrics(metrics)

        # æµ‹è¯•è¶‹åŠ¿åˆ†æ
        trends = monitor.get_performance_trends(hours=24)

        logger.info(f"âœ… åˆ†ææ—¶é—´æ®µ: {trends.get('period_hours', 0)} å°æ—¶")
        logger.info(f"âœ… æ€»æŸ¥è¯¢æ•°: {trends.get('total_queries', 0)}")
        logger.info(f"âœ… è¶‹åŠ¿æ–¹å‘: {trends.get('trend_direction', 'unknown')}")
        logger.info(f"âœ… æ€§èƒ½å˜åŒ–: {trends.get('performance_change_percent', 0):.1f}%")

        if 'hourly_breakdown' in trends:
            hourly_count = len(trends['hourly_breakdown'])
            logger.info(f"âœ… å°æ—¶æ•°æ®ç‚¹: {hourly_count}")

        return True

    except Exception as e:
        logger.error(f"æ€§èƒ½è¶‹åŠ¿æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_tenant_performance_scoring():
    """æµ‹è¯•ç§Ÿæˆ·æ€§èƒ½è¯„åˆ†åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•ç§Ÿæˆ·æ€§èƒ½è¯„åˆ† ===")

    try:
        from services.query_performance_monitor import QueryPerformanceMonitor, QueryMetrics

        monitor = QueryPerformanceMonitor()

        # æ¨¡æ‹Ÿä¸åŒç§Ÿæˆ·çš„æ€§èƒ½æ•°æ®
        tenant_scenarios = [
            ("excellent_tenant", 0.5, 90, 1),   # å¿«é€Ÿã€é«˜ç¼“å­˜å‘½ä¸­ã€ä½é”™è¯¯ç‡
            ("good_tenant", 1.0, 70, 3),       # ä¸­ç­‰é€Ÿåº¦ã€ä¸­ç­‰ç¼“å­˜ã€ä½é”™è¯¯ç‡
            ("poor_tenant", 3.0, 30, 15),      # æ…¢é€Ÿã€ä½ç¼“å­˜ã€é«˜é”™è¯¯ç‡
            ("average_tenant", 1.5, 50, 8),     # å¹³å‡æ€§èƒ½
        ]

        for tenant_id, avg_time, cache_hit_rate, error_rate in tenant_scenarios:
            for i in range(20):  # æ¯ä¸ªç§Ÿæˆ·20ä¸ªæŸ¥è¯¢
                metrics = QueryMetrics(
                    query_id=f"{tenant_id}_query_{i}",
                    tenant_id=tenant_id,
                    query_type="SELECT",
                    query_hash=f"hash_{tenant_id}",
                    execution_time=avg_time * random.uniform(0.8, 1.2),
                    sql_generation_time=0.1,
                    sql_validation_time=0.05,
                    result_processing_time=0.02,
                    total_time=avg_time * random.uniform(0.8, 1.2),
                    row_count=random.randint(10, 100),
                    cache_hit=random.random() < (cache_hit_rate / 100),
                    error=random.random() < (error_rate / 100)
                )
                monitor._record_query_metrics(metrics)

        # æµ‹è¯•ç§Ÿæˆ·æ€§èƒ½å¯¹æ¯”
        comparison = monitor.get_tenant_performance_comparison()

        logger.info(f"âœ… åˆ†æäº† {comparison.get('total_tenants', 0)} ä¸ªç§Ÿæˆ·")
        logger.info(f"âœ… é¡¶çº§è¡¨ç°è€…: {len(comparison.get('top_performers', {}))}")

        # æ˜¾ç¤ºæ’å
        top_tenants = comparison.get('top_performers', {})
        for i, (tenant, stats) in enumerate(top_tenants.items(), 1):
            score = stats.get('performance_score', 0)
            avg_time = stats.get('avg_execution_time', 0)
            cache_rate = stats.get('cache_hit_rate', 0)
            error_rate = stats.get('error_rate', 0)
            logger.info(f"  {i}. {tenant}: è¯„åˆ† {score:.1f} (æ—¶é—´ {avg_time:.3f}s, ç¼“å­˜ {cache_rate:.1f}%, é”™è¯¯ {error_rate:.1f}%)")

        # æµ‹è¯•æ€§èƒ½åˆ†å¸ƒ
        distribution = comparison.get('performance_distribution', {})
        if distribution:
            tiers = distribution.get('performance_tiers', {})
            logger.info(f"âœ… æ€§èƒ½ç­‰çº§åˆ†å¸ƒ: ä¼˜ç§€ {tiers.get('excellent', 0)}, è‰¯å¥½ {tiers.get('good', 0)}, ä¸€èˆ¬ {tiers.get('fair', 0)}, è¾ƒå·® {tiers.get('poor', 0)}")

        return True

    except Exception as e:
        logger.error(f"ç§Ÿæˆ·æ€§èƒ½è¯„åˆ†æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_optimization_suggestions():
    """æµ‹è¯•ä¼˜åŒ–å»ºè®®ç”Ÿæˆ"""
    logger.info("=== æµ‹è¯•ä¼˜åŒ–å»ºè®®ç”Ÿæˆ ===")

    try:
        from services.query_performance_monitor import QueryPerformanceMonitor, QueryMetrics

        monitor = QueryPerformanceMonitor(slow_query_threshold=1.0)

        # æ¨¡æ‹Ÿéœ€è¦ä¼˜åŒ–çš„åœºæ™¯
        optimization_scenarios = [
            # åœºæ™¯1: ä½ç¼“å­˜å‘½ä¸­ç‡
            *[QueryMetrics(
                query_id=f"cache_miss_{i}",
                tenant_id="cache_test_tenant",
                query_type="SELECT",
                query_hash="cache_test",
                execution_time=0.5,
                sql_generation_time=0.1,
                sql_validation_time=0.05,
                result_processing_time=0.02,
                total_time=0.67,
                row_count=50,
                cache_hit=False,  # å…¨éƒ¨ç¼“å­˜æœªå‘½ä¸­
                error=False
            ) for i in range(30)],

            # åœºæ™¯2: æ…¢æŸ¥è¯¢
            *[QueryMetrics(
                query_id=f"slow_query_{i}",
                tenant_id="slow_test_tenant",
                query_type="JOIN",
                query_hash="slow_test",
                execution_time=2.5,
                sql_generation_time=0.1,
                sql_validation_time=0.05,
                result_processing_time=0.02,
                total_time=2.67,
                row_count=1000,
                cache_hit=False,
                error=False
            ) for i in range(15)],

            # åœºæ™¯3: é«˜é”™è¯¯ç‡
            *[QueryMetrics(
                query_id=f"error_query_{i}",
                tenant_id="error_test_tenant",
                query_type="SELECT",
                query_hash="error_test",
                execution_time=0.3,
                sql_generation_time=0.1,
                sql_validation_time=0.05,
                result_processing_time=0.02,
                total_time=0.47,
                row_count=10,
                cache_hit=False,
                error=True,
                error_message="Connection timeout"
            ) for i in range(8)],
        ]

        # è®°å½•æ‰€æœ‰åœºæ™¯
        for metrics in optimization_scenarios:
            monitor._record_query_metrics(metrics)

        # æµ‹è¯•æŸ¥è¯¢æ¨¡å¼åˆ†æå’Œä¼˜åŒ–å»ºè®®
        patterns = monitor.get_query_patterns_analysis()
        suggestions = patterns.get('optimization_suggestions', [])

        logger.info(f"âœ… ç”Ÿæˆäº† {len(suggestions)} æ¡ä¼˜åŒ–å»ºè®®")
        for i, suggestion in enumerate(suggestions, 1):
            logger.info(f"  {i}. {suggestion}")

        # æµ‹è¯•é”™è¯¯åˆ†ç±»
        error_patterns = patterns.get('error_patterns', {})
        if error_patterns:
            logger.info("âœ… é”™è¯¯æ¨¡å¼åˆ†æ:")
            for error_type, count in error_patterns.items():
                logger.info(f"  - {error_type}: {count} æ¬¡")

        # æµ‹è¯•æ€§èƒ½å››åˆ†ä½æ•°
        quartiles = patterns.get('performance_quartiles', {})
        if quartiles:
            logger.info("âœ… æ€§èƒ½å››åˆ†ä½æ•°:")
            logger.info(f"  - æœ€å°å€¼: {quartiles.get('min', 0):.3f}s")
            logger.info(f"  - ç¬¬ä¸€å››åˆ†ä½: {quartiles.get('q1', 0):.3f}s")
            logger.info(f"  - ä¸­ä½æ•°: {quartiles.get('median', 0):.3f}s")
            logger.info(f"  - ç¬¬ä¸‰å››åˆ†ä½: {quartiles.get('q3', 0):.3f}s")
            logger.info(f"  - æœ€å¤§å€¼: {quartiles.get('max', 0):.3f}s")

        return True

    except Exception as e:
        logger.error(f"ä¼˜åŒ–å»ºè®®æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_performance_report_generation():
    """æµ‹è¯•æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ"""
    logger.info("=== æµ‹è¯•æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ ===")

    try:
        from services.query_performance_monitor import QueryPerformanceMonitor

        monitor = QueryPerformanceMonitor()

        # æ¨¡æ‹Ÿä¸€äº›æ•°æ®ç”¨äºæŠ¥å‘Š
        for i in range(40):
            async with monitor.monitor_query(f"report_test_{i}", "report_tenant", "SELECT") as metrics:
                await asyncio.sleep(random.uniform(0.1, 0.8))
                metrics.row_count = random.randint(1, 200)
                metrics.cache_hit = random.random() < 0.4

        # ç”ŸæˆæŠ¥å‘Š
        report = await monitor.generate_performance_report(hours=1)

        # éªŒè¯æŠ¥å‘Šå†…å®¹
        if "æŸ¥è¯¢æ€§èƒ½ç›‘æ§æŠ¥å‘Š" in report:
            logger.info("âœ… æŠ¥å‘Šæ ‡é¢˜æ­£ç¡®")
        if "æ•´ä½“ç»Ÿè®¡" in report:
            logger.info("âœ… åŒ…å«æ•´ä½“ç»Ÿè®¡")
        if "æ€§èƒ½è¶‹åŠ¿" in report:
            logger.info("âœ… åŒ…å«æ€§èƒ½è¶‹åŠ¿")
        if "ä¼˜åŒ–å»ºè®®" in report:
            logger.info("âœ… åŒ…å«ä¼˜åŒ–å»ºè®®")
        if "ç§Ÿæˆ·æ€§èƒ½æ’å" in report:
            logger.info("âœ… åŒ…å«ç§Ÿæˆ·æ’å")

        lines = report.split('\n')
        logger.info(f"âœ… æŠ¥å‘Šæ€»è¡Œæ•°: {len(lines)}")

        # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        for line in lines[:10]:  # æ˜¾ç¤ºå‰10è¡Œ
            if line.strip():
                logger.info(f"  {line}")

        return True

    except Exception as e:
        logger.error(f"æ€§èƒ½æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("Data Agent V4 - å¢å¼ºæ€§èƒ½ç›‘æ§æµ‹è¯•")
    logger.info("=" * 50)

    success = True

    # åŸºç¡€å¢å¼ºåŠŸèƒ½æµ‹è¯•
    success &= await test_enhanced_monitoring_features()

    # æ€§èƒ½è¶‹åŠ¿æµ‹è¯•
    success &= await test_performance_trends()

    # ç§Ÿæˆ·æ€§èƒ½è¯„åˆ†æµ‹è¯•
    success &= await test_tenant_performance_scoring()

    # ä¼˜åŒ–å»ºè®®æµ‹è¯•
    success &= await test_optimization_suggestions()

    # æŠ¥å‘Šç”Ÿæˆæµ‹è¯•
    success &= await test_performance_report_generation()

    if success:
        logger.info("\nğŸ‰ æ‰€æœ‰å¢å¼ºç›‘æ§åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        logger.info("\næ–°å¢åŠŸèƒ½:")
        logger.info("1. âœ… æ€§èƒ½è¶‹åŠ¿åˆ†æ - æŒ‰å°æ—¶åˆ†ææŸ¥è¯¢æ€§èƒ½å˜åŒ–")
        logger.info("2. âœ… æŸ¥è¯¢æ¨¡å¼åˆ†æ - åˆ†ææŸ¥è¯¢ç±»å‹åˆ†å¸ƒå’Œç¼“å­˜æ•ˆæœ")
        logger.info("3. âœ… ç§Ÿæˆ·æ€§èƒ½è¯„åˆ† - ç»¼åˆè¯„åˆ†å’Œæ’åç³»ç»Ÿ")
        logger.info("4. âœ… æ™ºèƒ½ä¼˜åŒ–å»ºè®® - åŸºäºæ•°æ®çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        logger.info("5. âœ… æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ - è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š")
        logger.info("6. âœ… é”™è¯¯åˆ†ç±»åˆ†æ - æŒ‰é”™è¯¯ç±»å‹åˆ†ç±»å’Œç»Ÿè®¡")
        logger.info("7. âœ… æ€§èƒ½å››åˆ†ä½æ•°åˆ†æ - è¯¦ç»†çš„æ€§èƒ½åˆ†å¸ƒç»Ÿè®¡")
        logger.info("\nä½¿ç”¨æ–¹æ³•:")
        logger.info("- è·å–è¶‹åŠ¿: monitor.get_performance_trends(hours=24)")
        logger.info("- åˆ†ææ¨¡å¼: monitor.get_query_patterns_analysis()")
        logger.info("- ç§Ÿæˆ·å¯¹æ¯”: monitor.get_tenant_performance_comparison()")
        logger.info("- ç”ŸæˆæŠ¥å‘Š: await monitor.generate_performance_report(hours=24)")
    else:
        logger.error("\nâŒ éƒ¨åˆ†å¢å¼ºç›‘æ§åŠŸèƒ½æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())