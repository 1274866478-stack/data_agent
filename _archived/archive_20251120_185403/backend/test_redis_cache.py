#!/usr/bin/env python3
"""
Redisç¼“å­˜é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•Redisåˆ†å¸ƒå¼ç¼“å­˜åŠŸèƒ½
"""

import asyncio
import logging
import sys
import os
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_cache_service(cache_type: str = "memory"):
    """æµ‹è¯•ç¼“å­˜æœåŠ¡"""
    try:
        from services.cache_service import CacheFactory, TenantCacheKeyGenerator

        logger.info(f"=== æµ‹è¯• {cache_type} ç¼“å­˜ ===")

        # åˆ›å»ºç¼“å­˜å®ä¾‹
        cache = CacheFactory.create_cache(cache_type)
        logger.info(f"âœ… {cache_type} ç¼“å­˜åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        test_data = {
            "query": "SELECT * FROM products WHERE price > 100",
            "result": [{"id": 1, "name": "iPhone", "price": 999}],
            "timestamp": datetime.utcnow().isoformat()
        }

        # è®¾ç½®ç¼“å­˜
        test_key = f"test:query_{cache_type}"
        await cache.set(test_key, test_data, ttl=60)
        logger.info(f"âœ… ç¼“å­˜è®¾ç½®æˆåŠŸ: {test_key}")

        # è·å–ç¼“å­˜
        retrieved_data = await cache.get(test_key)
        if retrieved_data:
            logger.info(f"âœ… ç¼“å­˜è·å–æˆåŠŸ: {type(retrieved_data)}")
        else:
            logger.error("âŒ ç¼“å­˜è·å–å¤±è´¥")
            return False

        # æ£€æŸ¥å­˜åœ¨æ€§
        exists = await cache.exists(test_key)
        logger.info(f"âœ… ç¼“å­˜å­˜åœ¨æ€§æ£€æŸ¥: {exists}")

        # ç§Ÿæˆ·ç¼“å­˜æµ‹è¯•
        tenant_id = "test_tenant"
        connection_id = 1
        query = "SELECT COUNT(*) FROM users"

        # ç”Ÿæˆç§Ÿæˆ·ç¼“å­˜é”®
        query_key = TenantCacheKeyGenerator.query_result(tenant_id, connection_id, query)
        await cache.set(query_key, {"count": 100}, ttl=300)
        logger.info(f"âœ… ç§Ÿæˆ·ç¼“å­˜è®¾ç½®æˆåŠŸ: {query_key}")

        # è·å–ç§Ÿæˆ·ç¼“å­˜
        tenant_result = await cache.get(query_key)
        if tenant_result:
            logger.info(f"âœ… ç§Ÿæˆ·ç¼“å­˜è·å–æˆåŠŸ: {tenant_result}")

        # æµ‹è¯•ç¼“å­˜å¤§å°
        cache_size = await cache.get_size()
        logger.info(f"âœ… ç¼“å­˜å¤§å°: {cache_size}")

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await cache.clear_tenant_cache(tenant_id)
        await cache.delete(test_key)
        logger.info("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")

        # Redisç‰¹æœ‰æµ‹è¯•
        if cache_type == "redis" and hasattr(cache, 'health_check'):
            health = await cache.health_check()
            logger.info(f"âœ… Rediså¥åº·æ£€æŸ¥: {health}")

            info = await cache.get_info()
            logger.info(f"âœ… Redisä¿¡æ¯: {info}")

        return True

    except Exception as e:
        logger.error(f"{cache_type} ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_cache_factory():
    """æµ‹è¯•ç¼“å­˜å·¥å‚"""
    logger.info("=== æµ‹è¯•ç¼“å­˜å·¥å‚ ===")

    try:
        from services.cache_service import CacheFactory

        # æµ‹è¯•å†…å­˜ç¼“å­˜
        memory_cache = CacheFactory.create_cache("memory")
        logger.info("âœ… å†…å­˜ç¼“å­˜åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•Redisç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            redis_cache = CacheFactory.create_cache("redis")
            logger.info("âœ… Redisç¼“å­˜åˆ›å»ºæˆåŠŸ")

            # ç®€å•æµ‹è¯•
            await redis_cache.set("test", {"value": "redis_test"})
            result = await redis_cache.get("test")
            if result and result.get("value") == "redis_test":
                logger.info("âœ… Redisç¼“å­˜åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
            else:
                logger.warning("âš ï¸ Redisç¼“å­˜åŸºæœ¬åŠŸèƒ½å¼‚å¸¸")

        except Exception as e:
            logger.warning(f"âš ï¸ Redisç¼“å­˜ä¸å¯ç”¨: {e}")

        # æµ‹è¯•å•ä¾‹æ¨¡å¼
        cache1 = CacheFactory.create_cache("memory")
        cache2 = CacheFactory.create_cache("memory")
        if cache1 is cache2:
            logger.info("âœ… ç¼“å­˜å·¥å‚å•ä¾‹æ¨¡å¼æ­£å¸¸")
        else:
            logger.error("âŒ ç¼“å­˜å·¥å‚å•ä¾‹æ¨¡å¼å¼‚å¸¸")

        return True

    except Exception as e:
        logger.error(f"ç¼“å­˜å·¥å‚æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_performance_comparison():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½å¯¹æ¯”"""
    logger.info("=== ç¼“å­˜æ€§èƒ½æµ‹è¯• ===")

    try:
        from services.cache_service import CacheFactory
        import time

        # æµ‹è¯•æ•°æ®
        test_data = {
            "large_data": "x" * 1000,  # 1KBæ•°æ®
            "nested": {
                "level1": {
                    "level2": {"level3": "deep_value"}}
                },
                "array": list(range(100))
            },
            "timestamp": datetime.utcnow().isoformat()
        }

        cache_types = ["memory"]

        # å¦‚æœRediså¯ç”¨ï¼Œä¹Ÿæµ‹è¯•Redis
        try:
            redis_cache = CacheFactory.create_cache("redis")
            await redis_cache.set("perf_test", test_data)
            await redis_cache.get("perf_test")
            await redis_cache.delete("perf_test")
            cache_types.append("redis")
        except:
            pass

        results = {}

        for cache_type in cache_types:
            cache = CacheFactory.create_cache(cache_type)
            logger.info(f"æµ‹è¯• {cache_type} ç¼“å­˜æ€§èƒ½...")

            # å†™å…¥æ€§èƒ½æµ‹è¯•
            write_times = []
            for i in range(100):
                start_time = time.time()
                await cache.set(f"perf_test_{i}", test_data, ttl=60)
                write_times.append(time.time() - start_time)

            # è¯»å–æ€§èƒ½æµ‹è¯•
            read_times = []
            for i in range(100):
                start_time = time.time()
                await cache.get(f"perf_test_{i}")
                read_times.append(time.time() - start_time)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_write = sum(write_times) / len(write_times)
            avg_read = sum(read_times) / len(read_times)
            max_write = max(write_times)
            max_read = max(read_times)

            results[cache_type] = {
                "avg_write_ms": round(avg_write * 1000, 3),
                "avg_read_ms": round(avg_read * 1000, 3),
                "max_write_ms": round(max_write * 1000, 3),
                "max_read_ms": round(max_read * 1000, 3)
            }

            # æ¸…ç†æµ‹è¯•æ•°æ®
            for i in range(100):
                await cache.delete(f"perf_test_{i}")

        # è¾“å‡ºæ€§èƒ½å¯¹æ¯”ç»“æœ
        logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        for cache_type, metrics in results.items():
            logger.info(f"{cache_type.upper()} ç¼“å­˜:")
            logger.info(f"  å¹³å‡å†™å…¥: {metrics['avg_write_ms']}ms")
            logger.info(f"  å¹³å‡è¯»å–: {metrics['avg_read_ms']}ms")
            logger.info(f"  æœ€å¤§å†™å…¥: {metrics['max_write_ms']}ms")
            logger.info(f"  æœ€å¤§è¯»å–: {metrics['max_read_ms']}ms")

        return True

    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_cache_integration():
    """æµ‹è¯•ç¼“å­˜é›†æˆåˆ°RAG-SQLæœåŠ¡"""
    logger.info("=== æµ‹è¯•ç¼“å­˜é›†æˆ ===")

    try:
        from services.rag_sql_service import RAGSQLService

        # æµ‹è¯•å†…å­˜ç¼“å­˜æ¨¡å¼
        rag_sql_memory = RAGSQLService(cache_type="memory", use_ai=False)
        logger.info("âœ… RAG-SQLå†…å­˜ç¼“å­˜æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")

        # æµ‹è¯•Redisç¼“å­˜æ¨¡å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            rag_sql_redis = RAGSQLService(cache_type="redis", use_ai=False)
            logger.info("âœ… RAG-SQL Redisç¼“å­˜æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ RAG-SQL Redisç¼“å­˜æ¨¡å¼ä¸å¯ç”¨: {e}")

        return True

    except Exception as e:
        logger.error(f"ç¼“å­˜é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("Data Agent V4 - Redisç¼“å­˜é›†æˆæµ‹è¯•")
    logger.info("=" * 50)

    success = True

    # åŸºç¡€ç¼“å­˜æµ‹è¯•
    success &= await test_cache_service("memory")

    # Redisç¼“å­˜æµ‹è¯•
    success &= await test_cache_service("redis")

    # ç¼“å­˜å·¥å‚æµ‹è¯•
    success &= await test_cache_factory()

    # æ€§èƒ½æµ‹è¯•
    success &= await test_performance_comparison()

    # é›†æˆæµ‹è¯•
    success &= await test_cache_integration()

    if success:
        logger.info("\nğŸ‰ æ‰€æœ‰ç¼“å­˜æµ‹è¯•é€šè¿‡ï¼")
        logger.info("\nä½¿ç”¨æ–¹æ³•:")
        logger.info("1. å†…å­˜ç¼“å­˜ (é»˜è®¤): è®¾ç½® CACHE_TYPE=memory æˆ–ä¸è®¾ç½®")
        logger.info("2. Redisç¼“å­˜: è®¾ç½® CACHE_TYPE=redis å¹¶é…ç½® REDIS_URL")
        logger.info("3. ç¯å¢ƒå˜é‡ç¤ºä¾‹:")
        logger.info("   CACHE_TYPE=redis")
        logger.info("   REDIS_URL=redis://localhost:6379/0")
        logger.info("   REDIS_MAX_CONNECTIONS=10")
        logger.info("   REDIS_TIMEOUT=5")
    else:
        logger.error("\nâŒ éƒ¨åˆ†ç¼“å­˜æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())