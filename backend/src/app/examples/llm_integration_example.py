"""
LLMæœåŠ¡é›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç»Ÿä¸€çš„LLMæœåŠ¡è¿›è¡Œå„ç§æ“ä½œ
"""

import asyncio
from typing import List, Dict, Any

from ..services.llm_service import (
    llm_service,
    LLMProvider,
    LLMMessage,
    LLMResponse,
    LLMStreamChunk
)


async def example_zhipu_chat():
    """ç¤ºä¾‹ï¼šä½¿ç”¨æ™ºè°±AIè¿›è¡Œæ™®é€šå¯¹è¯"""
    print("=== æ™ºè°±AIæ™®é€šå¯¹è¯ç¤ºä¾‹ ===")

    # æ³¨å†Œæä¾›å•†ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¼šåœ¨åº”ç”¨å¯åŠ¨æ—¶å®Œæˆï¼‰
    llm_service.register_provider(
        tenant_id="demo_tenant",
        provider=LLMProvider.ZHIPU,
        api_key="your_zhipu_api_key_here"
    )

    # åˆ›å»ºå¯¹è¯æ¶ˆæ¯
    messages = [
        LLMMessage(role="user", content="ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚")
    ]

    try:
        # è°ƒç”¨èŠå¤©å®Œæˆ
        response: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=messages,
            provider=LLMProvider.ZHIPU,
            model="glm-4-flash",
            max_tokens=100,
            temperature=0.7
        )

        print(f"AIå›å¤: {response.content}")
        print(f"ä½¿ç”¨æ¨¡å‹: {response.model}")
        print(f"Tokenä½¿ç”¨: {response.usage}")
        print(f"æä¾›å•†: {response.provider}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_zhipu_thinking_mode():
    """ç¤ºä¾‹ï¼šä½¿ç”¨æ™ºè°±AIæ·±åº¦æ€è€ƒæ¨¡å¼"""
    print("\n=== æ™ºè°±AIæ·±åº¦æ€è€ƒæ¨¡å¼ç¤ºä¾‹ ===")

    # åˆ›å»ºå¤æ‚é—®é¢˜çš„å¯¹è¯
    messages = [
        LLMMessage(role="user", content="ä½œä¸ºä¸€åè¥é”€ä¸“å®¶ï¼Œè¯·ä¸ºæˆ‘ä»¬çš„AIæ•°æ®åˆ†æäº§å“è®¾è®¡ä¸€ä¸ªå¸å¼•äººçš„å£å·"),
        LLMMessage(role="assistant", content="å½“ç„¶å¯ä»¥ã€‚ä¸ºäº†è®¾è®¡ä¸€ä¸ªå¸å¼•äººçš„å£å·ï¼Œæˆ‘éœ€è¦äº†è§£ä¸€äº›å…³äºæ‚¨äº§å“çš„å…·ä½“ä¿¡æ¯ã€‚"),
        LLMMessage(role="user", content="æˆ‘ä»¬çš„äº§å“æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æå¹³å°ï¼Œå¸®åŠ©ä¼ä¸šä»æµ·é‡æ•°æ®ä¸­å‘ç°å•†ä¸šæ´å¯Ÿï¼Œæ”¯æŒå¯è§†åŒ–æŠ¥è¡¨å’Œé¢„æµ‹åˆ†æã€‚")
    ]

    try:
        # å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        response: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=messages,
            provider=LLMProvider.ZHIPU,
            model="glm-4.6",
            enable_thinking=True,  # å¯ç”¨æ€è€ƒæ¨¡å¼
            max_tokens=300,
            temperature=0.8
        )

        if response.thinking:
            print(f"[æ€è€ƒè¿‡ç¨‹]: {response.thinking}")
        print(f"[æ­£å¼å›å¤]: {response.content}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_zhipu_streaming():
    """ç¤ºä¾‹ï¼šä½¿ç”¨æ™ºè°±AIæµå¼è¾“å‡º"""
    print("\n=== æ™ºè°±AIæµå¼è¾“å‡ºç¤ºä¾‹ ===")

    messages = [
        LLMMessage(role="user", content="è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—")
    ]

    try:
        # æµå¼èŠå¤©å®Œæˆ
        stream_generator = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=messages,
            provider=LLMProvider.ZHIPU,
            model="glm-4-flash",
            stream=True,
            enable_thinking=True
        )

        print("AIå›å¤ï¼ˆæµå¼ï¼‰: ", end="", flush=True)
        thinking_content = ""
        main_content = ""

        async for chunk in stream_generator:
            if chunk.type == "thinking":
                thinking_content += chunk.content
                print(f"\n[æ€è€ƒ]: {chunk.content}", end="", flush=True)
            elif chunk.type == "content":
                main_content += chunk.content
                print(chunk.content, end="", flush=True)
            elif chunk.type == "error":
                print(f"\né”™è¯¯: {chunk.content}")
                break

        print("\n")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_openrouter_multimodal():
    """ç¤ºä¾‹ï¼šä½¿ç”¨OpenRouterè¿›è¡Œå¤šæ¨¡æ€å¯¹è¯"""
    print("\n=== OpenRouterå¤šæ¨¡æ€ç¤ºä¾‹ ===")

    # æ³¨å†ŒOpenRouteræä¾›å•†
    llm_service.register_provider(
        tenant_id="demo_tenant",
        provider=LLMProvider.OPENROUTER,
        api_key="your_openrouter_api_key_here"
    )

    # åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
    multimodal_content = [
        {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"},
        {
            "type": "image_url",
            "image_url": {
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
            }
        }
    ]

    messages = [
        LLMMessage(role="user", content=multimodal_content)
    ]

    try:
        # ä½¿ç”¨OpenRouterçš„å¤šæ¨¡æ€æ¨¡å‹
        response: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=messages,
            provider=LLMProvider.OPENROUTER,
            model="google/gemini-2.0-flash-exp",
            max_tokens=200,
            temperature=0.7
        )

        print(f"å›¾ç‰‡æè¿°: {response.content}")
        print(f"ä½¿ç”¨æ¨¡å‹: {response.model}")
        print(f"æä¾›å•†: {response.provider}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_auto_provider_selection():
    """ç¤ºä¾‹ï¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æä¾›å•†"""
    print("\n=== è‡ªåŠ¨æä¾›å•†é€‰æ‹©ç¤ºä¾‹ ===")

    messages = [
        LLMMessage(role="user", content="ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ")
    ]

    try:
        # ä¸æŒ‡å®šæä¾›å•†ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©
        response: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=messages,
            # ä¸æŒ‡å®šproviderï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æä¾›å•†
            max_tokens=150,
            temperature=0.7
        )

        print(f"AIå›å¤: {response.content}")
        print(f"ç³»ç»Ÿé€‰æ‹©çš„æä¾›å•†: {response.provider}")
        print(f"ä½¿ç”¨æ¨¡å‹: {response.model}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_provider_status_check():
    """ç¤ºä¾‹ï¼šæ£€æŸ¥æä¾›å•†çŠ¶æ€"""
    print("\n=== æä¾›å•†çŠ¶æ€æ£€æŸ¥ç¤ºä¾‹ ===")

    try:
        # éªŒè¯æ‰€æœ‰æä¾›å•†è¿æ¥çŠ¶æ€
        status = await llm_service.validate_providers("demo_tenant")

        print("æä¾›å•†è¿æ¥çŠ¶æ€:")
        for provider, is_available in status.items():
            status_text = "âœ… å¯ç”¨" if is_available else "âŒ ä¸å¯ç”¨"
            print(f"  {provider}: {status_text}")

        # è·å–å¯ç”¨æ¨¡å‹
        models = await llm_service.get_available_models("demo_tenant")

        print("\nå¯ç”¨æ¨¡å‹:")
        for provider, model_list in models.items():
            print(f"  {provider}:")
            for model in model_list:
                print(f"    - {model}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def example_multi_turn_conversation():
    """ç¤ºä¾‹ï¼šå¤šè½®å¯¹è¯"""
    print("\n=== å¤šè½®å¯¹è¯ç¤ºä¾‹ ===")

    # ç»´æŠ¤å¯¹è¯å†å²
    conversation_history = [
        LLMMessage(role="user", content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    ]

    try:
        # ç¬¬ä¸€è½®å¯¹è¯
        print("ç”¨æˆ·: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
        response1: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=conversation_history,
            provider=LLMProvider.ZHIPU,
            max_tokens=200
        )

        print(f"åŠ©æ‰‹: {response1.content}")

        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
        conversation_history.append(
            LLMMessage(role="assistant", content=response1.content)
        )

        # ç¬¬äºŒè½®å¯¹è¯
        user_input = "èƒ½ç»™æˆ‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿ"
        conversation_history.append(
            LLMMessage(role="user", content=user_input)
        )

        print(f"\nç”¨æˆ·: {user_input}")
        response2: LLMResponse = await llm_service.chat_completion(
            tenant_id="demo_tenant",
            messages=conversation_history,
            provider=LLMProvider.ZHIPU,
            max_tokens=200
        )

        print(f"åŠ©æ‰‹: {response2.content}")

        # æ˜¾ç¤ºå¯¹è¯ç»Ÿè®¡
        total_tokens = (response1.usage.get("total_tokens", 0) +
                       response2.usage.get("total_tokens", 0))
        print(f"\nå¯¹è¯ç»Ÿè®¡: æ€»å…±ä½¿ç”¨äº† {total_tokens} ä¸ªtokens")

    except Exception as e:
        print(f"é”™è¯¯: {e}")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ LLMæœåŠ¡é›†æˆç¤ºä¾‹å¼€å§‹\n")

    # è¿è¡Œå„ç§ç¤ºä¾‹
    await example_zhipu_chat()
    await example_zhipu_thinking_mode()
    await example_zhipu_streaming()
    await example_openrouter_multimodal()
    await example_auto_provider_selection()
    await example_provider_status_check()
    await example_multi_turn_conversation()

    print("\nâœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())