"""
ç»Ÿä¸€LLMæœåŠ¡
æ”¯æŒZhipu AIå’ŒOpenRouterå¤šæ¨¡æ€æ¨¡å‹
æä¾›ç§Ÿæˆ·éš”ç¦»å’Œæµå¼è¾“å‡ºåŠŸèƒ½
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional, List, AsyncGenerator, Union
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
from abc import ABC, abstractmethod

import aiohttp
from openai import OpenAI, AsyncOpenAI
from zhipuai import ZhipuAI

from src.app.core.config import settings
from src.app.services.multimodal_processor import multimodal_processor

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    DEEPSEEK = "deepseek"
    ZHIPU = "zhipu"
    OPENROUTER = "openrouter"


@dataclass
class LLMMessage:
    """LLMæ¶ˆæ¯ç»“æ„"""
    role: str  # "user", "assistant", "system", "tool"
    content: Union[str, List[Dict[str, Any]]]  # æ”¯æŒå¤šæ¨¡æ€å†…å®¹
    thinking: Optional[str] = None  # æ€è€ƒè¿‡ç¨‹
    tool_calls: Optional[List[Dict[str, Any]]] = None  # å·¥å…·è°ƒç”¨ï¼ˆassistantè§’è‰²ï¼‰
    tool_call_id: Optional[str] = None  # å·¥å…·è°ƒç”¨IDï¼ˆtoolè§’è‰²ï¼‰


@dataclass
class LLMResponse:
    """LLMå“åº”ç»“æ„"""
    content: str
    thinking: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    model: Optional[str] = None
    provider: Optional[str] = None
    finish_reason: Optional[str] = None
    created_at: Optional[str] = None


@dataclass
class LLMStreamChunk:
    """LLMæµå¼å“åº”å—"""
    type: str  # "thinking", "content", "error"
    content: str
    provider: Optional[str] = None
    finished: bool = False


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""

    def __init__(self, api_key: str, tenant_id: str):
        self.api_key = api_key
        self.tenant_id = tenant_id

    @abstractmethod
    async def chat_completion(
        self,
        messages: List[LLMMessage],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[LLMResponse, AsyncGenerator[LLMStreamChunk, None]]:
        """èŠå¤©å®Œæˆæ¥å£"""
        pass

    @abstractmethod
    async def validate_connection(self) -> bool:
        """éªŒè¯è¿æ¥"""
        pass


class ZhipuProvider(BaseLLMProvider):
    """æ™ºè°±AIæä¾›å•†"""

    def __init__(self, api_key: str, tenant_id: str):
        super().__init__(api_key, tenant_id)
        self.client = ZhipuAI(api_key=api_key)
        self.default_model = getattr(settings, 'zhipuai_default_model', 'glm-4.6')
        self.thinking_indicators = [
            "åˆ†æ", "æ¯”è¾ƒ", "è¯„ä¼°", "è§£é‡Š", "è®¾è®¡", "è§„åˆ’",
            "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åŸå› ", "å½±å“", "å»ºè®®", "æ­¥éª¤",
            "åŸç†", "æœºåˆ¶", "ç­–ç•¥", "æ–¹æ¡ˆ", "ä¼˜ç¼ºç‚¹"
        ]

    def should_enable_thinking(self, messages: List[LLMMessage]) -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦å»ºè®®å¯ç”¨æ€è€ƒæ¨¡å¼
        """
        try:
            # è·å–ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = ""
            for msg in messages:
                if msg.role == "user":
                    if isinstance(msg.content, str):
                        user_content += msg.content + " "
                    elif isinstance(msg.content, list):
                        for item in msg.content:
                            if item.get("type") == "text":
                                user_content += item.get("text", "") + " "

            user_content = user_content.strip()
            if not user_content:
                return False

            # æ£€æŸ¥æ€è€ƒæ¨¡å¼æŒ‡ç¤ºè¯
            content_lower = user_content.lower()
            for indicator in self.thinking_indicators:
                if indicator in content_lower:
                    logger.debug(f"æ£€æµ‹åˆ°æ€è€ƒæ¨¡å¼æŒ‡ç¤ºè¯: {indicator}")
                    return True

            # æ£€æŸ¥é—®é¢˜å¤æ‚åº¦ï¼ˆåŸºäºé•¿åº¦å’Œç»“æ„ï¼‰
            if len(user_content) > 200:  # è¾ƒé•¿çš„é—®é¢˜
                return True
            if "ï¼Ÿ" in user_content or user_content.count("?") > 1:  # å¤šä¸ªé—®å·
                return True
            if "æ­¥éª¤" in user_content or "è¯¦ç»†" in user_content:  # éœ€è¦è¯¦ç»†å›ç­”
                return True

            return False
        except Exception as e:
            logger.warning(f"æ€è€ƒæ¨¡å¼åˆ¤æ–­å¤±è´¥: {e}")
            return False

    async def chat_completion(
        self,
        messages: List[LLMMessage],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        enable_thinking: Optional[bool] = None,
        **kwargs
    ) -> Union[LLMResponse, AsyncGenerator[LLMStreamChunk, None]]:
        """æ™ºè°±AIèŠå¤©å®Œæˆ"""
        try:
            model = model or self.default_model
            max_tokens = max_tokens or getattr(settings, "llm_max_output_tokens", 8192)
            temperature = temperature if temperature is not None else 0.7

            # æ™ºèƒ½æ€è€ƒæ¨¡å¼åˆ¤æ–­
            if enable_thinking is None:
                enable_thinking = self.should_enable_thinking(messages)
                if enable_thinking:
                    logger.info("æ™ºèƒ½å¯ç”¨æ€è€ƒæ¨¡å¼")

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in messages:
                api_msg = {"role": msg.role, "content": msg.content}
                api_messages.append(api_msg)

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": model,
                "messages": api_messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": stream
            }

            # å¯ç”¨æ€è€ƒæ¨¡å¼
            if enable_thinking:
                params["thinking"] = {"type": "enabled"}
                logger.debug(f"æ™ºè°±AIå¯ç”¨æ€è€ƒæ¨¡å¼: {model}")

            if stream:
                return self._stream_response(params, model)
            else:
                response = self.client.chat.completions.create(**params)

                return LLMResponse(
                    content=response.choices[0].message.content or "",
                    thinking=getattr(response.choices[0].message, 'reasoning_content', None),
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    } if response.usage else None,
                    model=response.model,
                    provider=LLMProvider.ZHIPU.value,
                    finish_reason=response.choices[0].finish_reason,
                    created_at=datetime.fromtimestamp(response.created).isoformat()
                )

        except Exception as e:
            logger.error(f"Zhipu chat completion failed: {e}")
            if stream:
                async def error_stream():
                    yield LLMStreamChunk(
                        type="error",
                        content=f"Zhipu API error: {str(e)}",
                        provider=LLMProvider.ZHIPU.value
                    )
                return error_stream()
            else:
                raise

    async def _stream_response(
        self,
        params: Dict[str, Any],
        model: str
    ) -> AsyncGenerator[LLMStreamChunk, None]:
        """å¤„ç†æµå¼å“åº”"""
        stream_started = False
        thinking_phase = False
        content_phase = False

        try:
            logger.debug(f"å¼€å§‹æ™ºè°±AIæµå¼å“åº”: {model}")
            response = self.client.chat.completions.create(**params)

            for chunk in response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    stream_started = True

                    # æ€è€ƒè¿‡ç¨‹
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        if not thinking_phase:
                            thinking_phase = True
                            logger.debug("æ™ºè°±AIæ€è€ƒè¿‡ç¨‹å¼€å§‹")

                        yield LLMStreamChunk(
                            type="thinking",
                            content=delta.reasoning_content,
                            provider=LLMProvider.ZHIPU.value,
                            finished=False
                        )

                    # æ­£å¼å†…å®¹
                    elif hasattr(delta, 'content') and delta.content:
                        if not content_phase and thinking_phase:
                            content_phase = True
                            logger.debug("æ™ºè°±AIä»æ€è€ƒè¿‡ç¨‹åˆ‡æ¢åˆ°æ­£å¼å›å¤")

                        yield LLMStreamChunk(
                            type="content",
                            content=delta.content,
                            provider=LLMProvider.ZHIPU.value,
                            finished=False
                        )

            # å‘é€å®Œæˆæ ‡è®°
            if stream_started:
                yield LLMStreamChunk(
                    type="content",
                    content="",
                    provider=LLMProvider.ZHIPU.value,
                    finished=True
                )
                logger.debug("æ™ºè°±AIæµå¼å“åº”å®Œæˆ")

        except Exception as e:
            logger.error(f"Zhipu stream response failed: {e}")
            yield LLMStreamChunk(
                type="error",
                content=f"æ™ºè°±AIæµå¼å“åº”é”™è¯¯: {str(e)}",
                provider=LLMProvider.ZHIPU.value,
                finished=True
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯æ™ºè°±AIè¿æ¥"""
        try:
            response = self.client.chat.completions.create(
                model=self.default_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            return bool(response and response.choices)
        except Exception as e:
            logger.error(f"Zhipu connection validation failed: {e}")
            return False


class DeepSeekProvider(BaseLLMProvider):
    """DeepSeekæä¾›å•†ï¼ˆé»˜è®¤ï¼‰"""

    def __init__(self, api_key: str, tenant_id: str, base_url: str = None, default_model: str = None):
        super().__init__(api_key, tenant_id)
        base_url = base_url or getattr(settings, 'deepseek_base_url', 'https://api.deepseek.com')
        self.client = AsyncOpenAI(
            base_url=base_url,
            api_key=api_key
        )
        self.default_model = default_model or getattr(settings, 'deepseek_default_model', 'deepseek-chat')

    async def chat_completion(
        self,
        messages: List[LLMMessage],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> Union[LLMResponse, AsyncGenerator[LLMStreamChunk, None]]:
        """DeepSeekèŠå¤©å®Œæˆ"""
        try:
            model = model or self.default_model
            max_tokens = max_tokens or getattr(settings, "llm_max_output_tokens", 8192)
            temperature = temperature if temperature is not None else 0.7

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in messages:
                # å…¼å®¹ç›´æ¥ä¼ å…¥ dict è€Œä¸æ˜¯ LLMMessage çš„æƒ…å†µ
                if isinstance(msg, dict):
                    role = msg.get("role")
                    # ğŸ”§ ä¿®å¤ï¼šdict.get() åœ¨é”®å­˜åœ¨ä½†å€¼ä¸º None æ—¶ä¼šè¿”å› Noneï¼Œéœ€è¦é¢å¤–å¤„ç†
                    content_raw = msg.get("content")
                    if content_raw is None:
                        content_raw = ""  # å¼ºåˆ¶è½¬æ¢ None ä¸ºç©ºå­—ç¬¦ä¸²
                    # å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯
                    tool_calls = msg.get("tool_calls")
                    tool_call_id = msg.get("tool_call_id")
                else:
                    role = getattr(msg, "role", None)
                    # ğŸ”§ ä¿®å¤ï¼šgetattr() åœ¨å±æ€§å€¼ä¸º None æ—¶ä¹Ÿä¼šè¿”å› None
                    content_raw = getattr(msg, "content", None)
                    if content_raw is None:
                        content_raw = ""  # å¼ºåˆ¶è½¬æ¢ None ä¸ºç©ºå­—ç¬¦ä¸²
                    tool_calls = getattr(msg, "tool_calls", None)
                    tool_call_id = getattr(msg, "tool_call_id", None)

                message_dict = {"role": role}
                
                # å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯
                if tool_calls:
                    message_dict["tool_calls"] = tool_calls
                if tool_call_id:
                    message_dict["tool_call_id"] = tool_call_id
                
                # ğŸ”§ ä¿®å¤ï¼šå¤„ç† content ä¸º None çš„æƒ…å†µ
                # DeepSeek æœ‰æ—¶ä¼šç›´æ¥ç”Ÿæˆ tool_calls è€Œä¸ç”Ÿæˆæ–‡æœ¬ï¼Œæ­¤æ—¶ content ä¸º None
                # ä½† DeepSeek API è¦æ±‚ content å­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œä¸èƒ½æ˜¯ null
                if content_raw is None:
                    # å¦‚æœæ˜¯ assistant è§’è‰²ä¸”æœ‰ tool_callsï¼Œcontent å¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²
                    message_dict["content"] = ""
                elif isinstance(content_raw, str):
                    message_dict["content"] = content_raw
                elif isinstance(content_raw, list):
                    # å¤šæ¨¡æ€å†…å®¹å¤„ç†
                    content = []
                    for item in content_raw:
                        if item.get("type") == "text":
                            content.append({"type": "text", "text": item.get("text", "")})
                        elif item.get("type") == "image_url":
                            content.append({
                                "type": "image_url",
                                "image_url": item.get("image_url", {})
                            })
                    message_dict["content"] = content
                else:
                    # å…œåº•ï¼šè½¬æˆå­—ç¬¦ä¸²ï¼ˆå¤„ç†é None çš„å…¶ä»–ç±»å‹ï¼‰
                    message_dict["content"] = str(content_raw) if content_raw is not None else ""
                
                api_messages.append(message_dict)

            # ğŸ”§ ä¿®å¤ï¼šåœ¨å‘é€è¯·æ±‚å‰å†æ¬¡æ¸…æ´—æ¶ˆæ¯ï¼ˆç»ˆæä¿é™©ï¼‰
            # ç¡®ä¿æ‰€æœ‰ content ä¸º None çš„æ¶ˆæ¯éƒ½è¢«è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            for i, msg in enumerate(api_messages):
                if msg.get("content") is None:
                    logger.warning(f"[DeepSeek] æ„å»ºAPIæ¶ˆæ¯æ—¶å‘ç°æ¶ˆæ¯ {i} (role={msg.get('role')}) çš„ content ä¸º Noneï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²")
                    msg["content"] = ""
            
            # æ„å»º API è°ƒç”¨å‚æ•°
            api_kwargs = {
                "model": model,
                "messages": api_messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # å¦‚æœæä¾›äº†å·¥å…·ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if tools:
                api_kwargs["tools"] = tools
                api_kwargs["tool_choice"] = "auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·

            if stream:
                return self._stream_response(api_messages, model, max_tokens, temperature, tools=tools)
            else:
                response = await self.client.chat.completions.create(**api_kwargs)

                return LLMResponse(
                    content=response.choices[0].message.content or "",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    } if response.usage else None,
                    model=response.model,
                    provider=LLMProvider.DEEPSEEK.value,
                    finish_reason=response.choices[0].finish_reason,
                    created_at=datetime.fromtimestamp(response.created).isoformat()
                )

        except Exception as e:
            logger.error(f"DeepSeek chat completion failed: {e}")
            if stream:
                async def error_stream():
                    yield LLMStreamChunk(
                        type="error",
                        content=f"DeepSeek API error: {str(e)}",
                        provider=LLMProvider.DEEPSEEK.value
                    )
                return error_stream()
            else:
                raise

    async def _stream_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        max_tokens: int,
        temperature: float,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> AsyncGenerator[LLMStreamChunk, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            logger.info(f"[DeepSeek] Starting stream request for model: {model}, tools: {bool(tools)}")
            
            # ğŸ”§ ç»ˆæä¿®å¤ï¼šå¼ºåˆ¶æ¸…æ´—æ¶ˆæ¯ä¸­çš„æ‰€æœ‰ None å­—æ®µ
            # DeepSeek API å¯¹æ¶ˆæ¯æ ¼å¼éå¸¸ä¸¥æ ¼ï¼š
            # - content å­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼ˆä¸èƒ½æ˜¯ nullï¼‰
            # - tool_calls ä¸­çš„ idã€function.nameã€function.arguments éƒ½ä¸èƒ½æ˜¯ null
            import copy
            sanitized_messages = []
            for i, msg in enumerate(messages):
                # ğŸ”§ ä½¿ç”¨æ·±æ‹·è´é¿å…ä»»ä½•å¼•ç”¨é—®é¢˜
                sanitized_msg = copy.deepcopy(msg)
                
                # ğŸ”§ å¼ºåˆ¶æ£€æŸ¥å¹¶ä¿®å¤ None contentï¼ˆä¸ç®¡åŸå€¼æ˜¯ä»€ä¹ˆï¼‰
                content = sanitized_msg.get("content")
                if content is None:
                    logger.warning(f"[DeepSeek] æ¶ˆæ¯ {i} (role={sanitized_msg.get('role')}) çš„ content ä¸º Noneï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²")
                    sanitized_msg["content"] = ""
                
                # ğŸ”§ æ¸…æ´— tool_calls ä¸­çš„ null å­—æ®µ
                if sanitized_msg.get("tool_calls"):
                    cleaned_tool_calls = []
                    for tc in sanitized_msg["tool_calls"]:
                        cleaned_tc = {
                            "id": tc.get("id") if tc.get("id") is not None else f"call_{i}_{len(cleaned_tool_calls)}",
                            "type": tc.get("type", "function"),
                            "function": {
                                "name": tc.get("function", {}).get("name") if tc.get("function", {}).get("name") is not None else "",
                                "arguments": tc.get("function", {}).get("arguments") if tc.get("function", {}).get("arguments") is not None else "{}"
                            }
                        }
                        # è®°å½•æ¸…æ´—æƒ…å†µ
                        if tc.get("id") is None or tc.get("function", {}).get("name") is None:
                            logger.warning(f"[DeepSeek] æ¶ˆæ¯ {i} çš„ tool_call å­˜åœ¨ null å­—æ®µï¼Œå·²æ¸…æ´—: id={cleaned_tc['id']}, name={cleaned_tc['function']['name']}")
                        cleaned_tool_calls.append(cleaned_tc)
                    sanitized_msg["tool_calls"] = cleaned_tool_calls
                
                # ğŸ”§ é¢å¤–éªŒè¯ï¼šç¡®ä¿ content çœŸçš„æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
                final_content = sanitized_msg.get("content")
                if final_content is None:
                    logger.error(f"[DeepSeek] ä¸¥é‡é”™è¯¯ï¼šæ¶ˆæ¯ {i} çš„ content åœ¨ä¿®å¤åä»ç„¶æ˜¯ Noneï¼å¼ºåˆ¶è®¾ä¸ºç©ºå­—ç¬¦ä¸²")
                    sanitized_msg["content"] = ""
                
                sanitized_messages.append(sanitized_msg)
            
            # ğŸ”§ æœ€ç»ˆéªŒè¯ï¼šæ‰“å°æ¯æ¡æ¶ˆæ¯çš„ content ç±»å‹
            for i, msg in enumerate(sanitized_messages):
                content = msg.get("content")
                content_type = type(content).__name__
                content_preview = str(content)[:50] if content else "(empty)"
                logger.info(f"[DeepSeek] æœ€ç»ˆæ¶ˆæ¯ {i}: role={msg.get('role')}, content_type={content_type}, preview={content_preview}")
            
            logger.info(f"[DeepSeek] æ¸…æ´—åçš„æ¶ˆæ¯æ•°é‡: {len(sanitized_messages)}")
            
            # AsyncOpenAI çš„ create æ–¹æ³•æ˜¯å¼‚æ­¥çš„ï¼Œå¿…é¡» await æ‰èƒ½å¾—åˆ°æµå¯¹è±¡
            api_kwargs = {
                "model": model,
                "messages": sanitized_messages,  # ä½¿ç”¨æ¸…æ´—åçš„æ¶ˆæ¯
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True
            }
            
            # å¦‚æœæä¾›äº†å·¥å…·ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if tools:
                api_kwargs["tools"] = tools
                api_kwargs["tool_choice"] = "auto"
            
            stream = await self.client.chat.completions.create(**api_kwargs)
            
            # æ£€æŸ¥ stream å¯¹è±¡çš„ç±»å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.info(f"[DeepSeek] Stream object type: {type(stream)}, has __aiter__: {hasattr(stream, '__aiter__')}")

            # ç¡®ä¿æµå¯¹è±¡å­˜åœ¨
            if stream is None:
                logger.error("DeepSeek stream response is None")
                yield LLMStreamChunk(
                    type="error",
                    content="DeepSeek API returned None stream",
                    provider=LLMProvider.DEEPSEEK.value,
                    finished=True
                )
                return

            # è¿­ä»£æµï¼Œç¡®ä¿ç­‰å¾…æ¯ä¸ª chunk
            has_content = False
            has_tool_calls = False
            tool_calls_accumulator = {}  # ç”¨äºæ”¶é›†æµå¼å·¥å…·è°ƒç”¨å‚æ•°
            chunk_count = 0  # ğŸ” è°ƒè¯•ï¼šè®¡æ•°
            
            async for chunk in stream:
                chunk_count += 1
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    finish_reason = chunk.choices[0].finish_reason
                    
                    # ğŸ” è°ƒè¯•ï¼šæ¯ 10 ä¸ª chunk æ‰“å°ä¸€æ¬¡è¯¦æƒ…ï¼Œæˆ–è€…é‡åˆ°ç‰¹æ®Šæƒ…å†µæ—¶æ‰“å°
                    if chunk_count <= 3 or chunk_count % 50 == 0 or finish_reason:
                        logger.info(f"[DeepSeek RAW] chunk #{chunk_count}: "
                                   f"finish_reason={finish_reason}, "
                                   f"has_tool_calls={hasattr(delta, 'tool_calls') and bool(delta.tool_calls)}, "
                                   f"has_content={hasattr(delta, 'content') and bool(delta.content)}, "
                                   f"content_preview={repr(delta.content[:50] if hasattr(delta, 'content') and delta.content else None)}")
                    
                    # ğŸ” è°ƒè¯•ï¼šå¦‚æœæ£€æµ‹åˆ° tool_callsï¼Œè¯¦ç»†æ‰“å°
                    if hasattr(delta, 'tool_calls') and delta.tool_calls:
                        logger.info(f"[DeepSeek RAW] ğŸ‰ æ£€æµ‹åˆ° tool_calls! chunk #{chunk_count}: {delta.tool_calls}")

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if hasattr(delta, 'tool_calls') and delta.tool_calls:
                        has_tool_calls = True
                        for tool_call_delta in delta.tool_calls:
                            index = tool_call_delta.index
                            if index not in tool_calls_accumulator:
                                tool_calls_accumulator[index] = {
                                    "id": tool_call_delta.id if hasattr(tool_call_delta, 'id') else None,
                                    "type": "function",
                                    "function": {
                                        "name": "",
                                        "arguments": ""
                                    }
                                }
                            
                            # æ”¶é›†å·¥å…·åç§°ï¼ˆåªæœ‰é None æ—¶æ‰æ›´æ–°ï¼‰
                            if hasattr(tool_call_delta, 'function') and hasattr(tool_call_delta.function, 'name'):
                                if tool_call_delta.function.name is not None:
                                    tool_calls_accumulator[index]["function"]["name"] = tool_call_delta.function.name
                            
                            # æ”¶é›†å·¥å…·å‚æ•°ï¼ˆå¯èƒ½æ˜¯åˆ†ç‰‡çš„ï¼Œåªæœ‰é None æ—¶æ‰è¿½åŠ ï¼‰
                            if hasattr(tool_call_delta, 'function') and hasattr(tool_call_delta.function, 'arguments'):
                                if tool_call_delta.function.arguments is not None:
                                    tool_calls_accumulator[index]["function"]["arguments"] += tool_call_delta.function.arguments
                            
                            # æ”¶é›†å·¥å…· IDï¼ˆåªæœ‰é None æ—¶æ‰æ›´æ–°ï¼‰
                            if hasattr(tool_call_delta, 'id') and tool_call_delta.id is not None:
                                tool_calls_accumulator[index]["id"] = tool_call_delta.id

                    # å¤„ç†æ™®é€šå†…å®¹
                    if hasattr(delta, 'content') and delta.content:
                        has_content = True
                        yield LLMStreamChunk(
                            type="content",
                            content=delta.content,
                            provider=LLMProvider.DEEPSEEK.value,
                            finished=False
                        )
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå‘é€å·¥å…·è°ƒç”¨äº‹ä»¶
            if has_tool_calls and tool_calls_accumulator:
                tool_calls_list = [tool_calls_accumulator[i] for i in sorted(tool_calls_accumulator.keys())]
                yield LLMStreamChunk(
                    type="tool_input",
                    content=json.dumps(tool_calls_list, ensure_ascii=False),
                    provider=LLMProvider.DEEPSEEK.value,
                    finished=False
                )

            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œå‘é€å®Œæˆæ ‡è®°
            if not has_content:
                logger.warning("DeepSeek stream returned no content")
                yield LLMStreamChunk(
                    type="content",
                    content="",
                    provider=LLMProvider.DEEPSEEK.value,
                    finished=True
                )
            else:
                # å‘é€å®Œæˆæ ‡è®°
                yield LLMStreamChunk(
                    type="done",
                    content="",
                    provider=LLMProvider.DEEPSEEK.value,
                    finished=True
                )

        except Exception as e:
            logger.error(f"DeepSeek stream response failed: {e}", exc_info=True)
            yield LLMStreamChunk(
                type="error",
                content=f"DeepSeek stream error: {str(e)}",
                provider=LLMProvider.DEEPSEEK.value,
                finished=True
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯DeepSeekè¿æ¥"""
        try:
            response = await self.client.chat.completions.create(
                model=self.default_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            return bool(response and response.choices)
        except Exception as e:
            logger.error(f"DeepSeek connection validation failed: {e}")
            return False


class OpenRouterProvider(BaseLLMProvider):
    """OpenRouteræä¾›å•†"""

    def __init__(self, api_key: str, tenant_id: str):
        super().__init__(api_key, tenant_id)
        self.client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key
        )
        self.default_model = "google/gemini-2.0-flash-exp"

    async def chat_completion(
        self,
        messages: List[LLMMessage],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        tenant_id: Optional[str] = None,
        **kwargs
    ) -> Union[LLMResponse, AsyncGenerator[LLMStreamChunk, None]]:
        """OpenRouterèŠå¤©å®Œæˆ"""
        try:
            model = model or self.default_model
            max_tokens = max_tokens or getattr(settings, "llm_max_output_tokens", 8192)
            temperature = temperature if temperature is not None else 0.7

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in messages:
                if isinstance(msg.content, str):
                    api_messages.append({"role": msg.role, "content": msg.content})
                elif isinstance(msg.content, list):
                    # å¤šæ¨¡æ€å†…å®¹å¤„ç†
                    content = []

                    # å¦‚æœæä¾›äº†tenant_idï¼Œå¤„ç†å¤šæ¨¡æ€å†…å®¹ä¸Šä¼ 
                    if tenant_id:
                        try:
                            processed_content = await multimodal_processor.process_content_list(
                                msg.content, tenant_id
                            )
                            logger.debug(f"å¤šæ¨¡æ€å†…å®¹å¤„ç†å®Œæˆï¼Œé¡¹ç›®æ•°: {len(processed_content)}")
                        except Exception as e:
                            logger.warning(f"å¤šæ¨¡æ€å†…å®¹å¤„ç†å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹å†…å®¹")
                            processed_content = msg.content
                    else:
                        processed_content = msg.content

                    # è½¬æ¢ä¸ºOpenRouteræ ¼å¼
                    for item in processed_content:
                        if item.get("type") == "text":
                            content.append({"type": "text", "text": item.get("text", "")})
                        elif item.get("type") == "image_url":
                            content.append({
                                "type": "image_url",
                                "image_url": item.get("image_url", {})
                            })
                        elif item.get("type") == "input_audio":
                            content.append({
                                "type": "input_audio",
                                "input_audio": item.get("input_audio", {})
                            })
                        elif item.get("type") == "video_url":
                            content.append({
                                "type": "video_url",
                                "video_url": item.get("video_url", {})
                            })

                    api_messages.append({"role": msg.role, "content": content})

            if stream:
                return self._stream_response(api_messages, model, max_tokens, temperature)
            else:
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=api_messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    extra_headers={
                        "HTTP-Referer": getattr(settings, 'openrouter_referer', ''),
                        "X-Title": getattr(settings, 'openrouter_app_name', 'Data Agent')
                    }
                )

                return LLMResponse(
                    content=response.choices[0].message.content or "",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    } if response.usage else None,
                    model=response.model,
                    provider=LLMProvider.OPENROUTER.value,
                    finish_reason=response.choices[0].finish_reason,
                    created_at=datetime.fromtimestamp(response.created).isoformat()
                )

        except Exception as e:
            logger.error(f"OpenRouter chat completion failed: {e}")
            if stream:
                async def error_stream():
                    yield LLMStreamChunk(
                        type="error",
                        content=f"OpenRouter API error: {str(e)}",
                        provider=LLMProvider.OPENROUTER.value
                    )
                return error_stream()
            else:
                raise

    async def _stream_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        max_tokens: int,
        temperature: float
    ) -> AsyncGenerator[LLMStreamChunk, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True,
                extra_headers={
                    "HTTP-Referer": getattr(settings, 'openrouter_referer', ''),
                    "X-Title": getattr(settings, 'openrouter_app_name', 'Data Agent')
                }
            )

            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta

                    if hasattr(delta, 'content') and delta.content:
                        yield LLMStreamChunk(
                            type="content",
                            content=delta.content,
                            provider=LLMProvider.OPENROUTER.value
                        )

        except Exception as e:
            logger.error(f"OpenRouter stream response failed: {e}")
            yield LLMStreamChunk(
                type="error",
                content=f"OpenRouter stream error: {str(e)}",
                provider=LLMProvider.OPENROUTER.value
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯OpenRouterè¿æ¥"""
        try:
            response = await self.client.chat.completions.create(
                model=self.default_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            return bool(response and response.choices)
        except Exception as e:
            logger.error(f"OpenRouter connection validation failed: {e}")
            return False


class LLMService:
    """ç»Ÿä¸€LLMæœåŠ¡"""

    def __init__(self):
        self.providers: Dict[str, BaseLLMProvider] = {}
        self.tenant_configs: Dict[str, Dict[str, Any]] = {}

    def register_provider(
        self,
        tenant_id: str,
        provider: LLMProvider,
        api_key: str
    ) -> BaseLLMProvider:
        """æ³¨å†Œæä¾›å•†"""
        if tenant_id not in self.tenant_configs:
            self.tenant_configs[tenant_id] = {}

        if provider == LLMProvider.DEEPSEEK:
            base_url = getattr(settings, 'deepseek_base_url', 'https://api.deepseek.com')
            default_model = getattr(settings, 'deepseek_default_model', 'deepseek-chat')
            provider_instance = DeepSeekProvider(api_key, tenant_id, base_url, default_model)
        elif provider == LLMProvider.ZHIPU:
            provider_instance = ZhipuProvider(api_key, tenant_id)
        elif provider == LLMProvider.OPENROUTER:
            provider_instance = OpenRouterProvider(api_key, tenant_id)
        else:
            raise ValueError(f"Unsupported provider: {provider}")

        key = f"{tenant_id}_{provider.value}"
        self.providers[key] = provider_instance
        self.tenant_configs[tenant_id][provider.value] = api_key

        return provider_instance

    def get_provider(
        self,
        tenant_id: str,
        provider: LLMProvider = None
    ) -> Optional[BaseLLMProvider]:
        """è·å–æä¾›å•†"""
        if not provider:
            # é»˜è®¤ä¼˜å…ˆä½¿ç”¨DeepSeekï¼Œå›é€€åˆ°Zhipuï¼Œæœ€åOpenRouter
            for p in [LLMProvider.DEEPSEEK, LLMProvider.ZHIPU, LLMProvider.OPENROUTER]:
                key = f"{tenant_id}_{p.value}"
                if key in self.providers:
                    return self.providers[key]
            return None

        key = f"{tenant_id}_{provider.value}"
        return self.providers.get(key)

    async def chat_completion(
        self,
        tenant_id: str,
        messages: List[LLMMessage],
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        enable_thinking: Optional[bool] = None,
        **kwargs
    ) -> Union[LLMResponse, AsyncGenerator[LLMStreamChunk, None]]:
        """ç»Ÿä¸€èŠå¤©å®Œæˆæ¥å£"""
        provider_instance = self.get_provider(tenant_id, provider)

        if not provider_instance:
            # å°è¯•è‡ªåŠ¨æ³¨å†Œé»˜è®¤æä¾›å•†ï¼ˆä¼˜å…ˆDeepSeekï¼‰
            try:
                if hasattr(settings, 'deepseek_api_key') and settings.deepseek_api_key:
                    provider_instance = self.register_provider(
                        tenant_id, LLMProvider.DEEPSEEK, settings.deepseek_api_key
                    )
                elif hasattr(settings, 'zhipuai_api_key') and settings.zhipuai_api_key:
                    provider_instance = self.register_provider(
                        tenant_id, LLMProvider.ZHIPU, settings.zhipuai_api_key
                    )
                elif hasattr(settings, 'openrouter_api_key') and settings.openrouter_api_key:
                    provider_instance = self.register_provider(
                        tenant_id, LLMProvider.OPENROUTER, settings.openrouter_api_key
                    )
                else:
                    raise ValueError(f"No provider configured for tenant {tenant_id}")
            except Exception as e:
                raise ValueError(f"Failed to initialize provider for tenant {tenant_id}: {e}")

        # è°ƒç”¨å…·ä½“æä¾›å•†
        # æ³¨æ„ï¼šchat_completion æ˜¯å¼‚æ­¥å‡½æ•°ï¼Œå³ä½¿è¿”å› AsyncGenerator ä¹Ÿéœ€è¦ await
        # await ä¸€ä¸ªè¿”å›ç”Ÿæˆå™¨çš„å¼‚æ­¥å‡½æ•°ä¼šå¾—åˆ°ç”Ÿæˆå™¨å¯¹è±¡
        if stream:
            # æµå¼æ¨¡å¼ï¼šéœ€è¦ await å¼‚æ­¥å‡½æ•°æ¥è·å–ç”Ÿæˆå™¨å¯¹è±¡
            return await provider_instance.chat_completion(
                messages=messages,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=stream,
                enable_thinking=enable_thinking,
                tenant_id=tenant_id,  # ä¼ é€’ç§Ÿæˆ·IDç”¨äºå¤šæ¨¡æ€å¤„ç†
                **kwargs
            )
        else:
            # éæµå¼æ¨¡å¼ï¼šéœ€è¦ await ç­‰å¾…ç»“æœ
            return await provider_instance.chat_completion(
                messages=messages,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=stream,
                enable_thinking=enable_thinking,
                tenant_id=tenant_id,  # ä¼ é€’ç§Ÿæˆ·IDç”¨äºå¤šæ¨¡æ€å¤„ç†
                **kwargs
            )

    async def validate_providers(self, tenant_id: str) -> Dict[str, bool]:
        """éªŒè¯æ‰€æœ‰æä¾›å•†è¿æ¥"""
        results = {}

        for provider in [LLMProvider.DEEPSEEK, LLMProvider.ZHIPU, LLMProvider.OPENROUTER]:
            provider_instance = self.get_provider(tenant_id, provider)
            if provider_instance:
                results[provider.value] = await provider_instance.validate_connection()
            else:
                results[provider.value] = False

        return results

    async def get_available_models(self, tenant_id: str) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = {
            "deepseek": ["deepseek-chat", "deepseek-coder"],
            "zhipu": ["glm-4-flash", "glm-4", "glm-4.6", "glm-4v", "glm-4-9b"],
            "openrouter": [
                "google/gemini-2.0-flash-exp",
                "google/gemini-2.5-flash",
                "anthropic/claude-3.5-sonnet",
                "openai/gpt-4o",
                "meta-llama/llama-3.2-90b-vision-instruct"
            ]
        }

        # æ ¹æ®å®é™…å¯ç”¨çš„æä¾›å•†è¿‡æ»¤
        available_providers = await self.validate_providers(tenant_id)
        return {
            provider: models_list
            for provider, models_list in models.items()
            if available_providers.get(provider, False)
        }

    def analyze_conversation_complexity(self, messages: List[LLMMessage]) -> Dict[str, Any]:
        """
        åˆ†æå¯¹è¯å¤æ‚åº¦ï¼Œç”¨äºæ™ºèƒ½å‚æ•°è°ƒæ•´

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

        Returns:
            Dict: å¤æ‚åº¦åˆ†æç»“æœ
        """
        try:
            total_chars = 0
            user_messages = 0
            question_marks = 0
            complex_words = 0

            complexity_words = [
                "åˆ†æ", "è§£é‡Š", "è¯„ä¼°", "æ¯”è¾ƒ", "è®¾è®¡", "è§„åˆ’", "ç­–ç•¥",
                "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åŸå› ", "å½±å“", "æ­¥éª¤", "åŸç†"
            ]

            for msg in messages:
                if msg.role == "user":
                    user_messages += 1
                    if isinstance(msg.content, str):
                        content = msg.content
                    elif isinstance(msg.content, list):
                        # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œåªè®¡ç®—æ–‡æœ¬éƒ¨åˆ†
                        content = ""
                        for item in msg.content:
                            if item.get("type") == "text":
                                content += item.get("text", "")
                    else:
                        content = str(msg.content)

                    total_chars += len(content)
                    question_marks += content.count("?") + content.count("ï¼Ÿ")

                    content_lower = content.lower()
                    for word in complexity_words:
                        if word in content_lower:
                            complex_words += 1

            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = (
                min(total_chars / 1000, 1.0) * 0.3 +  # å†…å®¹é•¿åº¦
                min(user_messages / 5, 1.0) * 0.2 +     # æ¶ˆæ¯æ•°é‡
                min(question_marks / 3, 1.0) * 0.3 +    # é—®é¢˜æ•°é‡
                min(complex_words / 5, 1.0) * 0.2       # å¤æ‚è¯æ±‡
            )

            return {
                "complexity_score": complexity_score,
                "total_chars": total_chars,
                "user_messages": user_messages,
                "question_marks": question_marks,
                "complex_words": complex_words,
                "recommend_thinking": complexity_score > 0.4,
                "recommend_temperature": 0.3 + complexity_score * 0.5
            }
        except Exception as e:
            logger.warning(f"å¯¹è¯å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            return {
                "complexity_score": 0.5,
                "recommend_thinking": False,
                "recommend_temperature": 0.7
            }


# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()