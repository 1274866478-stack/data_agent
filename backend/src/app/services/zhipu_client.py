"""
æ™ºè°± AI å®¢æˆ·ç«¯
GLM API è°ƒç”¨å°è£…
"""

import zhipuai
from zhipuai import ZhipuAI
from typing import Dict, Any, Optional, List, AsyncGenerator
import logging
import json
import time
import asyncio
import hashlib
import secrets
from datetime import datetime
from functools import wraps

from src.app.core.config import settings
from src.app.core.performance_optimizer import performance_monitor, resource_monitor
from src.app.core.security_monitor import security_monitor, SensitiveDataFilter

logger = logging.getLogger(__name__)


def retry_on_failure(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0,
                      circuit_breaker_threshold: int = 5):
    """
    å¢å¼ºé‡è¯•è£…é¥°å™¨ï¼ŒåŒ…å«ç†”æ–­æœºåˆ¶
    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :param delay: åˆå§‹å»¶è¿Ÿæ—¶é—´(ç§’)
    :param backoff: å»¶è¿Ÿå€æ•°
    :param circuit_breaker_threshold: ç†”æ–­å™¨é˜ˆå€¼
    """
    def decorator(func):
        # ç†”æ–­å™¨çŠ¶æ€
        failure_count = 0
        last_failure_time = None
        circuit_open = False
        reset_timeout = 60  # ç†”æ–­å™¨é‡ç½®æ—¶é—´(ç§’)

        @wraps(func)
        async def wrapper(*args, **kwargs):
            nonlocal failure_count, last_failure_time, circuit_open

            # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
            if circuit_open:
                if time.time() - last_failure_time > reset_timeout:
                    circuit_open = False
                    failure_count = 0
                    logger.info(f"ç†”æ–­å™¨é‡ç½®: {func.__name__}")
                else:
                    raise Exception(f"ç†”æ–­å™¨å¼€å¯: {func.__name__} æœåŠ¡æš‚æ—¶ä¸å¯ç”¨")

            try:
                # å¦‚æœæ˜¯åç¨‹å‡½æ•°ï¼Œéœ€è¦ await
                if asyncio.iscoroutinefunction(func):
                    result = await func(*args, **kwargs)
                else:
                    result = func(*args, **kwargs)

                # æˆåŠŸæ—¶é‡ç½®å¤±è´¥è®¡æ•°
                if failure_count > 0:
                    failure_count = 0
                    logger.info(f"æœåŠ¡æ¢å¤: {func.__name__}")

                return result

            except Exception as e:
                last_exception = e
                failure_count += 1
                last_failure_time = time.time()

                # è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
                error_type = type(e).__name__
                error_msg = str(e)

                # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
                safe_error_msg = error_msg
                if "api_key" in error_msg.lower():
                    safe_error_msg = "API key error (sensitive information filtered)"
                if any(keyword in error_msg.lower() for keyword in ["token", "secret", "password"]):
                    safe_error_msg = "Authentication error (sensitive information filtered)"

                logger.error(f"å‡½æ•° {func.__name__} ç¬¬ {failure_count} æ¬¡å¤±è´¥: {error_type} - {safe_error_msg}")

                # æ£€æŸ¥æ˜¯å¦è§¦å‘ç†”æ–­å™¨
                if failure_count >= circuit_breaker_threshold:
                    circuit_open = True
                    logger.error(f"ç†”æ–­å™¨å¼€å¯: {func.__name__} å¤±è´¥æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼ {circuit_breaker_threshold}")

                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if failure_count >= max_retries:
                    logger.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥")
                    raise

                # è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰
                base_wait = delay * (backoff ** (failure_count - 1))
                jitter = secrets.randbelow(1000) / 1000  # 0-1ç§’éšæœºæŠ–åŠ¨
                wait_time = base_wait + jitter

                logger.warning(f"å‡½æ•° {func.__name__} å°†åœ¨ {wait_time:.1f}ç§’åè¿›è¡Œç¬¬ {failure_count + 1} æ¬¡é‡è¯•")
                await asyncio.sleep(wait_time)

            # ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
            raise last_exception or Exception("Unknown error in retry decorator")

        return wrapper
    return decorator


class ZhipuAIService:
    """
    æ™ºè°± AI API æœåŠ¡ç±»
    å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒæµå¼å¤„ç†ã€æ€è€ƒæ¨¡å¼ã€æ™ºèƒ½å‚æ•°è°ƒæ•´å’Œæ€§èƒ½ç›‘æ§
    """

    def __init__(self):
        self.client = ZhipuAI(api_key=settings.zhipuai_api_key)
        self.default_model = getattr(settings, 'zhipuai_default_model', 'glm-4-flash')
        self.max_tokens = 4000
        self.temperature = 0.7
        self.rate_limit_delay = 0.1  # é€Ÿç‡é™åˆ¶å»¶è¿Ÿ(ç§’)

        # æ€§èƒ½ç›‘æ§
        self.request_count = 0
        self.success_count = 0
        self.error_count = 0
        self.total_response_time = 0
        self.last_performance_log = time.time()

        # ç®€å•å†…å­˜ç¼“å­˜
        self.cache = {}
        self.cache_max_size = 100
        self.cache_ttl = 300  # 5åˆ†é’Ÿ

        # æ€è€ƒæ¨¡å¼æŒ‡ç¤ºè¯
        self.thinking_indicators = [
            "åˆ†æ", "æ¯”è¾ƒ", "è¯„ä¼°", "è§£é‡Š", "è®¾è®¡", "è§„åˆ’",
            "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åŸå› ", "å½±å“", "å»ºè®®", "æ­¥éª¤",
            "åŸç†", "æœºåˆ¶", "ç­–ç•¥", "æ–¹æ¡ˆ", "ä¼˜ç¼ºç‚¹", "è¯¦ç»†è¯´æ˜"
        ]

    def _get_cache_key(self, model: str, messages: List[Dict[str, str]],
                      max_tokens: int, temperature: float) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = json.dumps([model, messages, max_tokens, temperature], sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜å“åº”"""
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if time.time() - cached_data['timestamp'] < self.cache_ttl:
                logger.debug(f"ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")
                return cached_data['response']
            else:
                del self.cache[cache_key]  # è¿‡æœŸåˆ é™¤
        return None

    def _cache_response(self, cache_key: str, response: Dict[str, Any]):
        """ç¼“å­˜å“åº”"""
        # LRUæ·˜æ±°ç­–ç•¥
        if len(self.cache) >= self.cache_max_size:
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]

        self.cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }

    def _log_performance_metrics(self, operation: str, duration: float, success: bool):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        self.request_count += 1
        if success:
            self.success_count += 1
        else:
            self.error_count += 1

        self.total_response_time += duration

        # æ¯30ç§’è®°å½•ä¸€æ¬¡æ€§èƒ½æ—¥å¿—
        if time.time() - self.last_performance_log > 30:
            avg_response_time = self.total_response_time / self.request_count if self.request_count > 0 else 0
            success_rate = self.success_count / self.request_count * 100 if self.request_count > 0 else 0

            logger.info(f"ZhipuAIæ€§èƒ½ç»Ÿè®¡ - æ€»è¯·æ±‚: {self.request_count}, "
                       f"æˆåŠŸç‡: {success_rate:.1f}%, "
                       f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s, "
                       f"ç¼“å­˜å¤§å°: {len(self.cache)}")

            # é‡ç½®ç»Ÿè®¡
            self.last_performance_log = time.time()

    async def check_connection(self) -> bool:
        """
        æ£€æŸ¥æ™ºè°±AI APIè¿æ¥çŠ¶æ€
        """
        try:
            logger.info("æ­£åœ¨æµ‹è¯•æ™ºè°±AI APIè¿æ¥...")

            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚æ¥éªŒè¯APIè¿æ¥
            response = await self._call_api_with_retry(
                lambda: self.client.chat.completions.create(
                    model=self.default_model,
                    messages=[
                        {"role": "user", "content": "Hello"}
                    ],
                    max_tokens=5
                )
            )

            if response and hasattr(response, 'choices') and response.choices:
                content = response.choices[0].message.content
                logger.info(f"ZhipuAI API connection: OK - Response: {content[:50]}...")
                return True
            else:
                logger.error("ZhipuAI API connection failed: Invalid response structure")
                return False
        except Exception as e:
            logger.error(f"ZhipuAI API connection failed: {e}")
            return False

    def should_enable_thinking(self, messages: List[Dict[str, str]]) -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦å»ºè®®å¯ç”¨æ€è€ƒæ¨¡å¼
        """
        try:
            # è·å–ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = ""
            for msg in messages:
                if msg.get("role") == "user":
                    user_content += msg.get("content", "") + " "

            user_content = user_content.strip()
            if not user_content:
                return False

            # æ£€æŸ¥æ€è€ƒæ¨¡å¼æŒ‡ç¤ºè¯
            content_lower = user_content.lower()
            for indicator in self.thinking_indicators:
                if indicator in content_lower:
                    logger.debug(f"æ£€æµ‹åˆ°æ€è€ƒæ¨¡å¼æŒ‡ç¤ºè¯: {indicator}")
                    return True

            # æ£€æŸ¥é—®é¢˜å¤æ‚åº¦
            if len(user_content) > 200:  # è¾ƒé•¿çš„é—®é¢˜
                return True
            if "ï¼Ÿ" in user_content or user_content.count("?") > 1:  # å¤šä¸ªé—®å·
                return True
            if "æ­¥éª¤" in user_content or "è¯¦ç»†" in user_content:  # éœ€è¦è¯¦ç»†å›ç­”
                return True

            return False
        except Exception as e:
            logger.warning(f"æ€è€ƒæ¨¡å¼åˆ¤æ–­å¤±è´¥: {e}")
            return False

    @retry_on_failure(max_retries=3, delay=0.5)
    async def stream_chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        enable_thinking: Optional[bool] = None,
        show_thinking: bool = True
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼èŠå¤©å®Œæˆï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokens
            temperature: æ¸©åº¦å‚æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨åˆ¤æ–­
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹

        Yields:
            Dict: æµå¼å“åº”å—
        """
        try:
            model = model or self.default_model
            max_tokens = max_tokens or self.max_tokens
            temperature = temperature if temperature is not None else self.temperature

            # æ™ºèƒ½æ€è€ƒæ¨¡å¼åˆ¤æ–­
            if enable_thinking is None:
                enable_thinking = self.should_enable_thinking(messages)
                if enable_thinking:
                    logger.info("æ™ºèƒ½å¯ç”¨æ™ºè°±AIæ€è€ƒæ¨¡å¼")

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True
            }

            # å¯ç”¨æ€è€ƒæ¨¡å¼
            if enable_thinking:
                params["thinking"] = {"type": "enabled"}
                logger.debug(f"æ™ºè°±AIå¯ç”¨æ€è€ƒæ¨¡å¼: {model}")

            logger.info(f"å¼€å§‹æ™ºè°±AIæµå¼è°ƒç”¨: {model}, æ€è€ƒæ¨¡å¼: {enable_thinking}")

            # å‘èµ·æµå¼è¯·æ±‚
            response = self.client.chat.completions.create(**params)

            thinking_started = False
            content_started = False

            # å¤„ç†æµå¼å“åº”
            for chunk in response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta

                    # è¾“å‡ºæ€è€ƒè¿‡ç¨‹
                    if show_thinking and hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        if not thinking_started:
                            thinking_started = True
                            logger.debug("æ™ºè°±AIæ€è€ƒè¿‡ç¨‹å¼€å§‹")

                        yield {
                            'type': 'thinking',
                            'content': delta.reasoning_content,
                            'model': model,
                            'finished': False
                        }

                    # è¾“å‡ºæ­£å¼å›å¤å†…å®¹
                    elif hasattr(delta, 'content') and delta.content:
                        if not content_started and thinking_started:
                            content_started = True
                            logger.debug("æ™ºè°±AIä»æ€è€ƒè¿‡ç¨‹åˆ‡æ¢åˆ°æ­£å¼å›å¤")

                        yield {
                            'type': 'content',
                            'content': delta.content,
                            'model': model,
                            'finished': False
                        }

            # å‘é€å®Œæˆä¿¡å·
            yield {
                'type': 'content',
                'content': "",
                'model': model,
                'finished': True
            }

            logger.info("æ™ºè°±AIæµå¼è°ƒç”¨å®Œæˆ")

        except Exception as e:
            logger.error(f"æ™ºè°±AIæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield {
                'type': 'error',
                'content': f"æ™ºè°±AIè°ƒç”¨å‡ºé”™: {str(e)}",
                'model': model or self.default_model,
                'finished': True
            }

    async def _call_api_with_retry(self, api_call_func, max_retries: int = 3):
        """
        å¸¦é‡è¯•çš„APIè°ƒç”¨
        """
        last_exception = None

        for attempt in range(max_retries):
            try:
                # æ·»åŠ é€Ÿç‡é™åˆ¶å»¶è¿Ÿ
                if attempt > 0:
                    await asyncio.sleep(self.rate_limit_delay * (2 ** attempt))

                result = api_call_func()
                return result

            except Exception as e:
                last_exception = e

                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries - 1:
                    logger.error(f"APIè°ƒç”¨åœ¨ {max_retries} æ¬¡é‡è¯•åå¤±è´¥: {e}")
                    raise

                # è®°å½•é‡è¯•ä¿¡æ¯
                logger.warning(f"APIè°ƒç”¨ç¬¬ {attempt + 1} æ¬¡å¤±è´¥: {e}ï¼Œæ­£åœ¨é‡è¯•...")

        raise last_exception

    @performance_monitor("zhipu_chat_completion")
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        enable_cache: bool = True,
        skip_security_check: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨æ™ºè°±AIèŠå¤©å®ŒæˆAPI
        å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

        Args:
            skip_security_check: è·³è¿‡å®‰å…¨æ£€æŸ¥ï¼ˆä»…ç”¨äºå†…éƒ¨è°ƒç”¨å¦‚SQLä¿®å¤ï¼‰
        """
        start_time = time.time()
        operation = "chat_completion"

        # å®‰å…¨æ£€æŸ¥ï¼ˆå†…éƒ¨è°ƒç”¨å¯ä»¥è·³è¿‡ï¼‰
        if not skip_security_check and not security_monitor.check_request_security(
            {"messages": messages, "model": model, "stream": stream},
            source_ip=getattr(self, '_source_ip', None)
        ):
            logger.warning("å®‰å…¨æ£€æŸ¥å¤±è´¥ï¼Œæ‹’ç»è¯·æ±‚")
            return None

        try:
            model = model or self.default_model
            max_tokens = max_tokens or self.max_tokens
            temperature = temperature if temperature is not None else self.temperature

            # ç”Ÿæˆç¼“å­˜é”®ï¼ˆä»…å¯¹éæµå¼è¯·æ±‚å¯ç”¨ç¼“å­˜ï¼‰
            cache_key = None
            if not stream and enable_cache:
                cache_key = self._get_cache_key(model, messages, max_tokens, temperature)
                cached_response = self._get_cached_response(cache_key)
                if cached_response:
                    duration = time.time() - start_time
                    self._log_performance_metrics(operation + "_cache_hit", duration, True)
                    return cached_response

            logger.info(f"è°ƒç”¨æ™ºè°±AI API - Model: {model}, Messages: {len(messages)}, Stream: {stream}, Cache: {enable_cache}")

            response = await self._call_api_with_retry(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=stream
                )
            )

            if stream:
                duration = time.time() - start_time
                self._log_performance_metrics(operation + "_stream", duration, True)
                return response  # æµå¼å“åº”
            else:
                # éæµå¼å“åº”ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®
                result = {
                    "content": response.choices[0].message.content or "",
                    "role": response.choices[0].message.role,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    } if response.usage else {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                    "model": response.model,
                    "created_at": datetime.fromtimestamp(response.created).isoformat(),
                    "finish_reason": response.choices[0].finish_reason
                }

                # ç¼“å­˜å“åº”ï¼ˆå¦‚æœå¯ç”¨ä¸”å“åº”ä¸ä¸ºç©ºï¼‰
                if enable_cache and cache_key and result["content"].strip():
                    self._cache_response(cache_key, result)

                duration = time.time() - start_time
                self._log_performance_metrics(operation, duration, True)

                logger.info(f"æ™ºè°±AIè°ƒç”¨æˆåŠŸ - Tokens: {result['usage']['total_tokens']}, è€—æ—¶: {duration:.2f}s")
                return result

        except Exception as e:
            duration = time.time() - start_time
            self._log_performance_metrics(operation, duration, False)

            # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯åçš„é”™è¯¯æ—¥å¿—
            safe_error = str(e)
            if "api_key" in safe_error.lower():
                safe_error = "API authentication error (sensitive information filtered)"

            logger.error(f"Chat completion failed ({duration:.2f}s): {safe_error}")
            return None

    async def embedding(
        self,
        texts: List[str],
        model: str = "embedding-2"
    ) -> Optional[List[List[float]]]:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        """
        try:
            logger.info(f"ç”Ÿæˆæ–‡æœ¬åµŒå…¥ - æ–‡æœ¬æ•°é‡: {len(texts)}, æ¨¡å‹: {model}")

            response = await self._call_api_with_retry(
                lambda: self.client.embeddings.create(
                    model=model,
                    input=texts
                )
            )

            embeddings = [data.embedding for data in response.data]
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡")
            return embeddings
        except Exception as e:
            logger.error(f"åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def semantic_search(
        self,
        query: str,
        documents: List[str],
        top_k: int = 5
    ) -> Optional[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢
        """
        try:
            if not documents:
                return {"query": query, "results": [], "total_documents": 0}

            logger.info(f"è¯­ä¹‰æœç´¢ - æŸ¥è¯¢: {query[:50]}..., æ–‡æ¡£æ•°: {len(documents)}")

            # ä¸ºæŸ¥è¯¢å’Œæ–‡æ¡£ç”ŸæˆåµŒå…¥
            all_texts = [query] + documents
            embeddings = await self.embedding(all_texts)

            if not embeddings:
                return None

            query_embedding = embeddings[0]
            doc_embeddings = embeddings[1:]

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            try:
                from sklearn.metrics.pairwise import cosine_similarity
                import numpy as np

                similarities = cosine_similarity(
                    [query_embedding],
                    doc_embeddings
                )[0]

                # è·å–top_kç»“æœ
                top_indices = np.argsort(similarities)[::-1][:top_k]

                results = []
                for idx in top_indices:
                    results.append({
                        "document": documents[idx],
                        "similarity": float(similarities[idx]),
                        "index": int(idx)
                    })

                return {
                    "query": query,
                    "results": results,
                    "total_documents": len(documents)
                }
            except ImportError:
                logger.warning("sklearn å’Œ numpy ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—")
                # ç®€åŒ–çš„ç‚¹ç§¯ç›¸ä¼¼åº¦è®¡ç®—
                import math

                results = []
                for i, doc_embedding in enumerate(doc_embeddings):
                    # è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
                    similarity = sum(q * d for q, d in zip(query_embedding, doc_embedding))
                    similarity = similarity / (
                        math.sqrt(sum(q * q for q in query_embedding)) *
                        math.sqrt(sum(d * d for d in doc_embedding))
                    )
                    results.append({
                        "document": documents[i],
                        "similarity": float(similarity),
                        "index": i
                    })

                # æ’åºå¹¶å–top_k
                results.sort(key=lambda x: x["similarity"], reverse=True)
                results = results[:top_k]

                return {
                    "query": query,
                    "results": results,
                    "total_documents": len(documents)
                }

        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return None

    def analyze_conversation_complexity(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        åˆ†æå¯¹è¯å¤æ‚åº¦ï¼Œç”¨äºæ™ºèƒ½å‚æ•°è°ƒæ•´

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

        Returns:
            Dict: å¤æ‚åº¦åˆ†æç»“æœ
        """
        try:
            total_chars = 0
            user_messages = 0
            question_marks = 0
            complex_words = 0

            complexity_words = [
                "åˆ†æ", "è§£é‡Š", "è¯„ä¼°", "æ¯”è¾ƒ", "è®¾è®¡", "è§„åˆ’", "ç­–ç•¥",
                "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åŸå› ", "å½±å“", "æ­¥éª¤", "åŸç†",
                "æœºåˆ¶", "æ–¹æ¡ˆ", "ä¼˜ç¼ºç‚¹", "è¯¦ç»†è¯´æ˜"
            ]

            for msg in messages:
                if msg.get("role") == "user":
                    user_messages += 1
                    content = msg.get("content", "")

                    total_chars += len(content)
                    question_marks += content.count("?") + content.count("ï¼Ÿ")

                    content_lower = content.lower()
                    for word in complexity_words:
                        if word in content_lower:
                            complex_words += 1

            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = (
                min(total_chars / 1000, 1.0) * 0.3 +  # å†…å®¹é•¿åº¦
                min(user_messages / 5, 1.0) * 0.2 +     # æ¶ˆæ¯æ•°é‡
                min(question_marks / 3, 1.0) * 0.3 +    # é—®é¢˜æ•°é‡
                min(complex_words / 5, 1.0) * 0.2       # å¤æ‚è¯æ±‡
            )

            return {
                "complexity_score": round(complexity_score, 3),
                "total_chars": total_chars,
                "user_messages": user_messages,
                "question_marks": question_marks,
                "complex_words": complex_words,
                "recommend_thinking": complexity_score > 0.4,
                "recommend_temperature": round(0.3 + complexity_score * 0.5, 2),
                "recommend_max_tokens": min(2000 + int(complexity_score * 6000), 8000)
            }
        except Exception as e:
            logger.warning(f"å¯¹è¯å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            return {
                "complexity_score": 0.5,
                "recommend_thinking": False,
                "recommend_temperature": 0.7,
                "recommend_max_tokens": 4000
            }

    async def smart_chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        auto_adjust_params: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        æ™ºèƒ½èŠå¤©å®Œæˆï¼Œè‡ªåŠ¨è°ƒæ•´å‚æ•°

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            auto_adjust_params: æ˜¯å¦è‡ªåŠ¨è°ƒæ•´å‚æ•°

        Returns:
            å“åº”ç»“æœ
        """
        try:
            # åˆ†æå¯¹è¯å¤æ‚åº¦
            complexity_analysis = self.analyze_conversation_complexity(messages)
            logger.info(f"å¯¹è¯å¤æ‚åº¦åˆ†æ: {complexity_analysis}")

            # è‡ªåŠ¨è°ƒæ•´å‚æ•°
            if auto_adjust_params:
                temperature = complexity_analysis.get("recommend_temperature", self.temperature)
                max_tokens = complexity_analysis.get("recommend_max_tokens", self.max_tokens)
                enable_thinking = complexity_analysis.get("recommend_thinking", False)
            else:
                temperature = self.temperature
                max_tokens = self.max_tokens
                enable_thinking = False

            return await self.chat_completion(
                messages=messages,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                enable_thinking=enable_thinking
            )
        except Exception as e:
            logger.error(f"æ™ºèƒ½èŠå¤©å®Œæˆå¤±è´¥: {e}")
            return None

    async def text_analysis(
        self,
        text: str,
        analysis_type: str = "summary"
    ) -> Optional[str]:
        """
        æ–‡æœ¬åˆ†æï¼ˆæ‘˜è¦ã€å…³é”®è¯æå–ç­‰ï¼‰
        """
        try:
            if analysis_type == "summary":
                prompt = f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š\n\n{text}"
            elif analysis_type == "keywords":
                prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–5-10ä¸ªå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼š\n\n{text}"
            elif analysis_type == "sentiment":
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºè¯„åˆ†ï¼ˆ0-100ï¼‰ï¼š\n\n{text}"
            elif analysis_type == "topics":
                prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«ä¸»è¦ä¸»é¢˜ï¼Œç”¨åˆ—è¡¨å½¢å¼è¾“å‡ºï¼š\n\n{text}"
            else:
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š\n\n{text}"

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æå¸ˆï¼Œè¯·å‡†ç¡®ã€ç®€æ´åœ°å®Œæˆåˆ†æä»»åŠ¡ã€‚"},
                {"role": "user", "content": prompt}
            ]

            response = await self.chat_completion(
                messages=messages,
                max_tokens=500,
                temperature=0.3
            )

            if response:
                return response["content"]
            else:
                return None
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†æå¤±è´¥: {e}")
            return None

    async def validate_api_key(self) -> bool:
        """
        éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
        """
        return await self.check_connection()

    async def generate_sql_from_natural_language(
        self,
        query: str,
        schema: str,
        db_type: str = "postgresql"
    ) -> Optional[str]:
        """
        å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥

        Args:
            query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            schema: æ•°æ®åº“schemaä¿¡æ¯
            db_type: æ•°æ®åº“ç±»å‹ (postgresql, mysql, etc.)

        Returns:
            ç”Ÿæˆçš„SQLè¯­å¥æˆ–None
        """
        try:
            logger.info(f"ç”ŸæˆSQLæŸ¥è¯¢ - æŸ¥è¯¢: {query[:100]}..., æ•°æ®åº“ç±»å‹: {db_type}")

            # æ„å»ºSQLç”Ÿæˆä¸“ç”¨prompt
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLæŸ¥è¯¢ç”Ÿæˆå™¨ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œæä¾›çš„æ•°æ®åº“schemaï¼Œç”Ÿæˆå‡†ç¡®çš„SQLè¯­å¥ã€‚

è¦æ±‚ï¼š
1. åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–è¯´æ˜
2. ä½¿ç”¨{db_type}è¯­æ³•
3. ç¡®ä¿ç”Ÿæˆçš„SQLå®‰å…¨ä¸”é«˜æ•ˆ
4. åªä½¿ç”¨SELECTæŸ¥è¯¢ï¼Œç¦æ­¢UPDATEã€DELETEã€DROPç­‰å±é™©æ“ä½œ
5. å¤„ç†NULLå€¼å’Œå­—ç¬¦ä¸²è½¬ä¹‰
6. ä½¿ç”¨é€‚å½“çš„LIMITå­å¥é™åˆ¶ç»“æœæ•°é‡
7. ğŸ”´æå€¼æŸ¥è¯¢å¿…é¡»ä½¿ç”¨ LIMIT 1ï¼šå½“ç”¨æˆ·é—®"æœ€å¤§"ã€"æœ€å°"ã€"æœ€é•¿"ã€"æœ€çŸ­"ã€"æœ€é«˜"ã€"æœ€ä½"ã€"ç¬¬ä¸€ä¸ª"ã€"æœ€æ—©"ã€"æœ€æ™š"ç­‰æå€¼é—®é¢˜æ—¶ï¼Œå¿…é¡»ç”¨ ORDER BY + LIMIT 1 åªè¿”å›ä¸€æ¡è®°å½•
   - ä¾‹å¦‚ï¼š"è°å·¥ä½œæ—¶é—´æœ€é•¿"â†’ `ORDER BY hire_date ASC LIMIT 1`
   - ä¾‹å¦‚ï¼š"è°è–ªèµ„æœ€é«˜"â†’ `ORDER BY salary DESC LIMIT 1`

æ•°æ®åº“Schema:
{schema}"""

            user_prompt = f"""è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”ŸæˆSQLè¯­å¥ï¼š
{query}"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = await self.chat_completion(
                messages=messages,
                max_tokens=1000,
                temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            )

            if response and response.get("content"):
                sql_query = response["content"].strip()

                # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                if sql_query.startswith("```sql"):
                    sql_query = sql_query[6:]
                if sql_query.startswith("```"):
                    sql_query = sql_query[3:]
                if sql_query.endswith("```"):
                    sql_query = sql_query[:-3]

                sql_query = sql_query.strip()

                # åŸºæœ¬å®‰å…¨æ£€æŸ¥
                dangerous_keywords = ["DROP", "DELETE", "UPDATE", "INSERT", "ALTER", "CREATE", "TRUNCATE"]
                sql_upper = sql_query.upper()

                for keyword in dangerous_keywords:
                    if keyword in sql_upper:
                        logger.warning(f"æ£€æµ‹åˆ°å±é™©SQLå…³é”®è¯: {keyword}")
                        return None

                logger.info(f"æˆåŠŸç”ŸæˆSQLæŸ¥è¯¢: {sql_query[:100]}...")
                return sql_query
            else:
                logger.error("ç”ŸæˆSQLæŸ¥è¯¢å¤±è´¥: æ— å“åº”å†…å®¹")
                return None

        except Exception as e:
            logger.error(f"ç”ŸæˆSQLæŸ¥è¯¢å¤±è´¥: {e}")
            return None

    async def explain_query_result(
        self,
        query: str,
        sql: str,
        result_data: Dict[str, Any]
    ) -> Optional[str]:
        """
        è§£é‡ŠæŸ¥è¯¢ç»“æœï¼Œæä¾›è‡ªç„¶è¯­è¨€æè¿°

        Args:
            query: åŸå§‹è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            sql: æ‰§è¡Œçš„SQLè¯­å¥
            result_data: æŸ¥è¯¢ç»“æœæ•°æ®

        Returns:
            è‡ªç„¶è¯­è¨€è§£é‡Šæˆ–None
        """
        try:
            logger.info(f"è§£é‡ŠæŸ¥è¯¢ç»“æœ - åŸæŸ¥è¯¢: {query[:50]}...")

            # æ„å»ºç»“æœè§£é‡Šprompt
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢å’ŒæŸ¥è¯¢ç»“æœï¼Œæä¾›æ¸…æ™°ã€æœ‰æ´å¯ŸåŠ›çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚

è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜
2. æä¾›å…³é”®æ•°æ®ç‚¹çš„è§£é‡Š
3. å¦‚æœé€‚ç”¨ï¼Œæä¾›è¶‹åŠ¿æˆ–æ¨¡å¼åˆ†æ
4. è¯­è¨€ç®€æ´æ˜äº†ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­
5. å¦‚æœç»“æœä¸ºç©ºï¼Œè§£é‡Šå¯èƒ½çš„åŸå› """

            result_summary = {
                "row_count": len(result_data.get("data", [])),
                "columns": result_data.get("columns", []),
                "sample_data": result_data.get("data", [])[:5] if result_data.get("data") else []
            }

            user_prompt = f"""ç”¨æˆ·æŸ¥è¯¢: {query}
æ‰§è¡Œçš„SQL: {sql}
æŸ¥è¯¢ç»“æœ: {result_summary}

è¯·è§£é‡Šè¿™ä¸ªæŸ¥è¯¢ç»“æœã€‚"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = await self.chat_completion(
                messages=messages,
                max_tokens=500,
                temperature=0.3
            )

            if response and response.get("content"):
                explanation = response["content"].strip()
                logger.info(f"æˆåŠŸç”ŸæˆæŸ¥è¯¢è§£é‡Š: {explanation[:100]}...")
                return explanation
            else:
                logger.error("ç”ŸæˆæŸ¥è¯¢è§£é‡Šå¤±è´¥: æ— å“åº”å†…å®¹")
                return None

        except Exception as e:
            logger.error(f"ç”ŸæˆæŸ¥è¯¢è§£é‡Šå¤±è´¥: {e}")
            return None

    async def get_model_info(self, model: str = None) -> Optional[Dict[str, Any]]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        """
        try:
            model = model or self.default_model

            # é€šè¿‡ç®€å•çš„è°ƒç”¨æ¥æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§
            test_response = await self.chat_completion(
                messages=[
                    {"role": "user", "content": "æµ‹è¯•"}
                ],
                model=model,
                max_tokens=10
            )

            if test_response:
                return {
                    "model": model,
                    "status": "available",
                    "max_tokens": self.max_tokens,
                    "supports_streaming": True,
                    "supports_functions": True  # å‡è®¾æ”¯æŒå‡½æ•°è°ƒç”¨
                }
            else:
                return {
                    "model": model,
                    "status": "unavailable",
                    "error": "Model test failed"
                }

        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "model": model or self.default_model,
                "status": "error",
                "error": str(e)
            }


# å…¨å±€æ™ºè°±AIæœåŠ¡å®ä¾‹
zhipu_service = ZhipuAIService()