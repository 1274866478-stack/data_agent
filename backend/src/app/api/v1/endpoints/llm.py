"""
# [LLM] LLMæœåŠ¡APIç«¯ç‚¹

## [HEADER]
**æ–‡ä»¶å**: llm.py
**èŒè´£**: æä¾›ç»Ÿä¸€çš„LLMèŠå¤©å®Œæˆã€æµå¼è¾“å‡ºã€SQLæŸ¥è¯¢å’Œå¤šæ¨¡æ€æ”¯æŒAPIï¼Œé›†æˆæ™ºè°±AIå’ŒDeepSeekæœåŠ¡ï¼Œæ”¯æŒæ•°æ®æºè¿æ¥å’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œç¡®ä¿ç§Ÿæˆ·éš”ç¦»
**ä½œè€…**: Data Agent Team
**ç‰ˆæœ¬**: 1.0.0
**å˜æ›´è®°å½•**:
- v1.0.0 (2026-01-01): åˆå§‹ç‰ˆæœ¬ - å®ç°LLMæœåŠ¡APIç«¯ç‚¹

## [INPUT]
- **tenant: Tenant** - ç§Ÿæˆ·å¯¹è±¡ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥è·å–ï¼‰
- **user_with_tenant: dict** - ç”¨æˆ·å’Œç§Ÿæˆ·ä¿¡æ¯ï¼ˆä»JWT tokenä¸­æå–ï¼‰
- **chat_request: ChatRequest** - èŠå¤©è¯·æ±‚æ¨¡å‹ï¼ˆPydanticæ¨¡å‹ï¼‰
  - messages: æ¶ˆæ¯åˆ—è¡¨
  - model: æ¨¡å‹åç§°
  - temperature: æ¸©åº¦å‚æ•°
  - max_tokens: æœ€å¤§tokenæ•°
  - stream: æ˜¯å¦æµå¼è¾“å‡º
- **sql_query_request: SQLQueryRequest** - SQLæŸ¥è¯¢è¯·æ±‚æ¨¡å‹
  - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
  - connection_id: æ•°æ®æºè¿æ¥ID
  - data_source_id: æ•°æ®æºID
- **data_source_id: str** - æ•°æ®æºIDï¼ˆè·¯å¾„å‚æ•°ï¼‰
- **db: Session** - æ•°æ®åº“ä¼šè¯ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥è·å–ï¼‰

## [OUTPUT]
- **chat_response: LLMResponse** - LLMèŠå¤©å“åº”
  - content: ç”Ÿæˆå†…å®¹
  - role: è§’è‰²åç§°
  - usage: tokenä½¿ç”¨ç»Ÿè®¡
  - model: æ¨¡å‹åç§°
- **stream_response: StreamingResponse** - æµå¼å“åº”
  - åˆ†å—è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
- **sql_query_result: dict** - SQLæŸ¥è¯¢ç»“æœ
  - success: æŸ¥è¯¢æ˜¯å¦æˆåŠŸ
  - sql: ç”Ÿæˆçš„SQLè¯­å¥
  - results: æŸ¥è¯¢ç»“æœæ•°æ®
  - row_count: ç»“æœè¡Œæ•°
  - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
- **data_source_info: dict** - æ•°æ®æºä¿¡æ¯
  - id: æ•°æ®æºID
  - name: æ•°æ®æºåç§°
  - db_type: æ•°æ®åº“ç±»å‹
  - status: è¿æ¥çŠ¶æ€
- **error_response: HTTPException** - é”™è¯¯å“åº”ï¼ˆ400, 404, 500ï¼‰

## [LINK]
**ä¸Šæ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- [../../data/database.py](../../data/database.py) - get_db(), Session
- [../../data/models.py](../../data/models.py) - Tenant, DataSourceConnection, DataSourceConnectionStatus
- [../../services/llm_service.py](../../services/llm_service.py) - llm_service, LLMProvider, LLMMessage, LLMResponse
- [../../services/data_source_service.py](../../services/data_source_service.py) - data_source_service, æ•°æ®æºæœåŠ¡
- [../../services/minio_client.py](../../services/minio_client.py) - minio_service, å¯¹è±¡å­˜å‚¨
- [../../services/zhipu_client.py](../../services/zhipu_client.py) - zhipu_service, æ™ºè°±AIæœåŠ¡
- [../../services/database_interface.py](../../services/database_interface.py) - PostgreSQLAdapter, æ•°æ®åº“é€‚é…å™¨
- [../../core/auth.py](../../core/auth.py) - get_current_user_with_tenant, ç”¨æˆ·è®¤è¯

**ä¸‹æ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- æ— ï¼ˆAPIç«¯ç‚¹æ˜¯å¶å­æ¨¡å—ï¼‰

**è°ƒç”¨æ–¹**:
- å‰ç«¯èŠå¤©ç•Œé¢ - LLMå¯¹è¯äº¤äº’
- å‰ç«¯SQLæŸ¥è¯¢å·¥å…· - è‡ªç„¶è¯­è¨€ç”ŸæˆSQL
- å‰ç«¯æ•°æ®åˆ†ææ¨¡å— - æ™ºèƒ½æ•°æ®æŸ¥è¯¢
- æµå¼å“åº”å®¢æˆ·ç«¯ - å®æ—¶æ–‡æœ¬ç”Ÿæˆ

## [POS]
**è·¯å¾„**: backend/src/app/api/v1/endpoints/llm.py
**æ¨¡å—å±‚çº§**: Level 3 - APIç«¯ç‚¹å±‚
**ä¾èµ–æ·±åº¦**: ç›´æ¥ä¾èµ– data/*, services/*, core/*ï¼›è¢«å‰ç«¯èŠå¤©å’ŒæŸ¥è¯¢æ¨¡å—è°ƒç”¨
"""

import json
import asyncio
import logging
import io
import os
import sys
import time
from datetime import datetime, date
from decimal import Decimal
from typing import Dict, Any, Optional, List, Union
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import pandas as pd

from src.app.services.llm_service import (
    llm_service,
    LLMProvider,
    LLMMessage,
    LLMResponse,
    LLMStreamChunk
)
from src.app.core.auth import get_current_user_with_tenant
from src.app.data.models import Tenant, DataSourceConnection, DataSourceConnectionStatus
from src.app.data.database import get_db
from src.app.services.data_source_service import data_source_service
from src.app.services.minio_client import minio_service
from src.app.services.database_interface import PostgreSQLAdapter
from src.app.services.zhipu_client import zhipu_service
from src.app.services.sql_error_memory_service import SQLErrorMemoryService
import re
import duckdb

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["LLM"])


def _strip_sql_comments_and_check_select(sql: str) -> tuple[str, bool, str]:
    """
    å»é™¤SQLå¼€å¤´çš„æ³¨é‡Šï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æ˜¯SELECT/WITHæŸ¥è¯¢
    
    Args:
        sql: åŸå§‹SQLæŸ¥è¯¢
    
    Returns:
        tuple: (å»é™¤æ³¨é‡Šåçš„SQL, æ˜¯å¦ä¸ºSELECTæŸ¥è¯¢, è°ƒè¯•ä¿¡æ¯)
    """
    sql_for_check = sql.strip()
    original_len = len(sql_for_check)
    debug_info = []
    
    # å¾ªç¯å»é™¤å¼€å¤´çš„æ³¨é‡Šï¼ˆå•è¡Œå’Œå¤šè¡Œéƒ½è¦å¤„ç†ï¼‰
    max_iterations = 100  # é˜²æ­¢æ— é™å¾ªç¯
    iteration = 0
    
    while iteration < max_iterations:
        iteration += 1
        made_change = False
        
        # å»é™¤å•è¡Œæ³¨é‡Š (-- ...)
        while sql_for_check.startswith('--'):
            newline_pos = sql_for_check.find('\n')
            if newline_pos != -1:
                removed = sql_for_check[:newline_pos + 1]
                sql_for_check = sql_for_check[newline_pos + 1:].strip()
                debug_info.append(f"å»é™¤å•è¡Œæ³¨é‡Š: {repr(removed[:30])}")
                made_change = True
            else:
                # æ•´è¡Œéƒ½æ˜¯æ³¨é‡Šï¼Œæ²¡æœ‰æ¢è¡Œç¬¦ï¼Œè¯´æ˜SQLåªæ˜¯ä¸€ä¸ªæ³¨é‡Š
                debug_info.append(f"SQLåªæ˜¯æ³¨é‡Š: {repr(sql_for_check[:50])}")
                break
        
        # å»é™¤å¤šè¡Œæ³¨é‡Š (/* ... */)
        while sql_for_check.startswith('/*'):
            end_pos = sql_for_check.find('*/')
            if end_pos != -1:
                removed = sql_for_check[:end_pos + 2]
                sql_for_check = sql_for_check[end_pos + 2:].strip()
                debug_info.append(f"å»é™¤å¤šè¡Œæ³¨é‡Š: {repr(removed[:30])}")
                made_change = True
            else:
                debug_info.append("æœªé—­åˆçš„å¤šè¡Œæ³¨é‡Š")
                break
        
        # å¦‚æœè¿™æ¬¡å¾ªç¯æ²¡æœ‰å˜åŒ–ï¼Œé€€å‡º
        if not made_change:
            break
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯SELECTæŸ¥è¯¢
    sql_upper = sql_for_check.upper().strip()
    is_select = sql_upper.startswith('SELECT') or sql_upper.startswith('WITH')
    
    debug_msg = f"åŸå§‹é•¿åº¦={original_len}, å¤„ç†åé•¿åº¦={len(sql_for_check)}, " \
                f"è¿­ä»£æ¬¡æ•°={iteration}, æ˜¯SELECT={is_select}"
    if debug_info:
        debug_msg += f", å¤„ç†è¿‡ç¨‹: {'; '.join(debug_info)}"
    debug_msg += f", å¤„ç†åå‰50å­—ç¬¦: {repr(sql_for_check[:50])}"
    
    return sql_for_check, is_select, debug_msg


def _split_multiple_sql_statements(sql_block: str) -> List[str]:
    """
    æ‹†åˆ†ä¸€ä¸ªSQLä»£ç å—ä¸­å¯èƒ½åŒ…å«çš„å¤šä¸ªSQLè¯­å¥
    
    AIæœ‰æ—¶ä¼šåœ¨ä¸€ä¸ªä»£ç å—ä¸­è¿”å›å¤šä¸ªç”¨åˆ†å·åˆ†éš”çš„SQLè¯­å¥ï¼Œ
    PostgreSQLçš„prepared statementä¸æ”¯æŒåŒæ—¶æ‰§è¡Œå¤šä¸ªå‘½ä»¤ï¼Œ
    å› æ­¤éœ€è¦æ‹†åˆ†åé€ä¸ªæ‰§è¡Œã€‚
    
    Args:
        sql_block: å¯èƒ½åŒ…å«å¤šä¸ªSQLè¯­å¥çš„ä»£ç å—å†…å®¹
        
    Returns:
        List[str]: æ‹†åˆ†åçš„SQLè¯­å¥åˆ—è¡¨ï¼ˆè¿‡æ»¤æ‰ç©ºè¯­å¥å’Œçº¯æ³¨é‡Šï¼‰
    """
    # æŒ‰åˆ†å·æ‹†åˆ†ï¼Œä½†è¦æ³¨æ„åˆ†å·å¯èƒ½å‡ºç°åœ¨å­—ç¬¦ä¸²å†…éƒ¨
    # ä½¿ç”¨ç®€å•ç­–ç•¥ï¼šæŒ‰åˆ†å·æ‹†åˆ†ï¼Œç„¶åè¿‡æ»¤ç©ºè¯­å¥
    statements = []
    
    # é¦–å…ˆå°è¯•æŒ‰åˆ†å·åˆ†éš”
    raw_statements = sql_block.split(';')
    
    for stmt in raw_statements:
        stmt = stmt.strip()
        if not stmt:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åªæ˜¯æ³¨é‡Šï¼ˆå»é™¤æ³¨é‡Šåæ˜¯å¦è¿˜æœ‰å†…å®¹ï¼‰
        sql_cleaned, is_select, _ = _strip_sql_comments_and_check_select(stmt)
        
        # å¦‚æœå»é™¤æ³¨é‡Šåè¿˜æœ‰å†…å®¹ï¼Œä¸”æ˜¯SELECT/WITHæŸ¥è¯¢ï¼Œä¿ç•™å®ƒ
        if sql_cleaned and is_select:
            statements.append(stmt)
    
    # å¦‚æœæ²¡æœ‰æ‹†åˆ†å‡ºä»»ä½•æœ‰æ•ˆè¯­å¥ï¼Œä½†åŸå§‹å—éç©ºï¼Œå¯èƒ½æ˜¯æ²¡æœ‰åˆ†å·çš„å•ä¸ªæŸ¥è¯¢
    if not statements and sql_block.strip():
        sql_cleaned, is_select, _ = _strip_sql_comments_and_check_select(sql_block)
        if sql_cleaned and is_select:
            statements.append(sql_block.strip())
    
    return statements


def _remove_database_name_prefix(sql: str, database_name: str) -> str:
    """
    å»é™¤SQLä¸­å¤šä½™çš„æ•°æ®åº“åå‰ç¼€
    
    PostgreSQLä¸æ”¯æŒè·¨æ•°æ®åº“å¼•ç”¨ï¼Œå½“å·²è¿æ¥åˆ°æ•°æ®åº“æ—¶ï¼Œ
    SQLä¸­ä¸åº”è¯¥åŒ…å« "æ•°æ®åº“å.schema.è¡¨å" è¿™æ ·çš„æ ¼å¼ã€‚
    AIæœ‰æ—¶ä¼šé”™è¯¯åœ°ç”Ÿæˆè¿™ç§æ ¼å¼ï¼Œéœ€è¦è‡ªåŠ¨ä¿®æ­£ã€‚
    
    ä¾‹å¦‚ï¼š
    - "test_ecommerce_100k.information_schema.tables" -> "information_schema.tables"
    - "test_ecommerce_100k.public.users" -> "public.users"
    
    Args:
        sql: åŸå§‹SQLè¯­å¥
        database_name: å½“å‰è¿æ¥çš„æ•°æ®åº“å
        
    Returns:
        str: å»é™¤æ•°æ®åº“åå‰ç¼€åçš„SQL
    """
    if not database_name:
        return sql
    
    # æ„å»ºè¦æ›¿æ¢çš„æ¨¡å¼ï¼šæ•°æ®åº“ååè·Ÿä¸€ä¸ªç‚¹
    # éœ€è¦å¤„ç†å¤§å°å†™ä¸æ•æ„Ÿçš„æƒ…å†µ
    import re
    
    # åŒ¹é… æ•°æ®åº“å. çš„æ¨¡å¼ï¼ˆåé¢å¿…é¡»è·Ÿç€æœ‰æ•ˆçš„æ ‡è¯†ç¬¦ï¼‰
    # ä½¿ç”¨å•è¯è¾¹ç•Œç¡®ä¿ç²¾ç¡®åŒ¹é…
    pattern = re.compile(
        r'\b' + re.escape(database_name) + r'\.',
        re.IGNORECASE
    )
    
    original_sql = sql
    sql = pattern.sub('', sql)
    
    if sql != original_sql:
        logger.info(f"[SQLé¢„å¤„ç†] å»é™¤æ•°æ®åº“åå‰ç¼€ '{database_name}.': {original_sql[:100]}... -> {sql[:100]}...")
    
    return sql


def _extract_table_name_from_sql(sql: str) -> Optional[str]:
    """
    ä»SQLè¯­å¥ä¸­æå–ä¸»è¡¨å

    Args:
        sql: SQLè¯­å¥

    Returns:
        æå–çš„è¡¨åï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
    """
    try:
        # ç§»é™¤æ³¨é‡Š
        clean_sql = sql
        # ç§»é™¤å•è¡Œæ³¨é‡Š
        clean_sql = re.sub(r'--.*$', '', clean_sql, flags=re.MULTILINE)
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š
        clean_sql = re.sub(r'/\*.*?\*/', '', clean_sql, flags=re.DOTALL)

        # æŸ¥æ‰¾ FROM æˆ– JOIN å­å¥
        # ä¼˜å…ˆåŒ¹é… FROM (ä¸»è¡¨)
        from_match = re.search(r'\bFROM\s+["\']?([a-zA-Z_][a-zA-Z0-9_]*)', clean_sql, re.IGNORECASE)
        if from_match:
            return from_match.group(1).lower()

        # å¦‚æœæ²¡æœ‰FROMï¼Œå°è¯•JOIN
        join_match = re.search(r'\bJOIN\s+["\']?([a-zA-Z_][a-zA-Z0-9_]*)', clean_sql, re.IGNORECASE)
        if join_match:
            return join_match.group(1).lower()

        return None
    except Exception:
        return None


def _convert_decimal_to_float(data: Any) -> Any:
    """
    é€’å½’åœ°å°†æ•°æ®ä¸­çš„ Decimal å’Œ datetime ç±»å‹è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–çš„æ ¼å¼
    
    Args:
        data: éœ€è¦è½¬æ¢çš„æ•°æ®ï¼ˆå¯ä»¥æ˜¯ dict, list, æˆ–å…¶ä»–ç±»å‹ï¼‰
    
    Returns:
        è½¬æ¢åçš„æ•°æ®ï¼Œå…¶ä¸­ï¼š
        - Decimal -> float
        - datetime/date -> ISO æ ¼å¼å­—ç¬¦ä¸²
    """
    if isinstance(data, Decimal):
        return float(data)
    elif isinstance(data, datetime):
        return data.isoformat()
    elif isinstance(data, date):
        return data.isoformat()
    elif isinstance(data, dict):
        return {k: _convert_decimal_to_float(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [_convert_decimal_to_float(item) for item in data]
    else:
        return data


# ============================================================
# å·¥å…·å®šä¹‰ (Tool Definitions) - OpenAI Function Calling æ ¼å¼
# ============================================================

# SQL æ‰§è¡Œå·¥å…· Schema
SQL_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "execute_sql",
        "description": "æ‰§è¡Œ SQL SELECT æŸ¥è¯¢ä»¥è·å–æ•°æ®åº“æ•°æ®ã€‚åªèƒ½æ‰§è¡Œ SELECT æŸ¥è¯¢ï¼Œç¦æ­¢æ‰§è¡Œ INSERTã€UPDATEã€DELETE ç­‰ä¿®æ”¹æ“ä½œã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "sql_query": {
                    "type": "string",
                    "description": "è¦æ‰§è¡Œçš„ SQL SELECT æŸ¥è¯¢è¯­å¥"
                }
            },
            "required": ["sql_query"]
        }
    }
}

# å›¾è¡¨ç”Ÿæˆå·¥å…· Schema
CHART_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "generate_chart",
        "description": "æ ¹æ®æ•°æ®ç”Ÿæˆ ECharts å›¾è¡¨é…ç½®ã€‚å½“æ•°æ®åŒ…å«æ•°å­—ã€è¶‹åŠ¿æˆ–åˆ†ç±»å¯¹æ¯”æ—¶å¿…é¡»è°ƒç”¨æ­¤å·¥å…·è¿›è¡Œå¯è§†åŒ–ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "chart_type": {
                    "type": "string",
                    "enum": ["bar", "line", "pie", "scatter"],
                    "description": "å›¾è¡¨ç±»å‹ï¼šbar(æŸ±çŠ¶å›¾-åˆ†ç±»å¯¹æ¯”), line(æŠ˜çº¿å›¾-è¶‹åŠ¿å˜åŒ–), pie(é¥¼å›¾-å æ¯”åˆ†å¸ƒ), scatter(æ•£ç‚¹å›¾-ç›¸å…³æ€§)"
                },
                "title": {
                    "type": "string",
                    "description": "å›¾è¡¨æ ‡é¢˜"
                },
                "x_data": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Xè½´æ•°æ®ï¼ˆåˆ†ç±»åç§°æˆ–æ—¶é—´ï¼‰"
                },
                "y_data": {
                    "type": "array",
                    "items": {"type": "number"},
                    "description": "Yè½´æ•°æ®ï¼ˆæ•°å€¼ï¼‰"
                },
                "series_name": {
                    "type": "string",
                    "description": "æ•°æ®ç³»åˆ—åç§°ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "required": ["chart_type", "title", "x_data", "y_data"]
        }
    }
}


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser, assistant, system")
    content: Union[str, List[Dict[str, Any]]] = Field(
        ...,
        description="æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€å†…å®¹"
    )
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹ï¼ˆä»…assistantè§’è‰²ï¼‰")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹"""
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    provider: Optional[str] = Field(
        None,
        description="LLMæä¾›å•†ï¼šzhipu, openrouterï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©"
    )
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§è¾“å‡ºtokens")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•°(0-1)")
    stream: bool = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
    enable_thinking: bool = Field(False, description="æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä»…Zhipuæ”¯æŒï¼‰")
    data_source_ids: Optional[List[str]] = Field(None, description="æŒ‡å®šä½¿ç”¨çš„æ•°æ®æºIDåˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰æ´»è·ƒæ•°æ®æº")


class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”æ¨¡å‹"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨æƒ…å†µ")
    model: Optional[str] = Field(None, description="ä½¿ç”¨çš„æ¨¡å‹")
    provider: Optional[str] = Field(None, description="ä½¿ç”¨çš„æä¾›å•†")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")
    created_at: Optional[str] = Field(None, description="åˆ›å»ºæ—¶é—´")


class ProviderStatusResponse(BaseModel):
    """æä¾›å•†çŠ¶æ€å“åº”æ¨¡å‹"""
    zhipu: bool = Field(..., description="æ™ºè°±AIå¯ç”¨æ€§")
    openrouter: bool = Field(..., description="OpenRouterå¯ç”¨æ€§")


class AvailableModelsResponse(BaseModel):
    """å¯ç”¨æ¨¡å‹å“åº”æ¨¡å‹"""
    providers: Dict[str, List[str]] = Field(..., description="å„æä¾›å•†çš„å¯ç”¨æ¨¡å‹")


def _convert_chat_messages(messages: List[ChatMessage]) -> List[LLMMessage]:
    """è½¬æ¢èŠå¤©æ¶ˆæ¯æ ¼å¼"""
    llm_messages = []
    for msg in messages:
        llm_messages.append(LLMMessage(
            role=msg.role,
            content=msg.content,
            thinking=msg.thinking
        ))
    return llm_messages


def _get_column_type(dtype_str: str) -> str:
    """å°†pandasæ•°æ®ç±»å‹è½¬æ¢ä¸ºå‹å¥½çš„ç±»å‹æè¿°"""
    if 'int' in dtype_str:
        return 'integer'
    elif 'float' in dtype_str:
        return 'float'
    elif 'datetime' in dtype_str:
        return 'datetime'
    elif 'bool' in dtype_str:
        return 'boolean'
    else:
        return 'text'


def _build_table_schema(df: pd.DataFrame, table_name: str) -> Dict[str, Any]:
    """ä»DataFrameæ„å»ºå•ä¸ªè¡¨çš„schemaä¿¡æ¯"""
    columns = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        columns.append({
            "name": str(col),
            "type": _get_column_type(dtype),
            "nullable": df[col].isnull().any()
        })

    # è·å–ç¤ºä¾‹æ•°æ®ï¼ˆå‰5è¡Œï¼‰
    sample_rows = []
    for _, row in df.head(5).iterrows():
        row_data = {}
        for col in df.columns[:10]:  # é™åˆ¶åˆ—æ•°
            value = row[col]
            if pd.isna(value):
                row_data[str(col)] = None
            else:
                row_data[str(col)] = str(value)
        sample_rows.append(row_data)

    return {
        "table_info": {
            "name": table_name,
            "columns": columns,
            "row_count": len(df)
        },
        "sample_data": {
            "columns": [str(c) for c in df.columns[:10]],
            "data": sample_rows
        }
    }


async def _get_file_schema(connection_string: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶æ•°æ®æºè·å–schemaä¿¡æ¯ï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼ˆæ ¼å¼: file://data-sources/{tenant_id}/{file_id}.xlsx æˆ– /app/uploads/...ï¼‰
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsç­‰ï¼‰
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¡¨ï¼ˆSheetï¼‰çš„åˆ—åã€ç±»å‹å’Œç¤ºä¾‹æ•°æ®
    """
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„è·¯å¾„è§£æé€»è¾‘ï¼Œä¼˜å…ˆå°è¯•æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
        from src.app.services.agent.path_extractor import resolve_file_path_with_fallback
        
        # é¦–å…ˆå°è¯•è§£æè·¯å¾„ï¼ˆåŒ…å«æœ¬åœ°å›é€€é€»è¾‘ï¼‰
        local_file_path = resolve_file_path_with_fallback(connection_string)
        file_data = None
        use_local_file = False
        
        # å¦‚æœæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
        if local_file_path and os.path.exists(local_file_path):
            logger.info(f"ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶: {local_file_path}")
            use_local_file = True
            file_path_for_read = local_file_path
        else:
            # å°è¯•ä»MinIOä¸‹è½½
            storage_path = connection_string[7:] if connection_string.startswith("file://") else connection_string
            logger.info(f"å°è¯•ä»MinIOä¸‹è½½æ–‡ä»¶: {storage_path}")
            file_data = minio_service.download_file(
                bucket_name="data-sources",
                object_name=storage_path
            )
            
            if not file_data:
                logger.warning(f"æ— æ³•ä»MinIOè·å–æ–‡ä»¶: {storage_path}")
                # æœ€åå°è¯•æœ¬åœ°å›é€€
                if local_file_path:
                    logger.info(f"å°è¯•ä½¿ç”¨è§£æçš„æœ¬åœ°è·¯å¾„: {local_file_path}")
                    if os.path.exists(local_file_path):
                        use_local_file = True
                        file_path_for_read = local_file_path
                    else:
                        return {}
                else:
                    return {}

        tables = []
        sample_data = {}

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheet
            try:
                if use_local_file:
                    # ä»æœ¬åœ°æ–‡ä»¶è¯»å–
                    excel_file = pd.ExcelFile(file_path_for_read, engine='openpyxl')
                else:
                    # ä»MinIOä¸‹è½½çš„æ•°æ®è¯»å–
                    excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                return {}
            except Exception as e:
                logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                return {}
            
            sheet_names = excel_file.sheet_names
            logger.info(f"Excelæ–‡ä»¶åŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨å
                    table_schema = _build_table_schema(df, sheet_name)
                    tables.append(table_schema["table_info"])
                    sample_data[sheet_name] = table_schema["sample_data"]
                    logger.info(f"Sheet '{sheet_name}': {len(df)}è¡Œ, {len(df.columns)}åˆ—")
                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            if use_local_file:
                # ä»æœ¬åœ°æ–‡ä»¶è¯»å–
                for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                    try:
                        df = pd.read_csv(file_path_for_read, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
            else:
                # ä»MinIOä¸‹è½½çš„æ•°æ®è¯»å–
                for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                    try:
                        df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue

            if df is not None:
                table_schema = _build_table_schema(df, data_source_name)
                tables.append(table_schema["table_info"])
                sample_data[data_source_name] = table_schema["sample_data"]

        if not tables:
            path_info = file_path_for_read if use_local_file else (connection_string[7:] if connection_string.startswith("file://") else connection_string)
            logger.warning(f"æ— æ³•ä»æ–‡ä»¶è§£æä»»ä½•è¡¨: {path_info}")
            return {}

        schema_info = {
            "tables": tables,
            "sample_data": sample_data
        }

        total_rows = sum(t.get("row_count", 0) for t in tables)
        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
        return schema_info

    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶schemaå¤±è´¥: {e}")
        return {}


async def _try_find_file_in_minio(tenant_id: str, data_source_id: str, db_type: str) -> Optional[str]:
    """
    å°è¯•åœ¨MinIOä¸­æŸ¥æ‰¾æ•°æ®æºå¯¹åº”çš„æ–‡ä»¶

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹

    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
        prefix = f"{tenant_id}/{data_source_id}"
        ext = f".{db_type}"

        # åˆ—å‡ºMinIOä¸­çš„æ–‡ä»¶
        objects = minio_service.list_files(
            bucket_name="data-sources",
            prefix=prefix
        )

        for obj in objects:
            obj_name = obj.object_name if hasattr(obj, 'object_name') else str(obj)
            if obj_name.endswith(ext):
                return f"file://{obj_name}"

        logger.warning(f"åœ¨MinIOä¸­æœªæ‰¾åˆ°æ•°æ®æº {data_source_id} çš„æ–‡ä»¶")
        return None

    except Exception as e:
        logger.error(f"åœ¨MinIOä¸­æœç´¢æ–‡ä»¶å¤±è´¥: {e}")
        return None


async def _try_get_file_schema_fallback(tenant_id: str, data_source_id: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ä»MinIOç›´æ¥è·å–æ–‡ä»¶schemaï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
        ext = f".{db_type}"
        possible_paths = [
            f"{tenant_id}/{data_source_id}{ext}",
            f"{tenant_id}/{data_source_name}{ext}",
        ]

        for path in possible_paths:
            try:
                logger.info(f"å°è¯•ä»MinIOè·å–æ–‡ä»¶: {path}")
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=path
                )

                if file_data:
                    tables = []
                    sample_data = {}

                    if db_type in ["xlsx", "xls"]:
                        # è¯»å–æ‰€æœ‰Sheet
                        try:
                            # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                            excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
                        except ImportError as e:
                            logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        except Exception as e:
                            logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        
                        sheet_names = excel_file.sheet_names
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆ: ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

                        for sheet_name in sheet_names:
                            try:
                                # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                                df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                                if df.empty:
                                    continue
                                table_schema = _build_table_schema(df, sheet_name)
                                tables.append(table_schema["table_info"])
                                sample_data[sheet_name] = table_schema["sample_data"]
                            except Exception as e:
                                logger.debug(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                                continue

                    elif db_type == "csv":
                        df = None
                        for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                            try:
                                df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                                break
                            except UnicodeDecodeError:
                                continue

                        if df is not None:
                            table_schema = _build_table_schema(df, data_source_name)
                            tables.append(table_schema["table_info"])
                            sample_data[data_source_name] = table_schema["sample_data"]

                    if tables:
                        total_rows = sum(t.get("row_count", 0) for t in tables)
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆæˆåŠŸè·å–schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
                        return {
                            "tables": tables,
                            "sample_data": sample_data
                        }

            except Exception as e:
                logger.debug(f"å°è¯•è·¯å¾„ {path} å¤±è´¥: {e}")
                continue

        logger.warning(f"å¤‡é€‰æ–¹æ¡ˆæœªèƒ½è·å–æ•°æ®æº {data_source_name} çš„schema")
        return {}

    except Exception as e:
        logger.error(f"å¤‡é€‰æ–¹æ¡ˆè·å–schemaå¤±è´¥: {e}")
        return {}


async def _get_data_sources_context(tenant_id: str, db: Session, data_source_ids: Optional[List[str]] = None) -> str:
    """
    è·å–ç§Ÿæˆ·æ•°æ®æºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬schemaï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰æ´»è·ƒæ•°æ®æº

    Returns:
        æ•°æ®æºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    start_time = time.time()
    try:
        # è·å–ç§Ÿæˆ·çš„æ‰€æœ‰æ´»è·ƒæ•°æ®æº
        t1 = time.time()
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )
        perf_msg = f"[PERF] get_data_sources took {time.time() - t1:.2f}s, found {len(data_sources) if data_sources else 0} sources"
        print(perf_msg)  # ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
        logger.info(perf_msg)

        if not data_sources:
            return ""

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        if data_source_ids:
            original_count = len(data_sources)
            data_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            logger.info(f"ğŸ¯ [æ•°æ®æºç­›é€‰] æŒ‡å®šæ•°æ®æº: {data_source_ids}, ä» {original_count} ä¸ªä¸­ç­›é€‰å‡º {len(data_sources)} ä¸ªåŒ¹é…çš„æ•°æ®æº")
            for ds in data_sources:
                logger.info(f"  âœ… ä½¿ç”¨æ•°æ®æº: {ds.name} (ID: {ds.id}, ç±»å‹: {ds.db_type})")
            if not data_sources:
                logger.warning(f"âš ï¸ [æ•°æ®æºç­›é€‰] æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æºï¼è¯·æ±‚çš„ID: {data_source_ids}")
                return ""
        else:
            logger.warning(f"âš ï¸ [æ•°æ®æºç­›é€‰] æœªæŒ‡å®š data_source_idsï¼Œå°†ä½¿ç”¨æ‰€æœ‰ {len(data_sources)} ä¸ªæ´»è·ƒæ•°æ®æº:")
            for ds in data_sources:
                logger.info(f"  ğŸ“¦ æ´»è·ƒæ•°æ®æº: {ds.name} (ID: {ds.id}, ç±»å‹: {ds.db_type})")

        context_parts = []
        context_parts.append("## å¯ç”¨æ•°æ®æº\n")

        for ds in data_sources:
            try:
                ds_start = time.time()
                schema_info = None
                connection_string = None

                # å°è¯•è·å–è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
                try:
                    t2 = time.time()
                    connection_string = await data_source_service.get_decrypted_connection_string(
                        data_source_id=ds.id,
                        tenant_id=tenant_id,
                        db=db
                    )
                    print(f"[PERF] get_decrypted_connection_string for {ds.name} took {time.time() - t2:.2f}s")
                except Exception as decrypt_error:
                    print(f"[PERF] è§£å¯†æ•°æ®æº {ds.name} è¿æ¥å­—ç¬¦ä¸²å¤±è´¥: {decrypt_error}")
                    # å¯¹äºæ–‡ä»¶ç±»å‹æ•°æ®æºï¼Œå°è¯•ä»MinIOç›´æ¥æœç´¢æ–‡ä»¶
                    if ds.db_type in ["xlsx", "xls", "csv"]:
                        connection_string = await _try_find_file_in_minio(tenant_id, ds.id, ds.db_type)

                # æ ¹æ®æ•°æ®æºç±»å‹è·å–schema
                if ds.db_type == "postgresql" and connection_string:
                    t3 = time.time()
                    adapter = PostgreSQLAdapter(connection_string)
                    try:
                        await adapter.connect()
                        schema_result = await adapter.get_schema_info()
                        
                        # ğŸ”§ æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹æšä¸¾å­—æ®µå¹¶è·å–å…¶å®é™…å€¼
                        # å¸¸è§çš„æšä¸¾å­—æ®µåæ¨¡å¼
                        enum_field_patterns = [
                            'status', 'state', 'type', 'category', 'level', 'role',
                            'gender', 'priority', 'payment_method', 'payment_status',
                            'order_status', 'shipping_status', 'user_type'
                        ]
                        
                        # ç”¨äºå­˜å‚¨æ¯ä¸ªè¡¨çš„æšä¸¾å€¼
                        enum_values_cache = {}
                        
                        for table in schema_result.tables.values():
                            table_enum_values = {}
                            for col in table.columns:
                                col_lower = col.name.lower()
                                # æ£€æŸ¥æ˜¯å¦æ˜¯å¯èƒ½çš„æšä¸¾å­—æ®µ
                                is_enum_field = any(pattern in col_lower for pattern in enum_field_patterns)
                                # ä¹Ÿæ£€æŸ¥å­—ç¬¦ä¸²ç±»å‹çš„çŸ­å­—æ®µï¼ˆå¯èƒ½æ˜¯æšä¸¾ï¼‰
                                is_short_varchar = (
                                    col.data_type in ['character varying', 'varchar', 'text'] and
                                    col.max_length and col.max_length <= 50
                                )
                                
                                if is_enum_field or (is_short_varchar and col_lower.endswith(('_type', '_status', '_state'))):
                                    try:
                                        # æŸ¥è¯¢è¯¥å­—æ®µçš„distinctå€¼ï¼ˆé™åˆ¶10ä¸ªï¼Œé¿å…å¤ªå¤šï¼‰
                                        distinct_query = f"""
                                            SELECT DISTINCT "{col.name}" 
                                            FROM "{table.name}" 
                                            WHERE "{col.name}" IS NOT NULL 
                                            LIMIT 10
                                        """
                                        distinct_result = await adapter.execute_query(distinct_query)
                                        if distinct_result and distinct_result.data:
                                            values = [row[col.name] for row in distinct_result.data if row.get(col.name)]
                                            if values and len(values) <= 10:  # åªä¿ç•™åˆç†æ•°é‡çš„æšä¸¾å€¼
                                                table_enum_values[col.name] = values
                                    except Exception as enum_err:
                                        logger.debug(f"è·å–æšä¸¾å€¼å¤±è´¥ {table.name}.{col.name}: {enum_err}")
                            
                            if table_enum_values:
                                enum_values_cache[table.name] = table_enum_values
                        
                        # å°†SchemaInfoå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œå¹¶åŒ…å«æšä¸¾å€¼
                        schema_info = {
                            "database_type": schema_result.database_type.value if schema_result.database_type else "postgresql",
                            "tables": [
                                {
                                    "name": table.name,
                                    "columns": [
                                        {
                                            "name": col.name,
                                            "type": col.data_type,
                                            "nullable": col.is_nullable,
                                            # æ·»åŠ æšä¸¾å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
                                            "enum_values": enum_values_cache.get(table.name, {}).get(col.name)
                                        }
                                        for col in table.columns
                                    ]
                                }
                                for table in schema_result.tables.values()
                            ] if schema_result.tables else []
                        }
                    finally:
                        await adapter.disconnect()
                    print(f"[PERF] PostgreSQL get_schema for {ds.name} took {time.time() - t3:.2f}s")

                elif ds.db_type in ["xlsx", "xls", "csv"]:
                    # ğŸ”§ ä¿®å¤ï¼šä»connection_configæˆ–connection_stringæå–æ–‡ä»¶è·¯å¾„
                    file_path = connection_string
                    if hasattr(ds, 'connection_config') and ds.connection_config:
                        # å¦‚æœå­˜åœ¨connection_configå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
                        from src.app.services.agent.path_extractor import extract_file_path_from_config
                        extracted_path = extract_file_path_from_config(ds.connection_config, connection_string)
                        if extracted_path:
                            file_path = extracted_path
                    
                    if file_path:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä»æ–‡ä»¶è¯»å–å¹¶è§£æschema
                        t4 = time.time()
                        schema_info = await _get_file_schema(file_path, ds.db_type, ds.name)
                        print(f"[PERF] _get_file_schema for {ds.name} took {time.time() - t4:.2f}s")
                    else:
                        # è¿æ¥å­—ç¬¦ä¸²è·å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
                        print(f"[PERF] å°è¯•å¤‡é€‰æ–¹æ¡ˆè·å–æ•°æ®æº {ds.name} çš„schema")
                        schema_info = await _try_get_file_schema_fallback(tenant_id, ds.id, ds.db_type, ds.name)

                print(f"[PERF] Total processing for data source {ds.name} took {time.time() - ds_start:.2f}s")

                if schema_info and schema_info.get("tables"):
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ–‡ä»¶/æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")

                    # æ·»åŠ è¡¨ä¿¡æ¯
                    context_parts.append("\n#### è¡¨ç»“æ„:")
                    for table in schema_info["tables"][:20]:  # é™åˆ¶è¡¨æ•°é‡é¿å…tokenè¿‡å¤š
                        table_name = table.get("name", "unknown")
                        row_count = table.get("row_count", "æœªçŸ¥")
                        context_parts.append(f"\n**è¡¨: {table_name}** (å…±{row_count}è¡Œ)")

                        columns = table.get("columns", [])
                        if columns:
                            col_info = []
                            for col in columns[:30]:  # é™åˆ¶åˆ—æ•°é‡
                                col_name = col.get("name", "unknown")
                                col_type = col.get("type", "unknown")
                                nullable = "å¯ç©º" if col.get("nullable") else "éç©º"
                                # ğŸ”§ æ–°å¢ï¼šæ˜¾ç¤ºæšä¸¾å€¼
                                enum_values = col.get("enum_values")
                                if enum_values:
                                    enum_str = ", ".join([f"'{v}'" for v in enum_values[:8]])  # æœ€å¤šæ˜¾ç¤º8ä¸ª
                                    if len(enum_values) > 8:
                                        enum_str += ", ..."
                                    col_info.append(f"  - {col_name} ({col_type}, {nullable}) **å¯é€‰å€¼: [{enum_str}]**")
                                else:
                                    col_info.append(f"  - {col_name} ({col_type}, {nullable})")
                            context_parts.append("\n".join(col_info))

                        # æ·»åŠ ä¸»é”®ä¿¡æ¯
                        if table.get("primary_key"):
                            context_parts.append(f"  - ä¸»é”®: {', '.join(table['primary_key'])}")

                        # æ·»åŠ å¤–é”®ä¿¡æ¯
                        if table.get("foreign_keys"):
                            for fk in table["foreign_keys"]:
                                context_parts.append(
                                    f"  - å¤–é”®: {fk['column']} -> {fk['references_table']}.{fk['references_column']}"
                                )

                    # æ·»åŠ è¡¨å…³ç³»ä¿¡æ¯ï¼ˆå¤–é”®å…³è”ï¼‰
                    relationships = schema_info.get("relationships", [])
                    if relationships:
                        context_parts.append("\n#### è¡¨å…³ç³»ï¼ˆå¤–é”®ï¼‰:")
                        context_parts.append("**é‡è¦ï¼š** ä»¥ä¸‹æ˜¯è¡¨ä¹‹é—´çš„å…³è”å…³ç³»ï¼ŒæŸ¥è¯¢æ—¶å¿…é¡»é€šè¿‡è¿™äº›å¤–é”®è¿›è¡ŒJOINï¼š")
                        for rel in relationships:
                            from_table = rel.get("from_table", "unknown")
                            from_column = rel.get("from_column", "unknown")
                            to_table = rel.get("to_table", "unknown")
                            to_column = rel.get("to_column", "unknown")
                            context_parts.append(
                                f"  - {from_table}.{from_column} -> {to_table}.{to_column}"
                            )

                    # æ·»åŠ ç¤ºä¾‹æ•°æ®
                    sample_data = schema_info.get("sample_data", {})
                    if sample_data:
                        context_parts.append("\n#### ç¤ºä¾‹æ•°æ®:")
                        for table_name, samples in list(sample_data.items())[:5]:  # é™åˆ¶è¡¨æ•°é‡
                            if samples.get("data"):
                                context_parts.append(f"\n**{table_name}** (å‰5è¡Œ):")
                                for row in samples["data"][:3]:  # é™åˆ¶è¡Œæ•°
                                    row_str = ", ".join([f"{k}={v}" for k, v in list(row.items())[:5]])
                                    context_parts.append(f"  {row_str}")
                else:
                    # å…¶ä»–æ•°æ®åº“ç±»å‹æš‚æ—¶åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")
                    context_parts.append("- æ³¨: æ­¤æ•°æ®åº“ç±»å‹çš„schemaå‘ç°åŠŸèƒ½å¼€å‘ä¸­")

            except Exception as e:
                logger.warning(f"è·å–æ•°æ®æº {ds.name} çš„schemaå¤±è´¥: {e}")
                context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                context_parts.append(f"- ç±»å‹: {ds.db_type}")
                context_parts.append(f"- æ³¨: æ— æ³•è·å–schemaä¿¡æ¯ ({str(e)[:50]})")

        total_time = time.time() - start_time
        logger.info(f"[PERF] _get_data_sources_context TOTAL took {total_time:.2f}s")
        return "\n".join(context_parts)

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return ""


def _build_system_prompt_with_context(
    data_sources_context: str,
    db_type: str = "postgresql"  # æ–°å¢å‚æ•°ï¼šæ•°æ®åº“ç±»å‹
) -> str:
    """
    æ„å»ºåŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨ SQL ä»£ç å—æ ¼å¼ï¼‰

    Args:
        data_sources_context: æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯
        db_type: æ•°æ®åº“ç±»å‹ï¼ˆpostgresql, mysql, sqlite, xlsx, csvç­‰ï¼‰

    Returns:
        ç³»ç»Ÿæç¤ºè¯
    """
    # å¯¼å…¥æç¤ºè¯ç”Ÿæˆå™¨ï¼ˆæ”¯æŒæ•°æ®åº“ç±»å‹æ„ŸçŸ¥ï¼‰
    import sys
    from pathlib import Path

    # è·¯å¾„è®¡ç®—ï¼šä¼˜å…ˆä½¿ç”¨ Docker å®¹å™¨ä¸­çš„ç»å¯¹è·¯å¾„ /Agent
    # å¦‚æœ /Agent ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨ç›¸å¯¹è·¯å¾„è®¡ç®—ï¼ˆæœ¬åœ°å¼€å‘ç¯å¢ƒï¼‰
    if Path("/Agent").exists():
        agent_path = Path("/Agent")
    else:
        # æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šä» llm.py å‘ä¸Š 5 çº§åˆ° backendï¼Œç„¶ååˆ° Agent
        agent_path = Path(__file__).parent.parent.parent.parent.parent / "Agent"

    if str(agent_path) not in sys.path:
        sys.path.insert(0, str(agent_path))

    if data_sources_context:
        # åŸºç¡€ç³»ç»Ÿæç¤ºè¯
        base_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼ŒæŸ¥è¯¢æ•°æ®åº“å¹¶ç»™å‡ºåˆ†æç»“æœã€‚

## ğŸ”´ é‡è¦è§„åˆ™ ğŸ”´

**ä½ å¿…é¡»ä½¿ç”¨ SQL ä»£ç å—æ¥æŸ¥è¯¢æ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š**

```sql
SELECT * FROM è¡¨å WHERE æ¡ä»¶;
```

**ç¦æ­¢ä½¿ç”¨ä»»ä½•å·¥å…·è°ƒç”¨æˆ–å‡½æ•°è°ƒç”¨æ ¼å¼ï¼åªéœ€è¦åœ¨å›ç­”ä¸­ç›´æ¥å†™ SQL ä»£ç å—å³å¯ã€‚**

---

## å·¥ä½œæµç¨‹

### æ­¥éª¤ 1: åˆ†æé—®é¢˜
- ç†è§£ç”¨æˆ·æƒ³è¦æŸ¥è¯¢ä»€ä¹ˆæ•°æ®
- æ ¹æ®ä¸‹æ–¹çš„æ•°æ®åº“ Schema ç¡®å®šéœ€è¦æŸ¥è¯¢çš„è¡¨å’Œå­—æ®µ

### æ­¥éª¤ 2: ç”Ÿæˆ SQL
- åœ¨å›ç­”ä¸­ä½¿ç”¨ ```sql ... ``` ä»£ç å—æ ¼å¼è¾“å‡º SQL æŸ¥è¯¢è¯­å¥
- å¿…é¡»ä½¿ç”¨ä¸‹æ–¹ Schema ä¸­çš„**å®é™…è¡¨åå’Œåˆ—å**ï¼Œç¦æ­¢çŒœæµ‹
- ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œä½ çš„ SQL å¹¶è¿”å›ç»“æœ

### æ­¥éª¤ 3: åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰
- å¦‚æœä½ å·²ç»çŸ¥é“æ•°æ®ç‰¹å¾ï¼Œå¯ä»¥ç®€è¦è¯´æ˜é¢„æœŸçš„åˆ†ææ–¹å‘
- ä¾‹å¦‚ï¼š"è®©æˆ‘æŸ¥è¯¢ä¸€ä¸‹å„äº§å“çš„é”€é‡æ•°æ®..."

---

## æ•°æ®åº“ Schema

{data_sources_context}

---

## ğŸ”´ æŸ¥è¯¢è¡¨åˆ—è¡¨çš„æ­£ç¡®æ–¹å¼ï¼ˆé‡è¦ï¼ï¼‰

**å¦‚æœç”¨æˆ·é—®"æœ‰å“ªäº›è¡¨"ã€"æ•°æ®åº“é‡Œæœ‰ä»€ä¹ˆè¡¨"ç­‰é—®é¢˜ï¼š**

1. **ä¼˜å…ˆä½¿ç”¨ä¸Šæ–¹ Schema ä¿¡æ¯å›ç­”**ï¼šä¸Šæ–¹å·²ç»åˆ—å‡ºäº†æ‰€æœ‰å¯ç”¨çš„è¡¨å’Œåˆ—ä¿¡æ¯ï¼Œç›´æ¥æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·
2. **å¯¹äº Excel/CSV æ–‡ä»¶æ•°æ®æº**ï¼šè¡¨åå°±æ˜¯ Excel çš„ Sheet åç§°æˆ– CSV æ–‡ä»¶åï¼Œå·²ç»åœ¨ä¸Šæ–¹ Schema ä¸­åˆ—å‡º
3. **å¦‚æœéœ€è¦æ‰§è¡Œ SQL æŸ¥è¯¢è¡¨åˆ—è¡¨**ï¼š
   - âœ… æ­£ç¡®è¯­æ³•ï¼š`SHOW TABLES;`
   - âŒ é”™è¯¯è¯­æ³•ï¼š`SELECT name FROM sqlite_master WHERE type='table'`ï¼ˆè¿™æ˜¯ SQLite è¯­æ³•ï¼Œä¸é€‚ç”¨äºæœ¬ç³»ç»Ÿï¼‰
   - âŒ é”™è¯¯è¯­æ³•ï¼š`SELECT table_name FROM information_schema.tables`ï¼ˆPostgreSQL è¯­æ³•ï¼Œå¯¹äºæ–‡ä»¶æ•°æ®æºä¸é€‚ç”¨ï¼‰

**æ³¨æ„**ï¼šæœ¬ç³»ç»Ÿçš„æ–‡ä»¶æ•°æ®æºä½¿ç”¨ DuckDB å¼•æ“æ‰§è¡Œ SQLï¼Œè¯·ç¡®ä¿ä½¿ç”¨å…¼å®¹çš„è¯­æ³•ã€‚

---

## ğŸ”¥ æ—¥æœŸå¤„ç†é‡è¦è¯´æ˜ï¼ˆé’ˆå¯¹ Excel/CSV æ–‡ä»¶æ•°æ®æºï¼‰

**å¯¹äº Excel/CSV æ–‡ä»¶æ•°æ®æºï¼Œæ—¥æœŸå­—æ®µé€šå¸¸å­˜å‚¨ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å¤„ç†ï¼š**

### æ–¹å¼1ï¼šä½¿ç”¨ CAST è½¬æ¢åæ¯”è¾ƒï¼ˆæ¨èï¼‰
```sql
SELECT * FROM è®¢å•è¡¨
WHERE CAST(created_at AS DATE) >= '2023-01-01'
  AND CAST(created_at AS DATE) < '2024-01-01';
```

### æ–¹å¼2ï¼šä½¿ç”¨ LIKE è¿›è¡Œæ–‡æœ¬åŒ¹é…
```sql
-- ç­›é€‰2023å¹´çš„æ•°æ®
SELECT * FROM è®¢å•è¡¨ WHERE created_at LIKE '2023%';

-- ç­›é€‰2023å¹´æŸæœˆçš„æ•°æ®
SELECT * FROM è®¢å•è¡¨ WHERE created_at LIKE '2023-06%';
```

### æ–¹å¼3ï¼šæŒ‰å¹´æœˆåˆ†ç»„ç»Ÿè®¡
```sql
-- ä½¿ç”¨ strftime éœ€è¦å…ˆè½¬æ¢ä¸ºæ—¥æœŸç±»å‹
SELECT
    strftime(CAST(created_at AS DATE), '%Y-%m') as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡
FROM è®¢å•è¡¨
WHERE created_at LIKE '2023%'
GROUP BY strftime(CAST(created_at AS DATE), '%Y-%m')
ORDER BY æœˆä»½;
```

### æ–¹å¼4ï¼šä½¿ç”¨ SUBSTRING æå–å¹´æœˆï¼ˆæ›´é€šç”¨ï¼‰
```sql
SELECT
    SUBSTRING(created_at, 1, 7) as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡,
    SUM(final_amount) as æ€»é”€å”®é¢
FROM è®¢å•è¡¨
WHERE SUBSTRING(created_at, 1, 4) = '2023'
GROUP BY SUBSTRING(created_at, 1, 7)
ORDER BY æœˆä»½;
```

---

## SQL æ ¼å¼ç¤ºä¾‹

ç”¨æˆ·é—®ï¼š"åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘å¸®ä½ æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯ï¼š

```sql
SELECT * FROM ç”¨æˆ·è¡¨;
```

ç”¨æˆ·é—®ï¼š"å“ªä¸ªäº§å“å–å¾—æœ€å¥½ï¼Ÿ"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘æŸ¥è¯¢å„äº§å“çš„é”€é‡æ’åï¼š

```sql
SELECT äº§å“åç§°, SUM(é”€é‡) as æ€»é”€é‡
FROM è®¢å•è¡¨
GROUP BY äº§å“åç§°
ORDER BY æ€»é”€é‡ DESC
LIMIT 10;
```

ç”¨æˆ·é—®ï¼š"2023å¹´çš„é”€å”®è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘æŸ¥è¯¢2023å¹´æŒ‰æœˆçš„é”€å”®è¶‹åŠ¿ï¼š

```sql
SELECT
    SUBSTRING(created_at, 1, 7) as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡,
    SUM(final_amount) as æ€»é”€å”®é¢
FROM è®¢å•è¡¨
WHERE SUBSTRING(created_at, 1, 4) = '2023'
GROUP BY SUBSTRING(created_at, 1, 7)
ORDER BY æœˆä»½;
```

---

## ğŸ§  æ¨¡ç³ŠæŸ¥è¯¢æ™ºèƒ½æ¨æ–­è§„åˆ™ï¼ˆé‡è¦ï¼ï¼‰

å½“ç”¨æˆ·é—®**æ¨¡ç³Šé—®é¢˜**ï¼ˆå¦‚"æœ€è¿‘ç”Ÿæ„æ€ä¹ˆæ ·"ã€"é”€å”®å¦‚ä½•"ã€"ä¸šç»©å¥½ä¸å¥½"ï¼‰æ—¶ï¼Œä½ å¿…é¡»ï¼š

### 1ï¸âƒ£ é»˜è®¤æ—¶é—´èŒƒå›´
| ç”¨æˆ·è¯´ | åº”ç†è§£ä¸º | SQLæ¡ä»¶ç¤ºä¾‹ |
|--------|----------|-------------|
| "æœ€è¿‘" | æœ€è¿‘30å¤© | `WHERE date_column >= '2023-12-08'`ï¼ˆç”¨å½“å‰æ—¥æœŸå‡30å¤©ï¼‰|
| "æœ€è¿‘ä¸€å‘¨" | æœ€è¿‘7å¤© | `WHERE date_column >= '2024-01-01'`ï¼ˆç”¨å½“å‰æ—¥æœŸå‡7å¤©ï¼‰|
| "æœ€è¿‘ä¸€æœˆ" | æœ€è¿‘30å¤© | `WHERE date_column >= '2023-12-08'` |
| "æœ¬æœˆ" | å½“æœˆ1æ—¥è‡³ä»Š | `WHERE date_column LIKE '2024-01%'` |
| "ä¸Šæœˆ" | ä¸Šæœˆæ•´æœˆ | `WHERE date_column LIKE '2023-12%'` |

### 2ï¸âƒ£ é»˜è®¤ä¸šåŠ¡æŒ‡æ ‡
| ç”¨æˆ·è¯´ | åº”ç†è§£ä¸º | ä¼˜å…ˆæŸ¥è¯¢æŒ‡æ ‡ |
|--------|----------|--------------|
| "ç”Ÿæ„"ã€"é”€å”®"ã€"ä¸šç»©" | è®¢å•é‡å’Œé”€å”®é¢ | COUNT(*) è®¢å•æ•°, SUM(amount) é”€å”®é¢ |
| "å®¢æˆ·"ã€"ç”¨æˆ·" | å®¢æˆ·æ•°é‡ | COUNT(DISTINCT customer_id) å®¢æˆ·æ•° |
| "æ”¶å…¥"ã€"é’±" | é‡‘é¢ | SUM(amount), AVG(amount) |
| "è¶‹åŠ¿"ã€"å˜åŒ–" | æ—¶é—´åºåˆ—æ•°æ® | æŒ‰æ—¥æœŸ/æœˆä»½åˆ†ç»„ç»Ÿè®¡ |

### 3ï¸âƒ£ ğŸ”´ æ¨¡ç³ŠæŸ¥è¯¢å¿…é¡»ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®ï¼ˆç”¨äºå›¾è¡¨ï¼‰ï¼

**å…³é”®è§„åˆ™**ï¼š
- æŸ¥è¯¢æ—¶å¿…é¡»**æŒ‰æ—¥æœŸåˆ†ç»„**ï¼ˆå¦‚æŒ‰å¤©æˆ–æŒ‰æœˆï¼‰ï¼Œè¿™æ ·æ‰èƒ½ç”»å‡ºè¶‹åŠ¿å›¾
- ä¸è¦åªæŸ¥æ€»æ•°ï¼Œè¦æŸ¥**æ—¶é—´åºåˆ—æ•°æ®**ç”¨äºå›¾è¡¨
- å¿…é¡»è°ƒç”¨ `generate_chart` å·¥å…·ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨

**SQLæŸ¥è¯¢ç¤ºä¾‹ï¼ˆé”™è¯¯ vs æ­£ç¡®ï¼‰**ï¼š
```sql
-- âŒ é”™è¯¯ï¼šåªæŸ¥æ€»æ•°ï¼Œæ— æ³•ç”»å›¾
SELECT COUNT(*), SUM(amount) FROM è®¢å•è¡¨ WHERE created_at LIKE '2023%';
-- åªè¿”å›ä¸€è¡Œï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿å›¾

-- âœ… æ­£ç¡®ï¼šæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œå¯ç”Ÿæˆè¶‹åŠ¿å›¾
SELECT
    SUBSTRING(created_at, 1, 7) as æœˆä»½,
    COUNT(*) as è®¢å•æ•°,
    SUM(final_amount) as é”€å”®é¢
FROM è®¢å•è¡¨
WHERE SUBSTRING(created_at, 1, 4) = '2023'
GROUP BY SUBSTRING(created_at, 1, 7)
ORDER BY æœˆä»½;
-- è¿”å›å¤šè¡Œæ•°æ®ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªæœˆä»½çš„æ•°æ®
```

### 4ï¸âƒ£ å…³é”®è¦æ±‚
- ğŸ”´ **æ¨¡ç³Šæ—¶é—´å¿…é¡»ä½¿ç”¨é»˜è®¤å€¼**ï¼ˆ"æœ€è¿‘"é»˜è®¤30å¤©ï¼Œä¸è¦é—®ç”¨æˆ·"å¤šä¹…"ï¼‰
- ğŸ”´ **æŸ¥è¯¢å¿…é¡»æŒ‰æ—¥æœŸåˆ†ç»„**ï¼ˆç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®ç”¨äºç”»å›¾ï¼‰
- ğŸ”´ **å¿…é¡»è°ƒç”¨ `generate_chart` å·¥å…·**ç”ŸæˆæŠ˜çº¿å›¾æˆ–æŸ±çŠ¶å›¾
- ğŸ”´ **ä¸»åŠ¨æ‰¾è¡¨**ï¼ˆé€šè¿‡ä¸Šæ–¹Schemaä¿¡æ¯æ™ºèƒ½æ¨æ–­è¡¨åï¼‰

### 5ï¸âƒ£ ğŸ”´ å›¾è¡¨ç”Ÿæˆè§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰

**å½“æŸ¥è¯¢ç»“æœåŒ…å«ç»Ÿè®¡æ•°æ®æ—¶ï¼ˆæ—¶é—´åºåˆ—ã€å¯¹æ¯”æ•°æ®ã€è¶‹åŠ¿åˆ†æï¼‰ï¼Œä½ å¿…é¡»ï¼š**

1. **è°ƒç”¨ `generate_chart` å·¥å…·**ç”Ÿæˆå›¾è¡¨ï¼Œå·¥å…·æ ¼å¼ï¼š
```json
{{
  "chart_type": "line",
  "title": "æ ‡é¢˜",
  "x_data": ["2023-01", "2023-02", ...],
  "y_data": [1000, 1200, ...],
  "series_name": "é”€å”®é¢"
}}
```

2. **å›¾è¡¨ç±»å‹é€‰æ‹©**ï¼š
   - åŒ…å«"è¶‹åŠ¿"ã€"å˜åŒ–"ã€"æ—¶é—´" â†’ `chart_type: "line"` (æŠ˜çº¿å›¾)
   - åŒ…å«"å¯¹æ¯”"ã€"æ¯”è¾ƒ"ã€"æ’å" â†’ `chart_type: "bar"` (æŸ±çŠ¶å›¾)
   - åŒ…å«"å æ¯”"ã€"åˆ†å¸ƒ"ã€"æ¯”ä¾‹" â†’ `chart_type: "pie"` (é¥¼å›¾)

3. **å›ç­”æ ¼å¼**ï¼šå…ˆè§£é‡Šä½ çš„åˆ†ææ€è·¯ â†’ æä¾›SQLæŸ¥è¯¢ â†’ è°ƒç”¨generate_chartå·¥å…·ç”Ÿæˆå›¾è¡¨

---

**è®°ä½ï¼šåªéœ€è¦åœ¨å›ç­”ä¸­å†™ SQL ä»£ç å—ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœï¼å¯¹äºæ•°æ®æŸ¥è¯¢ï¼Œå¿…é¡»è°ƒç”¨ generate_chart å·¥å…·ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼**"""

        # ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„æç¤ºè¯ç”Ÿæˆå™¨
        try:
            from src.app.services.prompt_generator import generate_database_aware_system_prompt
            result = generate_database_aware_system_prompt(db_type, base_prompt)
            logger.info(f"ğŸ” [LLMç«¯ç‚¹] ä½¿ç”¨æ•°æ®åº“ç±»å‹æ„ŸçŸ¥æç¤ºè¯ç”Ÿæˆå™¨ï¼Œdb_type={db_type}")
            return result
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥ prompt_generator: {e}ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
            return base_prompt
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆæ•°æ®åº“ç‰¹å®šæç¤ºè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
            return base_prompt
    else:
        # æ²¡æœ‰æ•°æ®æºæ—¶çš„æç¤º
        return """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚

å½“å‰ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰è¿æ¥ä»»ä½•æ•°æ®æºã€‚

å¦‚æœç”¨æˆ·è¯¢é—®æ•°æ®ç›¸å…³é—®é¢˜ï¼Œè¯·å‘Šè¯‰ä»–ä»¬éœ€è¦å…ˆåœ¨"æ•°æ®æºç®¡ç†"é¡µé¢æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚

ä¸è¦å‡è®¾æˆ–çŒœæµ‹æ•°æ®åº“ç»“æ„ï¼Œä¸è¦ç”Ÿæˆä»»ä½•SQLæŸ¥è¯¢ã€‚"""


def _get_default_fix_prompt(
    original_sql: str,
    error_message: str,
    schema_context: str,
    original_question: str,
    error_details: dict
) -> str:
    """
    è·å–é»˜è®¤çš„SQLä¿®å¤æç¤ºè¯ï¼ˆå›é€€æ–¹æ¡ˆï¼Œå½“åŠ¨æ€ç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    ä¸»è¦é’ˆå¯¹PostgreSQLï¼Œä½†ä¹Ÿå¯ä»¥å¤„ç†åŸºæœ¬çš„å‡½æ•°ä¸å…¼å®¹é”™è¯¯
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯å‡½æ•°ä¸å…¼å®¹é”™è¯¯ï¼ˆå¦‚ TO_CHAR ä¸å­˜åœ¨ï¼‰
    if "does not exist" in error_message and ("to_char" in error_message.lower() or "date_trunc" in error_message.lower()):
        # æ·»åŠ å‡½æ•°ä¸å…¼å®¹çš„ç‰¹å®šæç¤º
        db_hint = """
## ğŸ”´ å‡½æ•°ä¸å…¼å®¹é”™è¯¯

ä½ ä½¿ç”¨çš„å‡½æ•°åœ¨å½“å‰æ•°æ®åº“ä¸­ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥å¹¶æ›¿æ¢ä¸ºå…¼å®¹çš„å‡½æ•°ï¼š

- **TO_CHAR()**: PostgreSQLä¸“ç”¨ï¼ŒMySQLä¸­ç”¨DATE_FORMAT()ï¼ŒSQLiteä¸­ç”¨strftime()
- **DATE_TRUNC()**: PostgreSQLä¸“ç”¨ï¼ŒMySQLä¸­ç”¨DATE_FORMAT()ï¼ŒSQLiteä¸­ç”¨strftime()
- **EXTRACT()**: PostgreSQLæ”¯æŒï¼ŒMySQLå¯ç”¨YEAR()/MONTH()ï¼ŒSQLiteç”¨strftime()

è¯·æ ¹æ®é”™è¯¯ä¿¡æ¯å’Œä¸Šè¿°è¯´æ˜ï¼Œæ›¿æ¢ä¸å…¼å®¹çš„å‡½æ•°ã€‚
"""
    else:
        db_hint = f"""
## æ•°æ®åº“æç¤º
{error_details.get('hint', 'æ— æç¤º')}
"""

    return f"""ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚ç”¨æˆ·çš„æŸ¥è¯¢æ‰§è¡Œå¤±è´¥äº†ï¼Œè¯·å¸®åŠ©ä¿®å¤SQLè¯­å¥ã€‚

# ç”¨æˆ·åŸå§‹é—®é¢˜
{original_question}

# å¤±è´¥çš„SQLæŸ¥è¯¢
```sql
{original_sql}
```

# é”™è¯¯ä¿¡æ¯
{error_details['main_error']}

{db_hint}

# ğŸ”´ğŸ”´ğŸ”´ æ•°æ®åº“Schemaä¿¡æ¯ï¼ˆå¿…é¡»ä½¿ç”¨è¿™é‡Œçš„å®é™…è¡¨åå’Œåˆ—åï¼‰
{schema_context}

# ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®å¤è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

## ç¬¬1æ­¥ï¼šç†è§£é”™è¯¯
- **ä¸»è¦é”™è¯¯**: {error_details['main_error']}

## ç¬¬2æ­¥ï¼šæŸ¥æ‰¾æ­£ç¡®çš„è¡¨å/åˆ—å
**ğŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šSQLä¸­ä½¿ç”¨äº†ä¸å­˜åœ¨çš„è¡¨åæˆ–åˆ—åï¼Œæˆ–è€…ä½¿ç”¨äº†ä¸å…¼å®¹çš„å‡½æ•°ï¼**

1. **å¦‚æœé”™è¯¯æ˜¯"å‡½æ•°ä¸å­˜åœ¨"**ï¼š
   - æ£€æŸ¥å¹¶æ›¿æ¢ä¸ºæ•°æ®åº“å…¼å®¹çš„å‡½æ•°
   - PostgreSQL: TO_CHAR(), DATE_TRUNC(), EXTRACT()
   - MySQL: DATE_FORMAT(), YEAR(), MONTH()
   - SQLite: strftime(), CAST(strftime() AS INTEGER)

2. **å¦‚æœé”™è¯¯æ˜¯"Table does not exist"**ï¼š
   - å¿…é¡»ä»ä¸Šé¢çš„Schemaä¿¡æ¯ä¸­æ‰¾åˆ°å®é™…å­˜åœ¨çš„è¡¨å

3. **å¦‚æœé”™è¯¯æ˜¯"Column does not exist"**ï¼š
   - åœ¨Schemaä¸­æ‰¾åˆ°æ­£ç¡®çš„åˆ—å

## ç¬¬3æ­¥ï¼šä¿®å¤SQL
1. æ£€æŸ¥å¹¶æ›¿æ¢æ‰€æœ‰ä¸å…¼å®¹çš„å‡½æ•°
2. ä»”ç»†é˜…è¯»Schemaä¿¡æ¯ï¼Œæ‰¾åˆ°å¯¹åº”çš„**å®é™…è¡¨åå’Œåˆ—å**
3. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®
4. åªä½¿ç”¨SELECTæŸ¥è¯¢
5. ğŸ”´ æå€¼æŸ¥è¯¢å¿…é¡»ä½¿ç”¨ LIMIT 1

## ç¬¬4æ­¥ï¼šè¿”å›ç»“æœ
- **åªè¿”å›ä¿®å¤åçš„SQLè¯­å¥** - ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–markdownæ ‡è®°
- **å¦‚æœSchemaä¸­æ²¡æœ‰ç›¸å…³çš„è¡¨æˆ–åˆ—** - è¿”å›"CANNOT_FIX"
- **ä¸è¦æ·»åŠ ```sqlæ ‡è®°** - ç›´æ¥è¿”å›çº¯SQLè¯­å¥

ç°åœ¨è¯·ä¿®å¤ä¸Šè¿°å¤±è´¥çš„SQLæŸ¥è¯¢ï¼Œç›´æ¥è¿”å›ä¿®å¤åçš„SQLè¯­å¥ï¼š"""


def _parse_sql_error(error_message: str) -> Dict[str, str]:
    """
    è§£æSQLé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯

    Args:
        error_message: å®Œæ•´çš„é”™è¯¯ä¿¡æ¯

    Returns:
        åŒ…å«main_error, hint, suggestionçš„å­—å…¸
    """
    result = {
        'main_error': error_message,
        'hint': None,
        'suggestion': None
    }

    try:
        # æå–ä¸»è¦é”™è¯¯ä¿¡æ¯
        lines = error_message.split('\n')

        # é¦–å…ˆå°è¯•ä»å®Œæ•´é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ—/è¡¨ä¸å­˜åœ¨çš„é”™è¯¯
        column_match = re.search(r'column "([^"]+)" does not exist', error_message, re.IGNORECASE)
        table_match = re.search(r'table "([^"]+)" does not exist', error_message, re.IGNORECASE)
        relation_match = re.search(r'relation "([^"]+)" does not exist', error_message, re.IGNORECASE)

        if column_match:
            wrong_column = column_match.group(1)
            result['main_error'] = f'column "{wrong_column}" does not exist'
        elif table_match:
            wrong_table = table_match.group(1)
            result['main_error'] = f'table "{wrong_table}" does not exist'
        elif relation_match:
            wrong_relation = relation_match.group(1)
            result['main_error'] = f'relation "{wrong_relation}" does not exist'
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”™è¯¯ï¼Œå°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
            for line in lines:
                if 'psycopg2.errors' in line:
                    # æå–æ‹¬å·åçš„å†…å®¹
                    match = re.search(r'\)\s*(.+?)(?:\n|$)', line)
                    if match:
                        result['main_error'] = match.group(1).strip()
                        break
                elif line.strip() and not line.startswith('LINE') and not line.startswith('[SQL') and not line.startswith('HINT'):
                    result['main_error'] = line.strip()
                    break

        # æå–HINTä¿¡æ¯
        hint_match = re.search(r'HINT:\s*(.+?)(?:\n|$)', error_message, re.IGNORECASE)
        if hint_match:
            result['hint'] = hint_match.group(1).strip()

            # æ ¹æ®HINTç”Ÿæˆå»ºè®®
            if 'Perhaps you meant to reference the column' in result['hint']:
                # æå–å»ºè®®çš„åˆ—å
                column_match = re.search(r'column "([^"]+)"', result['hint'])
                if column_match:
                    suggested_column = column_match.group(1)
                    # æå–ç®€å•åˆ—åï¼ˆå»æ‰è¡¨åå‰ç¼€ï¼‰
                    simple_column = suggested_column.split('.')[-1] if '.' in suggested_column else suggested_column
                    result['suggestion'] = f"è¯·ä½¿ç”¨åˆ—å `{simple_column}` è€Œä¸æ˜¯é”™è¯¯çš„åˆ—åã€‚"

        # å¦‚æœæ˜¯åˆ—ä¸å­˜åœ¨é”™è¯¯ä½†æ²¡æœ‰HINTå»ºè®®
        if column_match and not result['suggestion']:
            wrong_column = column_match.group(1)
            result['suggestion'] = f"åˆ— `{wrong_column}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…åˆ—åã€‚"

        # å¦‚æœæ˜¯è¡¨ä¸å­˜åœ¨é”™è¯¯
        if table_match and not result['suggestion']:
            wrong_table = table_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_table}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

        # å¦‚æœæ˜¯relationä¸å­˜åœ¨é”™è¯¯ï¼ˆPostgreSQLçš„è¡¨ä¸å­˜åœ¨é”™è¯¯ï¼‰
        if relation_match and not result['suggestion']:
            wrong_relation = relation_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_relation}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

    except Exception as e:
        logger.warning(f"è§£æSQLé”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

    return result


async def _fix_sql_with_ai(
    original_sql: str,
    error_message: str,
    schema_context: str,
    original_question: str,
    db_type: str = "postgresql",  # æ•°æ®åº“ç±»å‹å‚æ•°
    tenant_id: str = "default_tenant"  # ç§Ÿæˆ·IDï¼Œç”¨äºllm_service
) -> Optional[str]:
    """
    ä½¿ç”¨AIä¿®å¤å¤±è´¥çš„SQLæŸ¥è¯¢ï¼ˆä¼˜å…ˆä½¿ç”¨DeepSeekï¼‰

    Args:
        original_sql: åŸå§‹SQLæŸ¥è¯¢
        error_message: é”™è¯¯ä¿¡æ¯
        schema_context: æ•°æ®åº“schemaä¸Šä¸‹æ–‡
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜
        db_type: æ•°æ®åº“ç±»å‹ï¼ˆpostgresql, mysql, sqlite, xlsx, csvç­‰ï¼‰
        tenant_id: ç§Ÿæˆ·ID

    Returns:
        ä¿®å¤åçš„SQLï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
    """
    try:
        # å°è¯•ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„æ•°æ®åº“ç‰¹å®šä¿®å¤æç¤ºè¯
        try:
            from src.app.services.prompt_generator import generate_sql_fix_prompt_with_db_type
            fix_prompt = generate_sql_fix_prompt_with_db_type(
                original_sql=original_sql,
                error_message=error_message,
                schema_context=schema_context,
                original_question=original_question,
                db_type=db_type
            )
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥ prompt_generator: {e}ï¼Œä½¿ç”¨é»˜è®¤PostgreSQLä¿®å¤æç¤º")
            # å›é€€åˆ°åŸæœ‰çš„ç¡¬ç¼–ç æç¤ºè¯ï¼ˆä»…é’ˆå¯¹å‡½æ•°ä¸å…¼å®¹é”™è¯¯ï¼‰
            error_details = _parse_sql_error(error_message)
            fix_prompt = _get_default_fix_prompt(
                original_sql, error_message, schema_context, original_question, error_details
            )
        except Exception as e:
            logger.warning(f"ç”ŸæˆåŠ¨æ€ä¿®å¤æç¤ºè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤PostgreSQLä¿®å¤æç¤º")
            # å›é€€åˆ°åŸæœ‰çš„ç¡¬ç¼–ç æç¤ºè¯
            error_details = _parse_sql_error(error_message)
            fix_prompt = _get_default_fix_prompt(
                original_sql, error_message, schema_context, original_question, error_details
            )

        # æ›´æ–° system prompt ä»¥åæ˜ æ•°æ®åº“ç±»å‹
        system_content = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¿®å¤ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®é”™è¯¯ä¿¡æ¯å’Œschemaä¿®å¤{db_type.upper()}æ•°æ®åº“çš„SQLæŸ¥è¯¢ã€‚"

        # ä½¿ç”¨ LLMMessage æ ¼å¼ï¼Œé€šè¿‡ llm_service è°ƒç”¨ï¼ˆä¼˜å…ˆä½¿ç”¨ DeepSeekï¼‰
        messages = [
            LLMMessage(role="system", content=system_content),
            LLMMessage(role="user", content=fix_prompt)
        ]

        # è°ƒç”¨ llm_service ä¿®å¤SQLï¼ˆè‡ªåŠ¨ä¼˜å…ˆä½¿ç”¨ DeepSeekï¼Œå›é€€åˆ° Zhipuï¼‰
        logger.info(f"ä½¿ç”¨ llm_service ä¿®å¤SQL (tenant_id={tenant_id})")
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=messages,
            max_tokens=1000,
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            stream=False
        )

        if response and response.content:
            fixed_sql = response.content.strip()

            # æ¸…ç†è¿”å›çš„SQL
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            fixed_sql = re.sub(r'```sql\s*', '', fixed_sql)
            fixed_sql = re.sub(r'```\s*', '', fixed_sql)
            fixed_sql = fixed_sql.strip()

            # æ£€æŸ¥æ˜¯å¦æ— æ³•ä¿®å¤
            if "CANNOT_FIX" in fixed_sql.upper():
                logger.warning("AIè¡¨ç¤ºæ— æ³•ä¿®å¤æ­¤SQL")
                return None

            # éªŒè¯æ˜¯å¦æ˜¯SELECTæŸ¥è¯¢
            if not fixed_sql.upper().startswith('SELECT'):
                logger.warning("ä¿®å¤åçš„SQLä¸æ˜¯SELECTæŸ¥è¯¢")
                return None

            logger.info(f"AIæˆåŠŸä¿®å¤SQL: {fixed_sql[:100]}...")
            return fixed_sql

        return None

    except Exception as e:
        logger.error(f"AIä¿®å¤SQLå¤±è´¥: {e}")
        return None


async def _execute_sql_on_file_datasource(
    connection_string: str,
    db_type: str,
    sql_query: str,
    data_source_name: str
) -> Dict[str, Any]:
    """
    åœ¨æ–‡ä»¶ç±»å‹æ•°æ®æºä¸Šæ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨duckdbï¼Œæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsï¼‰
        sql_query: SQLæŸ¥è¯¢è¯­å¥
        data_source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºè¡¨åï¼‰

    Returns:
        æŸ¥è¯¢ç»“æœå­—å…¸
    """
    try:
        # è§£æå­˜å‚¨è·¯å¾„
        if connection_string.startswith("file://"):
            storage_path = connection_string[7:]
        else:
            storage_path = connection_string

        file_data = None
        file_path = None

        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œåªæœ‰æœ¬åœ°ä¸å­˜åœ¨æ—¶æ‰ä» MinIO ä¸‹è½½
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆé€šå¸¸åœ¨ /app/uploads/ ç›®å½•ä¸‹ï¼‰
        if storage_path.startswith("/app/uploads/") or storage_path.startswith("/app/data/"):
            file_path = storage_path
            if os.path.exists(file_path):
                logger.info(f"ç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: {file_path}")
                try:
                    with open(file_path, 'rb') as f:
                        file_data = f.read()
                except Exception as e:
                    logger.warning(f"è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä» MinIO ä¸‹è½½")
                    file_data = None
            else:
                logger.info(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œå°è¯•ä» MinIO ä¸‹è½½")
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œä» MinIO ä¸‹è½½
        if not file_data:
            # ä»è·¯å¾„ä¸­æå–æ­£ç¡®çš„ object_nameï¼ˆå»æ‰ /app/uploads/ å‰ç¼€ï¼‰
            if storage_path.startswith("/app/uploads/"):
                # æå–ç›¸å¯¹äº uploads çš„è·¯å¾„ä½œä¸º object_name
                object_name = storage_path.replace("/app/uploads/", "", 1)
            elif storage_path.startswith("/app/data/"):
                # æå–ç›¸å¯¹äº data çš„è·¯å¾„ä½œä¸º object_name
                object_name = storage_path.replace("/app/data/", "", 1)
            else:
                # å¦‚æœè·¯å¾„ä¸åŒ…å« /app/uploads/ æˆ– /app/data/ï¼Œç›´æ¥ä½¿ç”¨åŸè·¯å¾„
                object_name = storage_path.lstrip("/")
            
            logger.info(f"ä»MinIOä¸‹è½½æ–‡ä»¶ç”¨äºSQLæ‰§è¡Œ: bucket=data-sources, object_name={object_name}")
            try:
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=object_name
                )
            except Exception as e:
                logger.error(f"ä»MinIOä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                file_data = None

        if not file_data:
            return {
                "success": False,
                "error": f"æ— æ³•è·å–æ–‡ä»¶: {storage_path} (æœ¬åœ°è·¯å¾„: {file_path if file_path else 'N/A'})",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        # åˆ›å»ºduckdbè¿æ¥
        conn = duckdb.connect(':memory:')
        registered_tables = []

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheetå¹¶æ³¨å†Œä¸ºä¸åŒçš„è¡¨
            try:
                # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"System Error: Missing dependency 'openpyxl'. Please install it: pip install openpyxl. Original error: {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            except Exception as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"Execution Error: Failed to read Excel file. {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            
            sheet_names = excel_file.sheet_names
            logger.info(f"ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨åï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
                    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œå› ä¸ºDuckDBæ”¯æŒä¸­æ–‡è¡¨å
                    clean_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', sheet_name)
                    if not clean_table_name or clean_table_name[0].isdigit():
                        clean_table_name = f"sheet_{clean_table_name}"

                    conn.register(clean_table_name, df)
                    registered_tables.append(clean_table_name)
                    logger.info(f"æ³¨å†Œè¡¨ '{clean_table_name}' (æ¥è‡ªSheet '{sheet_name}'): {len(df)}è¡Œ")

                    # åŒæ—¶ç”¨åŸå§‹Sheetåæ³¨å†Œï¼ˆå¦‚æœä¸åŒï¼‰
                    if sheet_name != clean_table_name:
                        try:
                            conn.register(sheet_name, df)
                            registered_tables.append(sheet_name)
                        except Exception:
                            pass  # å¦‚æœåŸåæ³¨å†Œå¤±è´¥ï¼Œå¿½ç•¥

                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

            # å¦‚æœç¬¬ä¸€ä¸ªSheetå­˜åœ¨ï¼Œä¹Ÿç”¨æ•°æ®æºåç§°æ³¨å†Œï¼ˆå‘åå…¼å®¹ï¼‰
            if sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    first_df = pd.read_excel(excel_file, sheet_name=0, engine='openpyxl')
                    if not first_df.empty:
                        ds_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                        if ds_table_name not in registered_tables:
                            conn.register(ds_table_name, first_df)
                            registered_tables.append(ds_table_name)
                except Exception as e:
                    logger.warning(f"æ³¨å†Œæ•°æ®æºåç§°è¡¨å¤±è´¥: {e}")
                    pass

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                try:
                    df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if df is not None:
                table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                if not table_name or table_name[0].isdigit():
                    table_name = f"data_{table_name}"
                conn.register(table_name, df)
                registered_tables.append(table_name)
                # åŒæ—¶æ³¨å†Œä¸º 'data' ä½œä¸ºå¤‡ç”¨
                conn.register('data', df)

        if not registered_tables:
            conn.close()
            return {
                "success": False,
                "error": f"æ— æ³•ä»æ–‡ä»¶è¯»å–ä»»ä½•æ•°æ®: {storage_path}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        logger.info(f"æˆåŠŸæ³¨å†Œ {len(registered_tables)} ä¸ªè¡¨: {registered_tables}")

        # æ‰§è¡ŒSQLæŸ¥è¯¢
        try:
            result_df = conn.execute(sql_query).fetchdf()
        except Exception as sql_error:
            error_msg = str(sql_error)
            logger.warning(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«å¯ç”¨çš„è¡¨å
            conn.close()
            return {
                "success": False,
                "error": f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}\n\nå¯ç”¨çš„è¡¨: {', '.join(registered_tables)}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        conn.close()

        # è½¬æ¢ç»“æœä¸ºå­—å…¸åˆ—è¡¨
        columns = list(result_df.columns)
        data = result_df.to_dict('records')

        logger.info(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(data)} è¡Œ")

        return {
            "success": True,
            "data": data,
            "columns": columns,
            "row_count": len(data)
        }

    except Exception as e:
        logger.error(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡Œå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": [],
            "columns": [],
            "row_count": 0
        }


async def _execute_sql_if_needed(
    content: str,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None
) -> str:
    """
    æ£€æµ‹AIå›å¤ä¸­çš„SQLæŸ¥è¯¢å¹¶æ‰§è¡Œï¼Œå°†ç»“æœæ’å…¥å›å¤ä¸­ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰

    Args:
        content: AIç”Ÿæˆçš„å›å¤å†…å®¹
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº

    Returns:
        å¢å¼ºåçš„å›å¤å†…å®¹ï¼ˆåŒ…å«æŸ¥è¯¢ç»“æœï¼‰
    """
    try:
        # æ£€æµ‹SQLä»£ç å—
        sql_pattern = r'```sql\s*(.*?)\s*```'
        sql_matches = re.findall(sql_pattern, content, re.DOTALL | re.IGNORECASE)

        if not sql_matches:
            return content

        # æ‹†åˆ†æ¯ä¸ªä»£ç å—ä¸­å¯èƒ½åŒ…å«çš„å¤šä¸ªSQLè¯­å¥ï¼Œå¹¶å»é‡
        seen_sqls = set()
        unique_sql_matches = []
        for sql_block in sql_matches:
            # æ‹†åˆ†ä¸€ä¸ªä»£ç å—ä¸­çš„å¤šä¸ªSQLè¯­å¥
            individual_statements = _split_multiple_sql_statements(sql_block)
            logger.info(f"ä»ä»£ç å—ä¸­æ‹†åˆ†å‡º {len(individual_statements)} ä¸ªSQLè¯­å¥")
            
            for sql in individual_statements:
                normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
                if normalized_sql not in seen_sqls:
                    seen_sqls.add(normalized_sql)
                    unique_sql_matches.append(sql)
                else:
                    logger.warning(f"æ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

        sql_matches = unique_sql_matches
        logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

        # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )

        if not data_sources:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
            return content + "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚"

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
        if data_source_ids:
            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            if matching_sources:
                data_source = matching_sources[0]
                logger.info(f"ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
            else:
                data_source = data_sources[0]
                logger.warning(f"æœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
        else:
            data_source = data_sources[0]

        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
        connection_string = await data_source_service.get_decrypted_connection_string(
            data_source_id=data_source.id,
            tenant_id=tenant_id,
            db=db
        )

        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢
        enhanced_content = content
        for sql_query in sql_matches:
            current_sql = sql_query.strip()
            retry_count = 0
            max_retries = 2
            last_error = None
            execution_success = False

            while retry_count <= max_retries and not execution_success:
                try:
                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢ï¼ˆåŒ…æ‹¬WITH...SELECTçš„CTEæŸ¥è¯¢ï¼‰
                    # ä½¿ç”¨ç»Ÿä¸€çš„æ³¨é‡Šå»é™¤å’Œæ£€æŸ¥å‡½æ•°
                    sql_for_check, is_select, debug_msg = _strip_sql_comments_and_check_select(current_sql)
                    logger.debug(f"SQLæ£€æµ‹ç»“æœ: {debug_msg}")
                    
                    if not is_select:
                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:100]}")
                        logger.warning(f"æ£€æµ‹è¯¦æƒ…: {debug_msg}")
                        break

                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                        logger.info(f"ä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                        result = await _execute_sql_on_file_datasource(
                            connection_string=connection_string,
                            db_type=data_source.db_type,
                            sql_query=current_sql,
                            data_source_name=data_source.name
                        )
                        if not result.get("success", False) and result.get("error"):
                            raise Exception(result["error"])
                    else:
                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                        # é¢„å¤„ç†ï¼šå»é™¤AIå¯èƒ½é”™è¯¯æ·»åŠ çš„æ•°æ®åº“åå‰ç¼€
                        if data_source.database_name:
                            current_sql = _remove_database_name_prefix(current_sql, data_source.database_name)
                        
                        adapter = PostgreSQLAdapter(connection_string)
                        try:
                            await adapter.connect()
                            query_result = await adapter.execute_query(current_sql)
                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            result = {
                                "data": query_result.data,
                                "columns": query_result.columns,
                                "row_count": query_result.row_count
                            }
                        finally:
                            await adapter.disconnect()

                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                    row_count = len(result.get("data", []))

                    if result.get("data") and row_count > 0:
                        # è·å–åˆ—åï¼šä¼˜å…ˆä½¿ç”¨columnså­—æ®µï¼Œå¦åˆ™ä»æ•°æ®ä¸­æå–
                        columns = result.get("columns") or list(result["data"][0].keys())
                        # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                        result_text = "\n\n| " + " | ".join(columns) + " |\n"
                        result_text += "|" + "|".join(["---" for _ in columns]) + "|\n"

                        for row in result["data"][:10]:
                            row_values = [str(row.get(col, "")) for col in columns]
                            result_text += "| " + " | ".join(row_values) + " |\n"

                        # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                        if row_count > 10:
                            result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                    else:
                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ›¿æ¢ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                    if retry_count > 0:
                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                        # å®Œå…¨æ›¿æ¢åŸå§‹SQLå—ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                        sql_block = f"```sql\n{sql_query}\n```"
                        fixed_sql_block = f"**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            fixed_sql_block + result_text
                        )

                        # ğŸ”§ æ–°å¢ï¼šè®°å½•SQLé”™è¯¯åˆ°é”™è¯¯è®°å¿†ç³»ç»Ÿ
                        try:
                            error_memory_service = SQLErrorMemoryService(db)
                            await error_memory_service.record_error(
                                tenant_id=tenant_id,
                                original_query=sql_query,
                                error_message=last_error,
                                fixed_query=current_sql,
                                table_name=_extract_table_name_from_sql(sql_query),
                                schema_context=None  # å¯é€‰ï¼šå¯ä»¥ä¼ é€’schemaä¸Šä¸‹æ–‡
                            )
                            logger.info("SQLé”™è¯¯å·²è®°å½•åˆ°é”™è¯¯è®°å¿†ç³»ç»Ÿ")
                        except Exception as record_error:
                            logger.warning(f"è®°å½•SQLé”™è¯¯å¤±è´¥: {record_error}")
                    else:
                        # æ²¡æœ‰é‡è¯•ï¼Œç›´æ¥å°†ç»“æœæ’å…¥åˆ°SQLä»£ç å—åé¢
                        sql_block = f"```sql\n{sql_query}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            sql_block + result_text
                        )

                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result.get('data', []))} è¡Œ")
                    execution_success = True

                except Exception as e:
                    last_error = str(e)
                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                    if retry_count < max_retries:
                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                        fixed_sql = await _fix_sql_with_ai(
                            original_sql=current_sql,
                            error_message=last_error,
                            schema_context=schema_context,
                            original_question=original_question,
                            db_type=data_source.db_type,  # ä¼ é€’æ•°æ®åº“ç±»å‹
                            tenant_id=tenant_id  # ä¼ é€’ç§Ÿæˆ·IDç”¨äºllm_service
                        )

                        if fixed_sql:
                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                            current_sql = fixed_sql
                            retry_count += 1
                        else:
                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                            break
                    else:
                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                        break

            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if not execution_success and last_error:
                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                error_details = _parse_sql_error(last_error)

                # æ„å»ºé”™è¯¯ä¿¡æ¯
                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                if error_details.get('hint'):
                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                if retry_count > 0:
                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                # æ·»åŠ å»ºè®®
                if error_details.get('suggestion'):
                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                else:
                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                # æ›¿æ¢åŸå§‹SQLå—
                sql_block = f"```sql\n{sql_query}\n```"
                if retry_count > 0:
                    # å¦‚æœç»è¿‡é‡è¯•ï¼Œå®Œå…¨æ›¿æ¢ä¸ºé”™è¯¯ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºåŸå§‹SQLï¼‰
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        f"**âš ï¸ åŸå§‹SQLæœ‰è¯¯ï¼Œå°è¯•ä¿®å¤åä»ç„¶å¤±è´¥ï¼š**\n{error_text}"
                    )
                else:
                    # æ²¡æœ‰é‡è¯•ï¼Œåœ¨åŸå§‹SQLåæ·»åŠ é”™è¯¯ä¿¡æ¯
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        sql_block + error_text
                    )

        return enhanced_content

    except Exception as e:
        logger.error(f"SQLæ‰§è¡Œå¤„ç†å¤±è´¥: {e}")
        return content


def _convert_response(response: LLMResponse) -> ChatCompletionResponse:
    """è½¬æ¢å“åº”æ ¼å¼"""
    return ChatCompletionResponse(
        content=response.content,
        thinking=response.thinking,
        usage=response.usage,
        model=response.model,
        provider=response.provider,
        finish_reason=response.finish_reason,
        created_at=response.created_at
    )


async def _execute_tool_call(
    tool_call: Dict[str, Any],
    tenant_id: str,
    db: Session,
    data_source_ids: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆç›®å‰åªæ”¯æŒ execute_sqlï¼‰
    
    Returns:
        Dict with keys: success, result, error
    """
    try:
        tool_name = tool_call.get("function", {}).get("name", "")
        tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
        
        # è§£æå·¥å…·å‚æ•°
        try:
            tool_args = json.loads(tool_args_str)
        except json.JSONDecodeError:
            return {
                "success": False,
                "result": None,
                "error": f"æ— æ³•è§£æå·¥å…·å‚æ•°: {tool_args_str}"
            }
        
        if tool_name == "execute_sql":
            sql_query = tool_args.get("sql_query", "")
            if not sql_query:
                return {
                    "success": False,
                    "result": None,
                    "error": "SQLæŸ¥è¯¢è¯­å¥ä¸ºç©º"
                }
            
            # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ£€æµ‹å‡½æ•°å¤„ç†æ³¨é‡Šï¼‰
            _, is_select, debug_msg = _strip_sql_comments_and_check_select(sql_query)
            logger.debug(f"execute_sql SQLæ£€æµ‹: {debug_msg}")
            if not is_select:
                return {
                    "success": False,
                    "result": None,
                    "error": "åªå…è®¸æ‰§è¡Œ SELECT æŸ¥è¯¢ï¼Œç¦æ­¢æ‰§è¡Œä¿®æ”¹æ“ä½œ"
                }
            
            # è·å–æ•°æ®æº
            data_sources = await data_source_service.get_data_sources(
                tenant_id=tenant_id,
                db=db,
                active_only=True
            )
            
            if not data_sources:
                return {
                    "success": False,
                    "result": None,
                    "error": "æœªæ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æº"
                }
            
            # é€‰æ‹©æ•°æ®æº
            if data_source_ids:
                matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
                if matching_sources:
                    data_source = matching_sources[0]
                else:
                    data_source = data_sources[0]
            else:
                data_source = data_sources[0]
            
            # è·å–è¿æ¥å­—ç¬¦ä¸²
            connection_string = await data_source_service.get_decrypted_connection_string(
                data_source_id=data_source.id,
                tenant_id=tenant_id,
                db=db
            )
            
            # æ‰§è¡ŒSQL
            try:
                if data_source.db_type in ["xlsx", "xls", "csv"]:
                    # æ–‡ä»¶ç±»å‹æ•°æ®æº
                    result = await _execute_sql_on_file_datasource(
                        connection_string=connection_string,
                        db_type=data_source.db_type,
                        sql_query=sql_query,
                        data_source_name=data_source.name
                    )
                    if not result.get("success", False):
                        return {
                            "success": False,
                            "result": None,
                            "error": result.get("error", "æ‰§è¡Œå¤±è´¥")
                        }
                else:
                    # æ•°æ®åº“ç±»å‹æ•°æ®æº
                    # é¢„å¤„ç†ï¼šå»é™¤AIå¯èƒ½é”™è¯¯æ·»åŠ çš„æ•°æ®åº“åå‰ç¼€
                    processed_sql = sql_query
                    if data_source.database_name:
                        processed_sql = _remove_database_name_prefix(sql_query, data_source.database_name)
                    
                    adapter = PostgreSQLAdapter(connection_string)
                    try:
                        await adapter.connect()
                        query_result = await adapter.execute_query(processed_sql)
                        result = {
                            "data": query_result.data,
                            "columns": query_result.columns,
                            "row_count": query_result.row_count
                        }
                    finally:
                        await adapter.disconnect()
                
                # æ ¼å¼åŒ–ç»“æœä¸ºJSONå­—ç¬¦ä¸²
                result_json = json.dumps(result, ensure_ascii=False, default=str)
                return {
                    "success": True,
                    "result": result_json,
                    "error": None
                }
            except Exception as e:
                logger.error(f"æ‰§è¡ŒSQLå¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "result": None,
                    "error": str(e)
                }
        
        elif tool_name == "generate_chart":
            # å¤„ç†å›¾è¡¨ç”Ÿæˆå·¥å…·è°ƒç”¨
            try:
                chart_type = tool_args.get("chart_type", "bar")
                title = tool_args.get("title", "æ•°æ®å›¾è¡¨")
                x_data = tool_args.get("x_data", [])
                y_data = tool_args.get("y_data", [])
                series_name = tool_args.get("series_name", "æ•°æ®")
                
                logger.info(f"ç”Ÿæˆå›¾è¡¨: type={chart_type}, title={title}, x_data_len={len(x_data)}, y_data_len={len(y_data)}")
                
                # æ ¹æ®å›¾è¡¨ç±»å‹ç”Ÿæˆ ECharts é…ç½®
                if chart_type == "pie":
                    # é¥¼å›¾éœ€è¦ç‰¹æ®Šçš„æ•°æ®æ ¼å¼
                    pie_data = [{"name": x, "value": y} for x, y in zip(x_data, y_data)]
                    echarts_option = {
                        "title": {"text": title, "left": "center"},
                        "tooltip": {"trigger": "item", "formatter": "{b}: {c} ({d}%)"},
                        "legend": {"orient": "vertical", "left": "left"},
                        "series": [{
                            "type": "pie",
                            "radius": "50%",
                            "data": pie_data,
                            "emphasis": {
                                "itemStyle": {
                                    "shadowBlur": 10,
                                    "shadowOffsetX": 0,
                                    "shadowColor": "rgba(0, 0, 0, 0.5)"
                                }
                            }
                        }]
                    }
                else:
                    # æŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾
                    echarts_option = {
                        "title": {"text": title, "left": "center"},
                        "tooltip": {"trigger": "axis"},
                        "xAxis": {"type": "category", "data": x_data},
                        "yAxis": {"type": "value"},
                        "series": [{
                            "name": series_name,
                            "type": chart_type,
                            "data": y_data,
                            "smooth": True if chart_type == "line" else False
                        }]
                    }
                
                # è¿”å› ECharts é…ç½®
                result = {
                    "chart_generated": True,
                    "echarts_option": echarts_option
                }
                
                return {
                    "success": True,
                    "result": json.dumps(result, ensure_ascii=False),
                    "error": None,
                    "echarts_option": echarts_option  # é¢å¤–è¿”å›é…ç½®ï¼Œæ–¹ä¾¿ç›´æ¥ä½¿ç”¨
                }
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "result": None,
                    "error": f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}"
                }
        
        else:
            return {
                "success": False,
                "result": None,
                "error": f"æœªçŸ¥çš„å·¥å…·: {tool_name}"
            }
    except Exception as e:
        logger.error(f"æ‰§è¡Œå·¥å…·è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "result": None,
            "error": str(e)
        }


def _create_processing_step(
    step: int,
    title: str,
    description: str,
    status: str = "running",
    duration: int = None,
    details: str = None,
    tenant_id: str = None,
    content_type: str = None,
    content_data: dict = None
) -> str:
    """
    åˆ›å»ºå¤„ç†æ­¥éª¤äº‹ä»¶çš„JSONå­—ç¬¦ä¸²

    Args:
        step: æ­¥éª¤ç¼–å·
        title: æ­¥éª¤æ ‡é¢˜
        description: æ­¥éª¤æè¿°
        status: æ­¥éª¤çŠ¶æ€ (pending/running/completed/error)
        duration: è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        details: è¯¦æƒ…æ–‡æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
        tenant_id: ç§Ÿæˆ·ID
        content_type: å†…å®¹ç±»å‹ (sql/table/chart/error)
        content_data: å†…å®¹æ•°æ®å­—å…¸

    Returns:
        str: SSEæ ¼å¼çš„JSONå­—ç¬¦ä¸²
    """
    step_data = {
        "type": "processing_step",
        "step": {
            "step": step,
            "title": title,
            "description": description,
            "status": status,
            "timestamp": datetime.now().isoformat(),
        },
        "tenant_id": tenant_id
    }
    if duration is not None:
        step_data["step"]["duration"] = duration
    if details:
        step_data["step"]["details"] = details
    # æ–°å¢ï¼šæ”¯æŒå¯Œå†…å®¹ç±»å‹
    if content_type:
        step_data["step"]["content_type"] = content_type
    if content_data:
        step_data["step"]["content_data"] = content_data

    # æ·»åŠ æ—¥å¿—ç¡®è®¤æ­¥éª¤å‘é€
    logger.info(f"ğŸ“¤ å‘é€å¤„ç†æ­¥éª¤ {step}: {title} [{status}]")
    if content_type:
        logger.info(f"   â””â”€ content_type: {content_type}")
    return f"data: {json.dumps(step_data, ensure_ascii=False)}\n\n"


async def _stream_general_chat_generator(
    stream_generator,
    tenant_id: str,
    original_question: str = "",
    has_data_source: bool = False
):
    """
    æ™®é€šå¯¹è¯çš„æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆåŠ¨æ€æ­¥éª¤æµç¨‹ï¼‰

    æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€ç”Ÿæˆä¸åŒçš„æ­¥éª¤ï¼š
    - ç®€å•é—®å€™ï¼ˆä½ å¥½ã€è°¢è°¢ï¼‰: 2æ­¥
    - SchemaæŸ¥è¯¢ï¼ˆæœ‰å“ªäº›è¡¨ï¼‰: 3æ­¥
    - æ•°æ®æŸ¥è¯¢: 5æ­¥
    - å¯è§†åŒ–éœ€æ±‚: 6æ­¥
    - æ™®é€šå¯¹è¯ï¼ˆé»˜è®¤ï¼‰: 6æ­¥

    Args:
        stream_generator: LLMæµå¼è¾“å‡ºç”Ÿæˆå™¨
        tenant_id: ç§Ÿæˆ·ID
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜
        has_data_source: æ˜¯å¦æœ‰å¯ç”¨çš„æ•°æ®æº
    """
    from src.app.services.processing_steps import (
        ProcessingStepBuilder,
        classify_question,
        QuestionType
    )

    # 1. åˆ†ç±»é—®é¢˜ç±»å‹
    question_type = classify_question(original_question, has_data_source)
    logger.info(f"[DYNAMIC_STEPS] Question type: {question_type.value}, has_data_source: {has_data_source}")

    # 2. æ„å»ºåŠ¨æ€æ­¥éª¤
    builder = ProcessingStepBuilder()
    steps_config = builder.build_dynamic_steps(
        question_type=question_type,
        question=original_question,
        has_context=False
    )

    # ========== å‘é€è¿æ¥åˆå§‹åŒ–äº‹ä»¶ ==========
    init_event = {
        "type": "connection_init",
        "message": "Stream connection established",
        "tenant_id": tenant_id
    }
    yield f"data: {json.dumps(init_event, ensure_ascii=False)}\n\n"
    await asyncio.sleep(0.05)

    # ========== åŠ¨æ€å‘é€æ­¥éª¤ ==========
    # å‘é€é™¤æœ€åä¸€ä¸ªæ­¥éª¤å¤–çš„æ‰€æœ‰æ­¥éª¤ï¼ˆæ ‡è®°ä¸ºå·²å®Œæˆï¼‰
    step_count = len(steps_config)
    for i, step_cfg in enumerate(steps_config):
        if i < step_count - 1:
            # å‰é¢çš„æ­¥éª¤æ ‡è®°ä¸ºå·²å®Œæˆ
            yield _create_processing_step(
                step=step_cfg.step,
                title=step_cfg.title,
                description=step_cfg.description,
                status="completed",
                duration=step_cfg.duration or 100,
                tenant_id=tenant_id
            )
            await asyncio.sleep(0.05)
        else:
            # æœ€åä¸€ä¸ªæ­¥éª¤æ ‡è®°ä¸ºè¿è¡Œä¸­ï¼ˆLLMç”Ÿæˆä¸­ï¼‰
            last_step_number = step_cfg.step
            yield _create_processing_step(
                step=last_step_number,
                title=step_cfg.title,
                description=step_cfg.description,
                status="running",
                tenant_id=tenant_id
            )
            await asyncio.sleep(0.05)
            break  # å¼€å§‹LLMç”Ÿæˆ

    # ========== æ”¶é›†LLMè¾“å‡º ==========
    full_content = ""
    llm_start_time = time.time()

    # ç”¨äºç´¯ç§¯å’Œæ›´æ–°æœ€åæ­¥éª¤çš„å†…å®¹é¢„è§ˆ
    last_step_content_preview = ""
    last_update_time = time.time()

    async for chunk in stream_generator:
        if chunk.type == "content":
            full_content += chunk.content

            # å®æ—¶å‘é€content deltaåˆ°å‰ç«¯
            content_delta = {
                "type": "content_delta",
                "delta": chunk.content,
                "provider": chunk.provider,
                "tenant_id": tenant_id
            }
            yield f"data: {json.dumps(content_delta, ensure_ascii=False)}\n\n"

            # å®šæœŸæ›´æ–°æœ€åæ­¥éª¤çš„æè¿°ï¼Œæ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            last_step_content_preview += chunk.content
            current_time = time.time()
            if current_time - last_update_time >= 0.1:  # 100msé—´éš”
                # ç”Ÿæˆå†…å®¹é¢„è§ˆï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                preview_text = last_step_content_preview[-150:] if len(last_step_content_preview) > 150 else last_step_content_preview
                # æ¸…ç†é¢„è§ˆæ–‡æœ¬
                preview_text = preview_text.replace("\n", " ").strip()

                step_update = {
                    "type": "step_update",
                    "step": last_step_number,
                    "description": f"æ­£åœ¨ç”Ÿæˆå›å¤... {len(last_step_content_preview)} å­—ç¬¦",
                    "content_preview": preview_text,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(step_update, ensure_ascii=False)}\n\n"
                last_update_time = current_time

        elif chunk.type == "thinking":
            # å‘é€thinkingäº‹ä»¶
            chunk_data = {
                "type": "thinking",
                "delta": chunk.content,
                "provider": chunk.provider,
                "finished": chunk.finished,
                "tenant_id": tenant_id
            }
            yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
        elif chunk.type == "error":
            error_data = {
                "type": "error",
                "message": chunk.content,
                "tenant_id": tenant_id
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

    llm_duration = int((time.time() - llm_start_time) * 1000)

    # ========== å†…å®¹ç”Ÿæˆå®Œæˆï¼Œå®Œæˆæœ€åä¸€ä¸ªæ­¥éª¤ ==========
    # è·å–æœ€åä¸€ä¸ªæ­¥éª¤é…ç½®
    last_step_cfg = steps_config[-1]
    yield _create_processing_step(
        step=last_step_number,
        title=last_step_cfg.title,
        description="å›å¤å·²å®Œæˆ",
        status="completed",
        duration=llm_duration,
        content_type="text",
        content_data={"text": full_content},
        tenant_id=tenant_id
    )
    await asyncio.sleep(0.05)

    # ========== å‘é€å®Œæˆä¿¡å· ==========
    done_event = {"type": "done", "tenant_id": tenant_id}
    yield f"data: {json.dumps(done_event, ensure_ascii=False)}\n\n"


async def _stream_response_generator(
    stream_generator,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None,
    initial_messages: Optional[List[LLMMessage]] = None,
    schema_info: Optional[dict] = None,  # Schemaè·å–ä¿¡æ¯
    question_type: Optional[Any] = None  # ğŸ”§ æ–°å¢ï¼šé—®é¢˜ç±»å‹ï¼Œç”¨äºå†³å®šæ˜¯å¦ç”Ÿæˆå›¾è¡¨
):
    """
    æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆæ–¹æ¡ˆBï¼šSQL ä»£ç å—æ£€æµ‹æ¨¡å¼ï¼‰
    
    ä¸ä½¿ç”¨ Function Callingï¼Œè€Œæ˜¯æ£€æµ‹ AI è¾“å‡ºä¸­çš„ ```sql ... ``` ä»£ç å—ï¼Œ
    è‡ªåŠ¨æ‰§è¡Œ SQL æŸ¥è¯¢å¹¶è¿›è¡Œç¬¬äºŒæ¬¡ LLM è°ƒç”¨ã€‚
    
    å®Œæ•´çš„6ä¸ªæ­¥éª¤ï¼š
    1. ç†è§£ç”¨æˆ·é—®é¢˜
    2. è·å–æ•°æ®åº“Schema
    3. æ„å»ºAI Prompt
    4. AIç”ŸæˆSQL
    5. æå–SQLè¯­å¥
    6. æ‰§è¡ŒSQLæŸ¥è¯¢
    """
    try:
        # ğŸ”§ å¯¼å…¥QuestionTypeç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦å›¾è¡¨
        from src.app.services.processing_steps import QuestionType

        # ğŸ”§ åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆå›¾è¡¨ï¼ˆåªæœ‰VISUALIZATIONç±»å‹éœ€è¦å›¾è¡¨ï¼‰
        # DATA_QUERY: 5æ­¥ï¼Œä¸ç”Ÿæˆå›¾è¡¨
        # VISUALIZATION: 6-8æ­¥ï¼Œç”Ÿæˆå›¾è¡¨
        # SCHEMA_QUERY: 3æ­¥ï¼Œä¸ç”Ÿæˆå›¾è¡¨
        should_generate_chart = question_type == QuestionType.VISUALIZATION

        logger.info(f"[_stream_response_generator] question_type={question_type.value if question_type else 'None'}, should_generate_chart={should_generate_chart}")

        # ğŸ”§ğŸ”§ğŸ”§ æ£€æµ‹å›¾è¡¨æ‹†åˆ†è¯·æ±‚ï¼ˆé‡è¦ï¼ï¼‰
        # å½“ç”¨æˆ·è¯´"æŠŠå›¾åˆ†å¼€"ã€"æ‹†åˆ†"ã€"åˆ†åˆ«æ˜¾ç¤º"ç­‰å…³é”®è¯æ—¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        CHART_SPLIT_KEYWORDS = ["åˆ†å¼€", "æ‹†åˆ†", "åˆ†åˆ«æ˜¾ç¤º", "å•ç‹¬å±•ç¤º", "å•ç‹¬æ˜¾ç¤º", "å„è‡ªæ˜¾ç¤º", "æ‹†æˆ", "å•ç‹¬ç”»", "å„è‡ªç”»"]
        is_split_request = False
        chart_count = None  # ğŸ”´ ç”¨æˆ·æŒ‡å®šçš„å›¾è¡¨æ•°é‡
        if original_question:
            is_split_request = any(keyword in original_question for keyword in CHART_SPLIT_KEYWORDS)

            # ğŸ”´ğŸ”´ğŸ”´ æ£€æµ‹ç”¨æˆ·æŒ‡å®šçš„å›¾è¡¨æ•°é‡
            if is_split_request:
                # re æ¨¡å—å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
                number_patterns = [
                    r'æ‹†(?:åˆ†)?(?:æˆ)?([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ä¸ª',
                    r'åˆ†æˆ([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ä¸ª',
                    r'åˆ†[åˆ«æˆ]([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ä¸ª',
                    r'åˆ†åˆ«æ˜¾ç¤º([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ä¸ª',
                    r'å•ç‹¬å±•ç¤º([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ä¸ª',
                ]
                for pattern in number_patterns:
                    match = re.search(pattern, original_question)
                    if match:
                        num_str = match.group(1)
                        cn_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                                  'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
                                  '1': 1, '2': 2, '3': 3, '4': 4, '5': 5,
                                  '6': 6, '7': 7, '8': 8, '9': 9, '10': 10}
                        chart_count = cn_nums.get(num_str, int(num_str) if num_str.isdigit() else None)
                        if chart_count:
                            logger.info(f"ğŸ” [å›¾è¡¨æ•°é‡æ£€æµ‹] åŒ¹é…å€¼: {num_str} â†’ {chart_count} ä¸ªå›¾è¡¨")
                            break

        if is_split_request:
            count_info = f", è¦æ±‚ç”Ÿæˆ {chart_count} ä¸ªå›¾è¡¨" if chart_count else ""
            logger.info(f"ğŸ”§ğŸ”§ğŸ”§ æ£€æµ‹åˆ°å›¾è¡¨æ‹†åˆ†è¯·æ±‚{count_info}ï¼original_question={original_question[:50]}")
        else:
            logger.debug(f"æœªæ£€æµ‹åˆ°æ‹†åˆ†è¯·æ±‚ï¼Œoriginal_question={original_question[:50]}")

        # æ”¶é›†å®Œæ•´çš„å“åº”å†…å®¹
        full_content = ""
        thinking_content = ""
        
        # æ¶ˆæ¯å†å²ï¼ˆç”¨äºäºŒæ¬¡è°ƒç”¨ï¼‰
        messages = initial_messages or []
        
        # ========== é¦–å…ˆå‘é€è¿æ¥åˆå§‹åŒ–äº‹ä»¶ ==========
        # è¿™ä¸ªäº‹ä»¶ç¡®ä¿ SSE è¿æ¥å®Œå…¨å»ºç«‹åå†å‘é€é‡è¦æ•°æ®
        init_event = {
            "type": "connection_init",
            "message": "Stream connection established",
            "tenant_id": tenant_id
        }
        yield f"data: {json.dumps(init_event, ensure_ascii=False)}\n\n"
        # ç»™å‰ç«¯è¶³å¤Ÿæ—¶é—´å¤„ç†è¿æ¥å»ºç«‹
        await asyncio.sleep(0.05)
        
        # ========== Step 1: ç†è§£ç”¨æˆ·é—®é¢˜ ==========
        logger.info(f"ğŸš€ å¼€å§‹å‘é€æ­¥éª¤1-4ï¼Œoriginal_question={original_question[:30] if original_question else 'None'}")
        step_start_time = time.time()
        yield _create_processing_step(
            step=1,
            title="ç†è§£ç”¨æˆ·é—®é¢˜",
            description=f"åˆ†æé—®é¢˜: {original_question[:50]}..." if len(original_question) > 50 else f"åˆ†æé—®é¢˜: {original_question}",
            status="completed",
            duration=int((time.time() - step_start_time) * 1000),
            details=f"ç”¨æˆ·é—®é¢˜: {original_question}",
            tenant_id=tenant_id
        )
        # ç¡®ä¿äº‹ä»¶è¢«åˆ·æ–°åˆ°å®¢æˆ·ç«¯
        await asyncio.sleep(0.05)

        # ========== Step 2: è·å–æ•°æ®åº“Schema ==========
        if schema_info:
            schema_duration = schema_info.get("duration_ms", 0)
            schema_length = schema_info.get("length", 0)
            schema_tables = schema_info.get("tables", [])
            data_source_name = schema_info.get("data_source_name", "æœªçŸ¥")
            
            tables_preview = ", ".join(schema_tables[:5])
            if len(schema_tables) > 5:
                tables_preview += f" ç­‰{len(schema_tables)}ä¸ªè¡¨"
            
            yield _create_processing_step(
                step=2,
                title="è·å–æ•°æ®åº“Schema",
                description=f"ä» {data_source_name} è·å–åˆ° {len(schema_tables)} ä¸ªè¡¨ç»“æ„",
                status="completed",
                duration=schema_duration,
                details=f"æ•°æ®æº: {data_source_name}\nè¡¨: {tables_preview}\nSchemaå¤§å°: {schema_length} å­—ç¬¦",
                tenant_id=tenant_id
            )
        else:
            yield _create_processing_step(
                step=2,
                title="è·å–æ•°æ®åº“Schema",
                description="å·²è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯",
                status="completed",
                tenant_id=tenant_id
            )
        # ç¡®ä¿äº‹ä»¶è¢«åˆ·æ–°åˆ°å®¢æˆ·ç«¯
        await asyncio.sleep(0.05)

        # ========== Step 3: æ„å»ºAI Prompt ==========
        prompt_start_time = time.time()
        system_msg_content = ""
        for msg in messages:
            if msg.role == "system":
                system_msg_content = msg.content  # æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­
                break
        
        yield _create_processing_step(
            step=3,
            title="æ„å»ºAI Prompt",
            description="å°†Schemaæ³¨å…¥ç³»ç»Ÿæç¤ºè¯",
            status="completed",
            duration=int((time.time() - prompt_start_time) * 1000),
            details=f"System Prompt:\n{system_msg_content}",
            tenant_id=tenant_id
        )
        # ç¡®ä¿äº‹ä»¶è¢«åˆ·æ–°åˆ°å®¢æˆ·ç«¯
        await asyncio.sleep(0.05)

        # ========== Step 4: AIç”ŸæˆSQL ==========
        ai_start_time = time.time()
        yield _create_processing_step(
            step=4,
            title="AIç”ŸæˆSQL",
            description="æ­£åœ¨æ ¹æ®æ•°æ®åº“Schemaç”ŸæˆSQLæŸ¥è¯¢...",
            status="running",
            tenant_id=tenant_id
        )
        # ç¡®ä¿äº‹ä»¶è¢«åˆ·æ–°åˆ°å®¢æˆ·ç«¯
        await asyncio.sleep(0.05)

        # ğŸ”§ æ–°å¢ï¼šç”¨äºç´¯ç§¯å’Œæ›´æ–°æ­¥éª¤4çš„å†…å®¹é¢„è§ˆ
        step4_content_preview = ""
        last_update_time = time.time()

        async for chunk in stream_generator:
            # å¤„ç†æ™®é€šå†…å®¹
            if chunk.type == "content":
                full_content += chunk.content

                # ğŸ”§ æ–°å¢ï¼šå®æ—¶å‘é€content deltaåˆ°å‰ç«¯
                # ä½¿ç”¨content_deltaäº‹ä»¶ç±»å‹ï¼Œé¿å…ä¸æœ€ç»ˆcontentå†²çª
                content_delta = {
                    "type": "content_delta",
                    "delta": chunk.content,
                    "provider": chunk.provider,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(content_delta, ensure_ascii=False)}\n\n"

                # ğŸ”§ æ–°å¢ï¼šå®šæœŸæ›´æ–°æ­¥éª¤4çš„æè¿°ï¼Œæ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                # æ¯100msæ›´æ–°ä¸€æ¬¡é¢„è§ˆï¼Œé¿å…è¿‡äºé¢‘ç¹
                step4_content_preview += chunk.content
                current_time = time.time()
                if current_time - last_update_time >= 0.1:  # 100msé—´éš”
                    # ç”Ÿæˆå†…å®¹é¢„è§ˆï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                    preview_text = step4_content_preview[-200:] if len(step4_content_preview) > 200 else step4_content_preview
                    # æ¸…ç†é¢„è§ˆæ–‡æœ¬ï¼Œç§»é™¤markdownä»£ç å—æ ‡è®°ç­‰
                    preview_text = preview_text.replace("```sql", "").replace("```", "").strip()

                    step_update = {
                        "type": "step_update",
                        "step": 4,
                        "description": f"æ­£åœ¨ç”ŸæˆSQL... {len(step4_content_preview)} å­—ç¬¦",
                        "content_preview": preview_text,
                        "tenant_id": tenant_id
                    }
                    yield f"data: {json.dumps(step_update, ensure_ascii=False)}\n\n"
                    last_update_time = current_time

            elif chunk.type == "thinking":
                thinking_content += chunk.content
                # å‘é€thinking chunk
                chunk_data = {
                    "type": chunk.type,
                    "content": chunk.content,
                    "provider": chunk.provider,
                    "finished": chunk.finished,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

            # å¦‚æœæµç»“æŸï¼Œæ£€æµ‹å¹¶æ‰§è¡Œ SQLï¼ˆæ–¹æ¡ˆBï¼‰
            if chunk.finished:
                logger.info(f"æµå¼å“åº”å®Œæˆï¼Œæ£€æµ‹SQLæŸ¥è¯¢ã€‚å†…å®¹é•¿åº¦: {len(full_content)}")

                # æ£€æµ‹SQLä»£ç å—ï¼ˆæå‰æ£€æµ‹ï¼Œç”¨äºæ­¥éª¤4çš„content_dataï¼‰
                sql_pattern = r'```sql\s*(.*?)\s*```'
                sql_matches = re.findall(sql_pattern, full_content, re.DOTALL | re.IGNORECASE)

                # ========== Step 4å®Œæˆ: AIç”ŸæˆSQL ==========
                # å‡†å¤‡SQLçš„content_data
                sql_content_data = None
                if sql_matches:
                    sql_content_data = {"sql": sql_matches[0].strip()}

                yield _create_processing_step(
                    step=4,
                    title="AIç”ŸæˆSQL",
                    description="SQLæŸ¥è¯¢å·²ç”Ÿæˆ",
                    status="completed",
                    duration=int((time.time() - ai_start_time) * 1000),
                    details=f"AIå›å¤é•¿åº¦: {len(full_content)} å­—ç¬¦",
                    tenant_id=tenant_id,
                    content_type="sql" if sql_content_data else None,
                    content_data=sql_content_data
                )

                # ğŸ”§ ä¿®å¤ï¼šåœ¨å¤–å±‚åˆå§‹åŒ–å›¾è¡¨ç”Ÿæˆæ ‡å¿—ï¼Œç¡®ä¿fallbackè·¯å¾„å¯ä»¥è®¿é—®
                chart_already_generated = False

                if sql_matches:
                    # ========== Step 5: æå–SQLè¯­å¥ ==========
                    yield _create_processing_step(
                        step=5,
                        title="æå–SQLè¯­å¥",
                        description=f"æ­£åœ¨ä»AIå›å¤ä¸­æå–SQLä»£ç å—...",
                        status="running",
                        tenant_id=tenant_id
                    )
                    
                    # æ‹†åˆ†æ¯ä¸ªä»£ç å—ä¸­å¯èƒ½åŒ…å«çš„å¤šä¸ªSQLè¯­å¥ï¼Œå¹¶å»é‡
                    seen_sqls = set()
                    unique_sql_matches = []
                    for sql_block in sql_matches:
                        # æ‹†åˆ†ä¸€ä¸ªä»£ç å—ä¸­çš„å¤šä¸ªSQLè¯­å¥
                        individual_statements = _split_multiple_sql_statements(sql_block)
                        logger.info(f"æµå¼å“åº”ï¼šä»ä»£ç å—ä¸­æ‹†åˆ†å‡º {len(individual_statements)} ä¸ªSQLè¯­å¥")
                        
                        for sql in individual_statements:
                            normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
                            if normalized_sql not in seen_sqls:
                                seen_sqls.add(normalized_sql)
                                unique_sql_matches.append(sql)
                            else:
                                logger.warning(f"æµå¼å“åº”ï¼šæ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

                    sql_matches = unique_sql_matches
                    logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")
                    
                    # ========== Step 5å®Œæˆ: æå–SQLè¯­å¥ ==========
                    # æ ¼å¼åŒ–SQLé¢„è§ˆ
                    sql_preview = sql_matches[0].strip()
                    if len(sql_preview) > 300:
                        sql_preview = sql_preview[:300] + "\n..."
                    
                    yield _create_processing_step(
                        step=5,
                        title="æå–SQLè¯­å¥",
                        description=f"æˆåŠŸæå– {len(sql_matches)} ä¸ªSQLæŸ¥è¯¢",
                        status="completed",
                        details=f"SQLè¯­å¥:\n{sql_preview}",
                        tenant_id=tenant_id
                    )

                    # ========== Step 6: æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆæˆ–è¿”å›ç»“æœï¼Œå–å†³äºæ˜¯å¦éœ€è¦å›¾è¡¨ï¼‰==========
                    ds_start_time = time.time()
                    step6_title = "æ‰§è¡ŒSQLæŸ¥è¯¢" if should_generate_chart else "è¿”å›ç»“æœ"
                    step6_desc = "æ­£åœ¨è¿æ¥æ•°æ®æºå¹¶æ‰§è¡ŒæŸ¥è¯¢..." if should_generate_chart else "æ­£åœ¨æ•´ç†æŸ¥è¯¢ç»“æœ..."
                    yield _create_processing_step(
                        step=6,
                        title=step6_title,
                        description=step6_desc,
                        status="running",
                        tenant_id=tenant_id
                    )
                    
                    # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
                    data_sources = await data_source_service.get_data_sources(
                        tenant_id=tenant_id,
                        db=db,
                        active_only=True
                    )

                    if data_sources:
                        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                        if data_source_ids:
                            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
                            if matching_sources:
                                data_source = matching_sources[0]
                                logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
                            else:
                                data_source = data_sources[0]
                                logger.warning(f"æµå¼å“åº”ï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
                        else:
                            data_source = data_sources[0]
                        
                        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
                        connection_string = await data_source_service.get_decrypted_connection_string(
                            data_source_id=data_source.id,
                            tenant_id=tenant_id,
                            db=db
                        )

                        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
                        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

                        # æ›´æ–°Step 6è¿›åº¦
                        exec_start_time = time.time()
                        step6_title = "æ‰§è¡ŒSQLæŸ¥è¯¢" if should_generate_chart else "è¿”å›ç»“æœ"
                        step6_desc = f"å·²è¿æ¥ {data_source.name}ï¼Œæ­£åœ¨æ‰§è¡Œ {len(sql_matches)} ä¸ªæŸ¥è¯¢..." if should_generate_chart else f"å·²ä» {data_source.name} è·å–æŸ¥è¯¢ç»“æœ..."
                        yield _create_processing_step(
                            step=6,
                            title=step6_title,
                            description=step6_desc,
                            status="running",
                            details=f"æ•°æ®æº: {data_source.name}\nç±»å‹: {data_source.db_type}",
                            tenant_id=tenant_id
                        )

                        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
                        total_rows = 0
                        # ğŸ”§ ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰SQLæ‰§è¡Œç»“æœï¼Œåªåœ¨å…¨éƒ¨å¤±è´¥æ—¶æ˜¾ç¤ºé”™è¯¯
                        all_sql_results = []  # å­˜å‚¨æ¯ä¸ªSQLçš„æ‰§è¡Œç»“æœ {'success': bool, 'sql': str, 'result': dict, 'error': str}
                        any_sql_success = False  # æ ‡è®°æ˜¯å¦æœ‰ä»»ä½•SQLæˆåŠŸ
                        # ğŸ”§ é‡æ„ï¼šæ”¶é›†æ‰€æœ‰æˆåŠŸçš„SQLç»“æœï¼Œç”¨äºå¾ªç¯ç»“æŸåç»Ÿä¸€ç”Ÿæˆå›¾è¡¨
                        successful_query_results = []  # å­˜å‚¨æˆåŠŸçš„æŸ¥è¯¢ç»“æœ [{'sql': str, 'result': dict, 'columns': list}]

                        # ğŸ”§ æ–°å¢ï¼šç¬¬6æ­¥æµå¼è¾“å‡º - ç”¨äºè®°å½•æ‰§è¡Œè¿›åº¦
                        step6_query_index = 0
                        step6_total_queries = len(sql_matches)

                        for sql_query in sql_matches:
                            current_sql = sql_query.strip()
                            retry_count = 0
                            max_retries = 2
                            last_error = None
                            execution_success = False
                            step6_query_index += 1

                            while retry_count <= max_retries and not execution_success:
                                try:
                                    # ğŸ”§ æµå¼è¾“å‡ºï¼šæ­£åœ¨éªŒè¯SQLè¯­å¥
                                    step6_update = {
                                        "type": "step_update",
                                        "step": 6,
                                        "description": f"æ­£åœ¨éªŒè¯SQLè¯­å¥ ({step6_query_index}/{step6_total_queries})...",
                                        "content_preview": current_sql[:100] + ("..." if len(current_sql) > 100 else ""),
                                        "streaming": True,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(step6_update, ensure_ascii=False)}\n\n"

                                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢ï¼ˆåŒ…æ‹¬WITH...SELECTçš„CTEæŸ¥è¯¢ï¼‰
                                    # ä½¿ç”¨ç»Ÿä¸€çš„æ³¨é‡Šå»é™¤å’Œæ£€æŸ¥å‡½æ•°
                                    sql_for_check, is_select, debug_msg = _strip_sql_comments_and_check_select(current_sql)
                                    logger.info(f"[æµå¼SQLæ£€æµ‹] {debug_msg}")

                                    if not is_select:
                                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:100]}")
                                        logger.warning(f"æ£€æµ‹è¯¦æƒ…: {debug_msg}")
                                        break

                                    # ğŸ”§ æµå¼è¾“å‡ºï¼šæ­£åœ¨å»ºç«‹æ•°æ®åº“è¿æ¥
                                    step6_update = {
                                        "type": "step_update",
                                        "step": 6,
                                        "description": f"æ­£åœ¨è¿æ¥ {data_source.name}...",
                                        "content_preview": f"æ•°æ®æºç±»å‹: {data_source.db_type}",
                                        "streaming": True,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(step6_update, ensure_ascii=False)}\n\n"

                                    # ğŸ”§ æµå¼è¾“å‡ºï¼šæ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢
                                    step6_update = {
                                        "type": "step_update",
                                        "step": 6,
                                        "description": f"æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢ ({step6_query_index}/{step6_total_queries})...",
                                        "content_preview": f"æ‰§è¡Œ {data_source.db_type.upper()} æŸ¥è¯¢ä¸­...",
                                        "streaming": True,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(step6_update, ensure_ascii=False)}\n\n"

                                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                                        logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                                        result = await _execute_sql_on_file_datasource(
                                            connection_string=connection_string,
                                            db_type=data_source.db_type,
                                            sql_query=current_sql,
                                            data_source_name=data_source.name
                                        )
                                        if not result.get("success", False) and result.get("error"):
                                            raise Exception(result["error"])
                                    else:
                                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                                        # é¢„å¤„ç†ï¼šå»é™¤AIå¯èƒ½é”™è¯¯æ·»åŠ çš„æ•°æ®åº“åå‰ç¼€
                                        if data_source.database_name:
                                            current_sql = _remove_database_name_prefix(current_sql, data_source.database_name)

                                        adapter = PostgreSQLAdapter(connection_string)
                                        try:
                                            await adapter.connect()
                                            query_result = await adapter.execute_query(current_sql)
                                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                                            result = {
                                                "data": query_result.data,
                                                "columns": query_result.columns,
                                                "row_count": query_result.row_count
                                            }
                                        finally:
                                            await adapter.disconnect()

                                    # ğŸ”§ æµå¼è¾“å‡ºï¼šæ­£åœ¨å¤„ç†ç»“æœé›†
                                    row_count_preview = result.get('row_count', 0)
                                    step6_update = {
                                        "type": "step_update",
                                        "step": 6,
                                        "description": f"æ­£åœ¨å¤„ç†ç»“æœé›†...",
                                        "content_preview": f"å·²è·å– {row_count_preview} è¡Œæ•°æ®ï¼Œæ­£åœ¨æ ¼å¼åŒ–...",
                                        "streaming": True,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(step6_update, ensure_ascii=False)}\n\n"

                                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                                    row_count = result.get('row_count', 0)

                                    if result.get('data'):
                                        data = result['data'][:10]
                                        if data:
                                            headers = list(data[0].keys())
                                            # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                                            result_text = "\n\n| " + " | ".join(headers) + " |\n"
                                            result_text += "|" + "|".join(["---"] * len(headers)) + "|\n"

                                            for row in data:
                                                values = [str(row.get(h, '')) for h in headers]
                                                result_text += "| " + " | ".join(values) + " |\n"

                                            # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                                            if row_count > 10:
                                                result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                                        else:
                                            result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"
                                    else:
                                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œå‘é€ä¿®å¤è¯´æ˜
                                    if retry_count > 0:
                                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                                        # å‘é€ä¿®å¤åçš„SQL
                                        fix_info = f"\n\n**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```\n"
                                        fix_chunk = {
                                            "type": "content",
                                            "content": fix_info,
                                            "provider": chunk.provider,
                                            "finished": False,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(fix_chunk, ensure_ascii=False)}\n\n"

                                        # ğŸ”§ æ–°å¢ï¼šè®°å½•SQLé”™è¯¯åˆ°é”™è¯¯è®°å¿†ç³»ç»Ÿ
                                        logger.info(f"[SQLé”™è¯¯è®°å¿†] å‡†å¤‡è®°å½•SQLé”™è¯¯ï¼retry_count={retry_count}, tenant_id={tenant_id[:20] if tenant_id else None}")
                                        try:
                                            error_memory_service = SQLErrorMemoryService(db)
                                            await error_memory_service.record_error(
                                                tenant_id=tenant_id,
                                                original_query=sql_query,
                                                error_message=last_error,
                                                fixed_query=current_sql,
                                                table_name=_extract_table_name_from_sql(sql_query),
                                                schema_context=None
                                            )
                                            logger.info("[æµå¼ç”Ÿæˆ] SQLé”™è¯¯å·²è®°å½•åˆ°é”™è¯¯è®°å¿†ç³»ç»Ÿ")
                                        except Exception as record_error:
                                            logger.warning(f"[æµå¼ç”Ÿæˆ] è®°å½•SQLé”™è¯¯å¤±è´¥: {record_error}")

                                    # ğŸ”§ ä¿®å¤ï¼šä¸å†å‘é€ result_text ä½œä¸º contentï¼Œé¿å…ä¸ ProcessingSteps æ­¥éª¤6é‡å¤
                                    # è¡¨æ ¼æ•°æ®å°†é€šè¿‡æ­¥éª¤6 (content_type: 'table') æ˜¾ç¤º
                                    # result_chunk = {
                                    #     "type": "content",
                                    #     "content": result_text,
                                    #     "provider": chunk.provider,
                                    #     "finished": False,
                                    #     "tenant_id": tenant_id
                                    # }
                                    # yield f"data: {json.dumps(result_chunk, ensure_ascii=False)}\n\n"

                                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {result.get('row_count', 0)} è¡Œ")
                                    total_rows += row_count
                                    execution_success = True
                                    any_sql_success = True  # ğŸ”§ æ ‡è®°æœ‰SQLæˆåŠŸæ‰§è¡Œ

                                    # ğŸ”§ é‡æ„ï¼šæ”¶é›†æˆåŠŸçš„SQLç»“æœï¼Œç”¨äºåç»­ç»Ÿä¸€ç”Ÿæˆå›¾è¡¨
                                    successful_query_results.append({
                                        'sql': current_sql,
                                        'result': result,
                                        'columns': result.get('columns', []),
                                        'row_count': row_count
                                    })

                                    # ========== Step 6å®Œæˆ: æ‰§è¡ŒSQLæŸ¥è¯¢ ==========
                                    # å‡†å¤‡è¡¨æ ¼æ•°æ®çš„content_data
                                    table_content_data = None
                                    if result.get('data') and result.get('columns'):
                                        # é™åˆ¶æœ€å¤š50è¡Œç”¨äºå‰ç«¯æ˜¾ç¤º
                                        rows_for_display = result['data'][:50]
                                        table_content_data = {
                                            "table": {
                                                "columns": result['columns'],
                                                "rows": rows_for_display,
                                                "row_count": row_count
                                            }
                                        }

                                    yield _create_processing_step(
                                        step=6,
                                        title="è¿”å›ç»“æœ" if not should_generate_chart else "æ‰§è¡ŒSQLæŸ¥è¯¢",
                                        description=f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {row_count} è¡Œæ•°æ®",
                                        status="completed",
                                        duration=int((time.time() - exec_start_time) * 1000),
                                        details=f"æ•°æ®æº: {data_source.name}\nè¿”å›è¡Œæ•°: {row_count}\næ‰§è¡Œè€—æ—¶: {int((time.time() - exec_start_time) * 1000)}ms",
                                        tenant_id=tenant_id,
                                        content_type="table" if table_content_data else None,
                                        content_data=table_content_data
                                    )

                                    # ğŸ”§ é‡æ„ï¼šäºŒæ¬¡LLMè°ƒç”¨ç§»åˆ°å¾ªç¯ç»“æŸåç»Ÿä¸€å¤„ç†
                                    # è¿™é‡Œä¸å†åšä»»ä½•å¤„ç†ï¼Œç­‰å¾…æ‰€æœ‰SQLæ‰§è¡Œå®Œæ¯•åç»Ÿä¸€ç”Ÿæˆå›¾è¡¨
                                    # æ—§ä»£ç ï¼ˆå·²åºŸå¼ƒï¼‰ï¼šåªå¯¹ç¬¬ä¸€ä¸ªæˆåŠŸçš„SQLç”Ÿæˆå›¾è¡¨
                                    if False:  # ğŸ”§ ç¦ç”¨å¾ªç¯å†…çš„äºŒæ¬¡LLMè°ƒç”¨
                                        logger.info("å¼€å§‹äºŒæ¬¡LLMè°ƒç”¨ï¼šåˆ†ææ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨")
                                        
                                        # --- ğŸ§  æ•°æ®ç‰¹å¾åˆ†æä¸å†³ç­–æ³¨å…¥ ---
                                        data_for_analysis = result['data'][:20]  # æœ€å¤šå–20è¡Œç”¨äºåˆ†æ
                                        analysis_row_count = len(result['data'])
                                        
                                        # è·å–åˆ—ä¿¡æ¯
                                        columns = result.get('columns', [])
                                        if not columns and data_for_analysis:
                                            columns = list(data_for_analysis[0].keys())
                                        col_count = len(columns)
                                        
                                        # ç®€å•çš„ç±»å‹æ¨æ–­ï¼šæ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¶é—´å…³é”®è¯
                                        col_names_str = " ".join([str(c).lower() for c in columns])
                                        has_time_col = any(k in col_names_str for k in ['date', 'time', 'year', 'month', 'day', 'quarter', 'week', 'æ—¥æœŸ', 'æ—¶é—´', 'å¹´', 'æœˆ', 'æ—¥'])
                                        has_metric_col = col_count >= 2  # å‡è®¾é™¤äº†ç»´åº¦è¿˜æœ‰æŒ‡æ ‡
                                        
                                        # æ£€æµ‹å±‚çº§ç»“æ„æ•°æ®ï¼šparent_id, ä¸€çº§/äºŒçº§åˆ†ç±», levelç­‰å…³é”®è¯
                                        has_hierarchy_col = any(k in col_names_str for k in [
                                            'parent', 'child', 'level', 'depth', 'hierarchy', 
                                            'ä¸€çº§', 'äºŒçº§', 'ä¸‰çº§', 'çˆ¶', 'å­', 'å±‚çº§', 'åˆ†ç±»', 'category',
                                            'parent_id', 'parent_name', 'subcategory', 'å­ç±»', 'ç»“æ„'
                                        ])
                                        
                                        # æ£€æµ‹æ’åç±»æ•°æ®ï¼šTop Nã€æœ€é«˜/æœ€ä½ N ä¸ªã€æ’åç­‰
                                        original_question_lower = original_question.lower() if original_question else ""
                                        sql_lower = current_sql.lower() if current_sql else ""
                                        is_ranking_query = any(k in original_question_lower for k in [
                                            'top', 'æœ€é«˜', 'æœ€ä½', 'æœ€å¤š', 'æœ€å°‘', 'æ’å', 'å‰å‡ ', 'å‰5', 'å‰10',
                                            'è¯„åˆ†æœ€é«˜', 'è¯„åˆ†æœ€ä½', 'é”€é‡æœ€é«˜', 'é”€é‡æœ€ä½', 'æ’è¡Œ', 'æ’åº'
                                        ]) or ('order by' in sql_lower and 'limit' in sql_lower)
                                        
                                        analysis_directive = ""
                                        supplementary_stats = ""

                                        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœé—®é¢˜ç±»å‹ä¸éœ€è¦å›¾è¡¨ï¼ˆå¦‚SCHEMA_QUERYï¼‰ï¼Œå¼ºåˆ¶ç¦æ­¢ç”Ÿæˆå›¾è¡¨
                                        if not should_generate_chart:
                                            analysis_directive = (
                                                "ğŸ›‘ **CONSTRAINT**: SchemaæŸ¥è¯¢ä¸éœ€è¦å›¾è¡¨.\n"
                                                "- **DO NOT** generate any chart.\n"
                                                "- **DO NOT** explain why you are not generating a chart. Just skip it silently.\n\n"
                                                "- Focus on listing the tables and their structure clearly.\n"
                                            )
                                        # è§„åˆ™ 1: å•è¡Œæ•°æ®ï¼ˆèšåˆç»“æœï¼‰-> ç¦æ­¢ç”»å›¾ï¼Œä½†è¦å±•ç¤ºè®¡ç®—è¿‡ç¨‹
                                        elif analysis_row_count <= 1:
                                            # ğŸ”§ æ‰§è¡Œè¡¥å……æŸ¥è¯¢è·å–ç»Ÿè®¡ä¿¡æ¯
                                            try:
                                                # ä»SQLä¸­æå–è¡¨å
                                                table_match = re.search(r'\bFROM\s+([a-zA-Z_][a-zA-Z0-9_]*)', current_sql, re.IGNORECASE)
                                                if table_match:
                                                    table_name = table_match.group(1)
                                                    # æ„å»ºç»Ÿè®¡æŸ¥è¯¢
                                                    stats_sql = f"SELECT COUNT(*) as æ€»è®°å½•æ•° FROM {table_name}"
                                                    
                                                    # æ£€æµ‹åŸSQLä¸­ä½¿ç”¨çš„é‡‘é¢/æ•°é‡å­—æ®µ
                                                    amount_match = re.search(r'SUM\s*\(\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\)', current_sql, re.IGNORECASE)
                                                    if amount_match:
                                                        amount_field = amount_match.group(1)
                                                        stats_sql = f"""SELECT 
                                                            COUNT(*) as æ€»è®°å½•æ•°, 
                                                            MIN({amount_field}) as æœ€å°å€¼, 
                                                            MAX({amount_field}) as æœ€å¤§å€¼, 
                                                            ROUND(AVG({amount_field})::numeric, 2) as å¹³å‡å€¼
                                                        FROM {table_name}"""
                                                    
                                                    logger.info(f"æ‰§è¡Œè¡¥å……ç»Ÿè®¡æŸ¥è¯¢: {stats_sql}")
                                                    
                                                    # æ‰§è¡Œç»Ÿè®¡æŸ¥è¯¢
                                                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                                                        stats_result = await _execute_sql_on_file_datasource(
                                                            connection_string=connection_string,
                                                            db_type=data_source.db_type,
                                                            sql_query=stats_sql,
                                                            data_source_name=data_source.name
                                                        )
                                                    else:
                                                        stats_adapter = PostgreSQLAdapter(connection_string)
                                                        try:
                                                            await stats_adapter.connect()
                                                            stats_query_result = await stats_adapter.execute_query(stats_sql)
                                                            stats_result = {
                                                                "data": stats_query_result.data,
                                                                "columns": stats_query_result.columns
                                                            }
                                                        finally:
                                                            await stats_adapter.disconnect()
                                                    
                                                    # æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯
                                                    if stats_result.get('data') and len(stats_result['data']) > 0:
                                                        stats_data = stats_result['data'][0]
                                                        stats_parts = []
                                                        for key, value in stats_data.items():
                                                            if value is not None:
                                                                # æ ¼å¼åŒ–æ•°å­—
                                                                if isinstance(value, (int, float)):
                                                                    formatted_value = f"{value:,.2f}" if isinstance(value, float) else f"{value:,}"
                                                                else:
                                                                    formatted_value = str(value)
                                                                stats_parts.append(f"{key}: {formatted_value}")
                                                        supplementary_stats = " | ".join(stats_parts)
                                                        logger.info(f"è¡¥å……ç»Ÿè®¡ä¿¡æ¯: {supplementary_stats}")
                                            except Exception as stats_error:
                                                logger.warning(f"è·å–è¡¥å……ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats_error}")
                                            
                                            # æ„å»ºå¸¦æœ‰ç»Ÿè®¡ä¿¡æ¯çš„åˆ†ææŒ‡ä»¤
                                            stats_context = ""
                                            if supplementary_stats:
                                                stats_context = f"\n\nğŸ“Š **è¡¥å……ç»Ÿè®¡æ•°æ®**ï¼ˆå·²è‡ªåŠ¨æŸ¥è¯¢ï¼‰:\n{supplementary_stats}\nè¯·åœ¨å›ç­”ä¸­å¼•ç”¨è¿™äº›æ•°æ®æ¥å±•ç¤ºè®¡ç®—è¿‡ç¨‹ã€‚"
                                            
                                            analysis_directive = (
                                                "ğŸ›‘ **CONSTRAINT**: The result contains only 1 row (aggregated result like SUM, COUNT, AVG).\n"
                                                "- **DO NOT** generate any chart.\n"
                                                "- **DO NOT** explain why you are not generating a chart. Just skip it silently.\n\n"
                                                "ğŸ“Š **IMPORTANT - å±•ç¤ºè®¡ç®—è¿‡ç¨‹**:\n"
                                                "- åœ¨ç»™å‡ºæœ€ç»ˆç»“æœä¹‹å‰ï¼Œå…ˆç”¨è¡¥å……ç»Ÿè®¡æ•°æ®è¯´æ˜è®¡ç®—ä¾æ®\n"
                                                "- æ ¼å¼è¦æ±‚ï¼š\n"
                                                "  ğŸ“‹ **è®¡ç®—ä¾æ®**ï¼šå…± X æ¡è®°å½•\n"
                                                "  ğŸ’° **æ•°æ®èŒƒå›´**ï¼šæœ€å°å€¼ Â¥X ~ æœ€å¤§å€¼ Â¥Xï¼ˆå¹³å‡ Â¥Xï¼‰\n"
                                                "  ğŸ“ˆ **æœ€ç»ˆç»“æœ**ï¼šÂ¥X,XXX,XXX"
                                                f"{stats_context}"
                                            )
                                        elif not has_metric_col and analysis_row_count < 50:
                                            analysis_directive = (
                                                "ğŸ›‘ **CONSTRAINT**: This appears to be a text list without numerical metrics.\n"
                                                "- **DO NOT** generate any chart.\n"
                                                "- **DO NOT** explain why you are not generating a chart. Just skip it silently.\n"
                                                "- Summarize the list content (e.g., total count, examples)."
                                            )
                                        # è§„åˆ™ 2: å±‚çº§ç»“æ„æ•°æ® -> ä½¿ç”¨æ ‘çŠ¶å›¾
                                        elif has_hierarchy_col:
                                            analysis_directive = (
                                                "âœ… **STRATEGY**: This is hierarchical/tree-structured data (parent-child relationship).\n"
                                                "- **ACTION**: You MUST call `generate_chart` with `chart_type='tree'`.\n"
                                                "- The tree chart is perfect for showing category structures, organizational hierarchies, etc.\n"
                                                "- **Analysis**: Describe the hierarchy structure, levels, and distribution."
                                            )
                                        # è§„åˆ™ 2.5: æ’åç±»æ•°æ® -> å¼ºåˆ¶ä½¿ç”¨æŸ±çŠ¶å›¾ï¼ˆä¸ç”¨é¥¼å›¾ï¼‰
                                        elif is_ranking_query and analysis_row_count > 1:
                                            # æ‰¾åˆ°æœ€å¯èƒ½çš„åç§°åˆ—å’Œæ•°å€¼åˆ—
                                            name_col = None
                                            value_col = None
                                            for col in columns:
                                                col_lower = str(col).lower()
                                                if col_lower in ['name', 'product_name', 'åç§°', 'äº§å“å', 'å•†å“å', 'title']:
                                                    name_col = col
                                                elif col_lower in ['count', 'total', 'sum', 'amount', 'quantity', 'review_count', 'sales', 'æ•°é‡', 'é‡‘é¢', 'é”€é‡', 'è¯„ä»·æ•°']:
                                                    value_col = col
                                            
                                            chart_hint = ""
                                            if name_col and value_col:
                                                chart_hint = f"\n- **Hint**: Use '{name_col}' for X-axis labels and '{value_col}' for Y-axis values."
                                            elif name_col:
                                                chart_hint = f"\n- **Hint**: Use '{name_col}' for X-axis labels. Find a numeric column for Y-axis."
                                            
                                            analysis_directive = (
                                                "âœ… **STRATEGY**: This is RANKING data (Top N, highest/lowest).\n"
                                                "- **ACTION**: You MUST generate a bar chart using [CHART_START]...[CHART_END] format.\n"
                                                "- âš ï¸ DO NOT skip chart generation! This ranking data NEEDS visualization.\n"
                                                "- âš ï¸ DO NOT use pie chart! Ranking data shows absolute values, not proportions.\n"
                                                f"{chart_hint}\n"
                                                "- **Analysis**: Compare the values, highlight the leader and gaps between ranks."
                                            )
                                        # è§„åˆ™ 3: å¤§æ•°æ®é‡ -> å¼ºåˆ¶ Top N æŸ±çŠ¶å›¾
                                        elif analysis_row_count > 20 and not has_time_col:
                                            analysis_directive = (
                                                f"âš ï¸ **CONSTRAINT**: The result has {analysis_row_count} rows, which is too many for a clean chart.\n"
                                                "- **ACTION**: You MUST generate a bar chart using [CHART_START]...[CHART_END] format.\n"
                                                "- âš ï¸ DO NOT skip chart generation! Only include the **Top 10** data points.\n"
                                                "- In your text analysis, mention that you are showing the top performers."
                                            )
                                        # è§„åˆ™ 4: æ—¶é—´åºåˆ— -> å¼ºåˆ¶æŠ˜çº¿å›¾
                                        elif has_time_col and analysis_row_count > 1:
                                            analysis_directive = (
                                                "âœ… **STRATEGY**: This is time-series data (trend analysis).\n"
                                                "- **ACTION**: You MUST generate a line chart using [CHART_START]...[CHART_END] format.\n"
                                                "- âš ï¸ DO NOT skip chart generation! Time-series data ALWAYS needs a trend chart.\n"
                                                "- Use 'line' chart type to show the trend over time.\n"
                                                "- **Analysis**: Focus on the trend (upward/downward), seasonality, or spikes."
                                            )
                                        # è§„åˆ™ 5: åˆ†ç±»å¯¹æ¯”æˆ–å…¶ä»–å¤šè¡Œæ•°æ® -> é»˜è®¤ç”Ÿæˆå›¾è¡¨
                                        else:
                                            chart_suggestion = "pie" if analysis_row_count <= 5 else "bar"
                                            analysis_directive = (
                                                f"âœ… **STRATEGY**: This data has {analysis_row_count} rows and can be visualized.\n"
                                                f"- **ACTION**: You MUST generate a chart using [CHART_START]...[CHART_END] format.\n"
                                                f"- âš ï¸ DO NOT skip chart generation! Use '{chart_suggestion}' chart type.\n"
                                                "- **Analysis**: Compare the magnitudes. Identify the leader and the laggard."
                                            )
                                        
                                        logger.info(f"æ•°æ®ç‰¹å¾åˆ†æ: rows={analysis_row_count}, cols={col_count}, has_time={has_time_col}, has_metric={has_metric_col}")
                                        
                                        # æ„å»ºåˆ†ææç¤ºï¼ˆåŒ…å«å†³ç­–æŒ‡ä»¤ï¼‰
                                        # å°† Decimal ç±»å‹è½¬æ¢ä¸º floatï¼Œé¿å… JSON åºåˆ—åŒ–å¤±è´¥
                                        serializable_data = _convert_decimal_to_float(data_for_analysis)
                                        data_json = json.dumps(serializable_data, ensure_ascii=False, indent=2)
                                        
                                        analysis_prompt = f"""ä½ åˆšåˆšæŸ¥è¯¢äº†æ•°æ®ï¼Œç»“æœå¦‚ä¸‹ï¼š

```json
{data_json}
```

--- ANALYSIS INSTRUCTIONS ---
{analysis_directive}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **æ•°æ®åˆ†æ**ï¼šç”¨ 2-3 å¥è¯åˆ†ææ•°æ®çš„å…³é”®æ´å¯Ÿï¼Œè§£é‡Šæ•°æ®çš„å•†ä¸šå«ä¹‰ï¼ˆä¸è¦åªé‡å¤æ•°å­—ï¼‰

2. **ç”Ÿæˆ ECharts å›¾è¡¨é…ç½®**ï¼ˆå¦‚æœä¸Šè¿°æŒ‡ä»¤å…è®¸ï¼‰ï¼š
   
   âš ï¸ **æ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
   - å¿…é¡»ä½¿ç”¨ `[CHART_START]` å¼€å§‹ï¼Œ`[CHART_END]` ç»“æŸ
   - ä¸­é—´æ˜¯**çº¯JSONæ ¼å¼çš„EChartsé…ç½®**
   - ä¸è¦ä½¿ç”¨ markdown ä»£ç å—åŒ…è£¹ JSON
   - **ç¦æ­¢ä½¿ç”¨JavaScriptå‡½æ•°**ï¼formatteråªèƒ½ç”¨å­—ç¬¦ä¸²æ¨¡æ¿ï¼Œå¦‚ "{{b}}: {{c}}"
   
   âœ… **æ­£ç¡®ç¤ºä¾‹ï¼ˆæŠ˜çº¿å›¾ï¼‰**ï¼š
[CHART_START]
{{"title":{{"text":"æœˆåº¦é”€å”®è¶‹åŠ¿","left":"center"}},"tooltip":{{"trigger":"axis","formatter":"{{b}}: {{c}}å…ƒ"}},"xAxis":{{"type":"category","data":["1æœˆ","2æœˆ","3æœˆ"]}},"yAxis":{{"type":"value","name":"é”€å”®é¢"}},"series":[{{"name":"é”€å”®é¢","type":"line","data":[12000,15000,18000]}}]}}
[CHART_END]

   âœ… **æ­£ç¡®ç¤ºä¾‹ï¼ˆæŸ±çŠ¶å›¾ï¼‰**ï¼š
[CHART_START]
{{"title":{{"text":"å•†å“åº“å­˜æ’å"}},"tooltip":{{"trigger":"axis"}},"xAxis":{{"type":"category","data":["åä¸ºMateBook","iPhone 15","å°ç±³ç”µè§†"]}},"yAxis":{{"type":"value","name":"åº“å­˜æ•°é‡"}},"series":[{{"name":"åº“å­˜","type":"bar","data":[100,80,50]}}]}}
[CHART_END]

   âœ… **æ­£ç¡®ç¤ºä¾‹ï¼ˆæ ‘çŠ¶å›¾ - é€‚ç”¨äºå±‚çº§/åˆ†ç±»ç»“æ„æ•°æ®ï¼‰**ï¼š
[CHART_START]
{{"title":{{"text":"äº§å“ç±»åˆ«ç»“æ„","left":"center"}},"tooltip":{{"trigger":"item"}},"series":[{{"type":"tree","data":[{{"name":"å…¨éƒ¨ç±»åˆ«","children":[{{"name":"ç”µå­äº§å“","children":[{{"name":"æ‰‹æœºé€šè®¯","value":3}},{{"name":"ç”µè„‘åŠå…¬","value":2}},{{"name":"æ•°ç é…ä»¶","value":5}}]}},{{"name":"æœè£…é‹åŒ…","children":[{{"name":"ç”·è£…","value":4}},{{"name":"å¥³è£…","value":6}}]}},{{"name":"å®¶å±…ç”Ÿæ´»","value":8}}]}}],"top":"5%","left":"10%","bottom":"5%","right":"10%","symbol":"circle","symbolSize":10,"orient":"TB","label":{{"position":"top","fontSize":12}},"leaves":{{"label":{{"position":"bottom"}}}},"expandAndCollapse":false,"animationDuration":550}}]}}
[CHART_END]

   âŒ **é”™è¯¯æ ¼å¼ï¼ˆç»å¯¹ç¦æ­¢ï¼‰**ï¼š
   - "formatter": function(params) {{...}} â† **ç¦æ­¢ä½¿ç”¨JavaScriptå‡½æ•°ï¼**
   - {{"chartType": "bar"}} â† è¿™ä¸æ˜¯ ECharts æ ¼å¼ï¼

è¯·ç›´æ¥è¾“å‡ºåˆ†æå’Œå›¾è¡¨ï¼š"""

                                        # æ„å»ºä¸“å®¶æ•°æ®åˆ†æå¸ˆçš„ç³»ç»Ÿæç¤º
                                        expert_system_prompt = (
                                            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æ•°æ®ä¸­æå–æ´å¯Ÿå¹¶æœ‰æ•ˆåœ°å¯è§†åŒ–å®ƒä»¬ã€‚\n\n"
                                            "**æ ¸å¿ƒåŸåˆ™ - é»˜è®¤ç”Ÿæˆå›¾è¡¨ï¼š**\n"
                                            "âš ï¸ é™¤éæ˜ç¡®è¢«å‘ŠçŸ¥'ç¦æ­¢ç”»å›¾'ï¼Œå¦åˆ™ä½ å¿…é¡»ç”Ÿæˆå›¾è¡¨ï¼æ•°æ®å¯è§†åŒ–å¯¹ç”¨æˆ·ç†è§£æ•°æ®éå¸¸é‡è¦ã€‚\n\n"
                                            "**æ ¸å¿ƒåè®®ï¼š**\n"
                                            "1. **ç§¯æå¯è§†åŒ–**ï¼šåªè¦æ•°æ®æœ‰å¤šè¡Œï¼Œå°±åº”è¯¥ç”Ÿæˆå›¾è¡¨ã€‚å›¾è¡¨æ˜¯æ•°æ®åˆ†æçš„æ ¸å¿ƒäº§å‡ºï¼\n"
                                            "2. **éµå¾ªæŒ‡ä»¤**ï¼šç³»ç»Ÿç»™å‡ºçš„çº¦æŸï¼ˆå¦‚'ç¦æ­¢ç”»å›¾'æˆ–'ä½¿ç”¨æŸ±çŠ¶å›¾'ï¼‰å¿…é¡»ä¸¥æ ¼éµå®ˆã€‚\n"
                                            "3. **æ•°æ®åˆ†æ**ï¼šä¸è¦åªé‡å¤æ•°å­—ã€‚è§£é‡Šæ•°æ®çš„æ„ä¹‰ï¼ˆä¾‹å¦‚ï¼Œä¸è¦è¯´'Aæ˜¯100ï¼ŒBæ˜¯50'ï¼Œè€Œè¦è¯´'Açš„è¡¨ç°æ˜¯Bçš„2å€'ï¼‰ã€‚\n"
                                            "4. **å›¾è¡¨æ ¼å¼**ï¼šå¿…é¡»ä½¿ç”¨æ ‡å‡†çš„ ECharts JSON é…ç½®æ ¼å¼ï¼Œç”¨ [CHART_START] å’Œ [CHART_END] æ ‡è®°åŒ…è£¹ã€‚\n"
                                            "5. **ä¸è§£é‡Šè·³è¿‡å›¾è¡¨çš„åŸå› **ï¼šå½“ä¸éœ€è¦ç”Ÿæˆå›¾è¡¨æ—¶ï¼Œç›´æ¥ä¸ç”Ÿæˆï¼Œä¸è¦è§£é‡Šä¸ºä»€ä¹ˆä¸ç”Ÿæˆå›¾è¡¨ã€‚\n"
                                            "6. **å±‚çº§ç»“æ„ç”¨æ ‘çŠ¶å›¾**ï¼šå½“æ•°æ®åŒ…å«å±‚çº§/åˆ†ç±»ç»“æ„ï¼ˆå¦‚ä¸€çº§åˆ†ç±»â†’äºŒçº§åˆ†ç±»ï¼‰æ—¶ï¼Œä½¿ç”¨ type='tree' çš„æ ‘çŠ¶å›¾å±•ç¤ºã€‚\n\n"
                                            "**å›¾è¡¨ç±»å‹é€‰æ‹©ï¼š**\n"
                                            "- æ—¶é—´åºåˆ—/è¶‹åŠ¿æ•°æ® â†’ æŠ˜çº¿å›¾ (line)\n"
                                            "- æ’å/å¯¹æ¯”æ•°æ® â†’ æŸ±çŠ¶å›¾ (bar)\n"
                                            "- å æ¯”/åˆ†å¸ƒæ•°æ® â†’ é¥¼å›¾ (pie)\n"
                                            "- å±‚çº§/åˆ†ç±»ç»“æ„ â†’ æ ‘çŠ¶å›¾ (tree)\n\n"
                                            "**é‡è¦æé†’ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š**\n"
                                            "- å›¾è¡¨é…ç½®å¿…é¡»æ˜¯**çº¯JSONæ ¼å¼**ï¼ŒåŒ…å« titleã€xAxisã€yAxisã€series ç­‰å­—æ®µ\n"
                                            "- **ç»å¯¹ç¦æ­¢ä½¿ç”¨JavaScriptå‡½æ•°ï¼** ä¾‹å¦‚ç¦æ­¢: \"formatter\": function(params){...}\n"
                                            "- tooltipçš„formatteråªèƒ½ç”¨å­—ç¬¦ä¸²æ¨¡æ¿ï¼Œå¦‚: \"formatter\": \"{b}: {c}å…ƒ\"\n"
                                            "- ä¸è¦ä½¿ç”¨è‡ªå®šä¹‰çš„ç®€åŒ–æ ¼å¼å¦‚ {chartType: 'bar', xAxis: {field: 'name'}}\n"
                                            "- ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹"
                                        )
                                        
                                        # æ„å»ºæ¶ˆæ¯å†å²
                                        analysis_messages = [
                                            LLMMessage(role="system", content=expert_system_prompt),
                                            LLMMessage(role="user", content=original_question),
                                            LLMMessage(role="assistant", content=f"è®©æˆ‘æŸ¥è¯¢ç›¸å…³æ•°æ®ï¼š\n\n```sql\n{current_sql}\n```\n\n{result_text}"),
                                            LLMMessage(role="user", content=analysis_prompt)
                                        ]

                                        # è·å–providerå®ä¾‹
                                        provider_instance = llm_service.get_provider(tenant_id, LLMProvider.DEEPSEEK)
                                        if provider_instance:
                                            try:
                                                # ğŸ”§ ä¿®å¤ï¼šä¸å†å‘é€"æ•°æ®åˆ†æä¸­..."çŠ¶æ€ï¼Œé¿å…ä¸ ProcessingSteps é‡å¤
                                                # analysis_status = {
                                                #     "type": "content",
                                                #     "content": "\n\nğŸ“Š **æ•°æ®åˆ†æä¸­...**\n\n",
                                                #     "provider": chunk.provider,
                                                #     "finished": False,
                                                #     "tenant_id": tenant_id
                                                # }
                                                # yield f"data: {json.dumps(analysis_status, ensure_ascii=False)}\n\n"
                                                
                                                # äºŒæ¬¡è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
                                                analysis_stream = await provider_instance.chat_completion(
                                                    messages=analysis_messages,
                                                    model=None,
                                                    max_tokens=2000,
                                                    temperature=0.7,
                                                    stream=True,
                                                    tools=None
                                                )
                                                
                                                # æµå¼è¾“å‡ºåˆ†æç»“æœ
                                                analysis_content = ""
                                                async for analysis_chunk in analysis_stream:
                                                    if analysis_chunk.type == "content" and analysis_chunk.content:
                                                        analysis_content += analysis_chunk.content
                                                        # ğŸ”§ ä¿®å¤ï¼šä¸å†å‘é€ content äº‹ä»¶ï¼Œé¿å…ä¸ ProcessingSteps æ­¥éª¤8é‡å¤
                                                        # åˆ†æå†…å®¹å°†é€šè¿‡æ­¥éª¤8 (processing_step) å‘é€
                                                        # analysis_data = {
                                                        #     "type": "content",
                                                        #     "content": analysis_chunk.content,
                                                        #     "provider": analysis_chunk.provider,
                                                        #     "finished": False,
                                                        #     "tenant_id": tenant_id
                                                        # }
                                                        # yield f"data: {json.dumps(analysis_data, ensure_ascii=False)}\n\n"
                                                
                                                logger.info(f"äºŒæ¬¡LLMè°ƒç”¨å®Œæˆï¼Œåˆ†æå†…å®¹é•¿åº¦: {len(analysis_content)}")
                                                
                                                # æ£€æµ‹å¹¶æå–å›¾è¡¨é…ç½®
                                                chart_pattern = r'\[CHART_START\](.*?)\[CHART_END\]'
                                                chart_match = re.search(chart_pattern, analysis_content, re.DOTALL)
                                                
                                                if chart_match:
                                                    try:
                                                        chart_json_str = chart_match.group(1).strip()
                                                        logger.info(f"ğŸ“Š æå–åˆ°çš„ECharts JSON (å‰500å­—ç¬¦): {chart_json_str[:500]}")
                                                        
                                                        # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                                                        # 1. ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                                                        if chart_json_str.startswith('```'):
                                                            lines = chart_json_str.split('\n')
                                                            # ç§»é™¤ç¬¬ä¸€è¡Œ(```json)å’Œæœ€åä¸€è¡Œ(```)
                                                            if lines[0].startswith('```'):
                                                                lines = lines[1:]
                                                            if lines and lines[-1].strip() == '```':
                                                                lines = lines[:-1]
                                                            chart_json_str = '\n'.join(lines)
                                                        
                                                        # 2. ç§»é™¤JavaScriptå‡½æ•°ï¼ˆAIæœ‰æ—¶ä»ä¼šç”Ÿæˆï¼‰
                                                        # åŒ¹é… "formatter": function(...){...} ç­‰æ¨¡å¼
                                                        import re as regex_module
                                                        # ç®€å•çš„å‡½æ•°æ›¿æ¢ï¼šå°† function(...){...} æ›¿æ¢ä¸ºå­—ç¬¦ä¸²
                                                        chart_json_str = regex_module.sub(
                                                            r'"formatter":\s*function\s*\([^)]*\)\s*\{[^}]*(?:\{[^}]*\}[^}]*)*\}',
                                                            '"formatter": "{b}: {c}"',
                                                            chart_json_str
                                                        )
                                                        
                                                        # 3. ç¡®ä¿JSONå­—ç¬¦ä¸²æ˜¯å®Œæ•´çš„
                                                        chart_json_str = chart_json_str.strip()
                                                        
                                                        echarts_option = json.loads(chart_json_str)
                                                        logger.info(f"âœ… æˆåŠŸæå– ECharts é…ç½®: {list(echarts_option.keys())}")
                                                        
                                                        # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                                                        chart_event = {
                                                            "type": "chart_config",
                                                            "data": {"echarts_option": echarts_option},
                                                            "provider": "deepseek",
                                                            "finished": False,
                                                            "tenant_id": tenant_id
                                                        }
                                                        yield f"data: {json.dumps(chart_event, ensure_ascii=False)}\n\n"

                                                        # ========== Step 7: ç”Ÿæˆæ•°æ®å¯è§†åŒ– ==========
                                                        # æ¨æ–­å›¾è¡¨ç±»å‹
                                                        chart_type = "å›¾è¡¨"
                                                        series_list = echarts_option.get("series", [])
                                                        if series_list and len(series_list) > 0:
                                                            series_type = series_list[0].get("type", "")
                                                            if series_type:
                                                                chart_type = {
                                                                    "bar": "æŸ±çŠ¶å›¾", "line": "æŠ˜çº¿å›¾", "pie": "é¥¼å›¾",
                                                                    "scatter": "æ•£ç‚¹å›¾", "effectScatter": "æ°”æ³¡å›¾",
                                                                    "tree": "æ ‘å›¾", "treemap": "çŸ©å½¢æ ‘å›¾",
                                                                    "sunburst": "æ—­æ—¥å›¾", "funnel": "æ¼æ–—å›¾",
                                                                    "gauge": "ä»ªè¡¨ç›˜"
                                                                }.get(series_type, series_type)

                                                        chart_content_data = {
                                                            "chart": {
                                                                "echarts_option": echarts_option,
                                                                "chart_type": chart_type
                                                            }
                                                        }

                                                        yield _create_processing_step(
                                                            step=7,
                                                            title="ç”Ÿæˆæ•°æ®å¯è§†åŒ–",
                                                            description=f"åˆ›å»º {chart_type} å±•ç¤ºåˆ†æç»“æœ",
                                                            status="completed",
                                                            duration=int((time.time() - ai_start_time) * 1000 * 0.3),
                                                            tenant_id=tenant_id,
                                                            content_type="chart",
                                                            content_data=chart_content_data
                                                        )
                                                        # ğŸ”§ ä¿®å¤ï¼šæ ‡è®°å›¾è¡¨å·²ç”Ÿæˆï¼Œé¿å…fallbackè·¯å¾„é‡å¤
                                                        chart_already_generated = True

                                                        # ========== Step 8: æ•°æ®åˆ†ææ€»ç»“ ==========
                                                        # ç§»é™¤å›¾è¡¨æ ‡è®°ï¼Œæå–çº¯æ–‡æœ¬åˆ†æ
                                                        clean_analysis = re.sub(chart_pattern, '', analysis_content, flags=re.DOTALL).strip()
                                                        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
                                                        clean_analysis = re.sub(r'\n{3,}', '\n\n', clean_analysis)

                                                        if clean_analysis:
                                                            yield _create_processing_step(
                                                                step=8,
                                                                title="æ•°æ®åˆ†ææ€»ç»“",
                                                                description="AIå¯¹æŸ¥è¯¢ç»“æœçš„åˆ†æå’Œè§£è¯»",
                                                                status="completed",
                                                                duration=int((time.time() - ai_start_time) * 1000 * 0.2),
                                                                tenant_id=tenant_id,
                                                                content_type="text",
                                                                content_data={"text": clean_analysis}
                                                            )
                                                    except json.JSONDecodeError as e:
                                                        logger.warning(f"è§£æ ECharts JSON å¤±è´¥: {e}")
                                                        logger.warning(f"å¤±è´¥çš„JSONå†…å®¹ (å‰300å­—ç¬¦): {chart_json_str[:300] if chart_json_str else 'None'}")

                                                else:
                                                    # ========== Step 8: æ•°æ®åˆ†ææ€»ç»“ï¼ˆæ— å›¾è¡¨æƒ…å†µï¼‰==========
                                                    # æ²¡æœ‰å›¾è¡¨æ—¶ï¼Œç›´æ¥ä½¿ç”¨åˆ†æå†…å®¹ä½œä¸ºæ€»ç»“
                                                    clean_analysis = analysis_content.strip()
                                                    # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
                                                    clean_analysis = re.sub(r'\n{3,}', '\n\n', clean_analysis)

                                                    if clean_analysis:
                                                        yield _create_processing_step(
                                                            step=8,
                                                            title="æ•°æ®åˆ†ææ€»ç»“",
                                                            description="AIå¯¹æŸ¥è¯¢ç»“æœçš„åˆ†æå’Œè§£è¯»",
                                                            status="completed",
                                                            duration=int((time.time() - ai_start_time) * 1000 * 0.2),
                                                            tenant_id=tenant_id,
                                                            content_type="text",
                                                            content_data={"text": clean_analysis}
                                                        )

                                            except Exception as e:
                                                logger.error(f"äºŒæ¬¡LLMè°ƒç”¨å¤±è´¥: {e}")
                                    elif execution_success and result.get('data') and chart_already_generated:
                                        # ğŸ”§ ä¿®å¤ï¼šè·³è¿‡åç»­SQLçš„å›¾è¡¨ç”Ÿæˆï¼Œé¿å…å¤šä¸ªå›¾è¡¨å åŠ 
                                        logger.info("ğŸ”§ è·³è¿‡æ­¤SQLçš„å›¾è¡¨ç”Ÿæˆï¼Œå›¾è¡¨å·²é€šè¿‡ä¹‹å‰çš„SQLç»“æœç”Ÿæˆ")

                                except Exception as e:
                                    last_error = str(e)
                                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                                    # ğŸ”§ æµå¼è¾“å‡ºï¼šSQLæ‰§è¡Œå¤±è´¥é€šçŸ¥
                                    error_preview = str(e)[:100] + ("..." if len(str(e)) > 100 else "")
                                    step6_error = {
                                        "type": "step_update",
                                        "step": 6,
                                        "description": f"âŒ SQLæ‰§è¡Œå¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1})",
                                        "content_preview": f"é”™è¯¯: {error_preview}",
                                        "streaming": True,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(step6_error, ensure_ascii=False)}\n\n"

                                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                                    if retry_count < max_retries:
                                        # ğŸ”§ æµå¼è¾“å‡ºï¼šæ­£åœ¨ä½¿ç”¨AIä¿®å¤SQL
                                        step6_fixing = {
                                            "type": "step_update",
                                            "step": 6,
                                            "description": f"ğŸ”§ æ­£åœ¨ä½¿ç”¨AIä¿®å¤SQL... (ç¬¬ {retry_count + 1} æ¬¡é‡è¯•)",
                                            "content_preview": "åˆ†æé”™è¯¯åŸå› å¹¶ç”Ÿæˆä¿®å¤æ–¹æ¡ˆ",
                                            "streaming": True,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(step6_fixing, ensure_ascii=False)}\n\n"

                                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                                        fixed_sql = await _fix_sql_with_ai(
                                            original_sql=current_sql,
                                            error_message=last_error,
                                            schema_context=schema_context,
                                            original_question=original_question,
                                            db_type=data_source.db_type,  # ä¼ é€’æ•°æ®åº“ç±»å‹
                                            tenant_id=tenant_id  # ä¼ é€’ç§Ÿæˆ·IDç”¨äºllm_service
                                        )

                                        if fixed_sql:
                                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")

                                            # ğŸ”§ æµå¼è¾“å‡ºï¼šAIä¿®å¤æˆåŠŸé€šçŸ¥
                                            step6_fixed = {
                                                "type": "step_update",
                                                "step": 6,
                                                "description": f"âœ… AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•",
                                                "content_preview": fixed_sql[:100] + ("..." if len(fixed_sql) > 100 else ""),
                                                "streaming": True,
                                                "tenant_id": tenant_id
                                            }
                                            yield f"data: {json.dumps(step6_fixed, ensure_ascii=False)}\n\n"

                                            current_sql = fixed_sql
                                            retry_count += 1
                                        else:
                                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")

                                            # ğŸ”§ æµå¼è¾“å‡ºï¼šAIä¿®å¤å¤±è´¥é€šçŸ¥
                                            step6_fix_failed = {
                                                "type": "step_update",
                                                "step": 6,
                                                "description": "âŒ AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•",
                                                "content_preview": "å»ºè®®æ£€æŸ¥SQLè¯­æ³•æˆ–æ•°æ®æºç»“æ„",
                                                "streaming": True,
                                                "tenant_id": tenant_id
                                            }
                                            yield f"data: {json.dumps(step6_fix_failed, ensure_ascii=False)}\n\n"
                                            break
                                    else:
                                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")

                                        # ğŸ”§ æµå¼è¾“å‡ºï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°é€šçŸ¥
                                        step6_max_retries = {
                                            "type": "step_update",
                                            "step": 6,
                                            "description": f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ",
                                            "content_preview": "æ‰€æœ‰å°è¯•å‡å¤±è´¥",
                                            "streaming": True,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(step6_max_retries, ensure_ascii=False)}\n\n"
                                        break

                            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ­¤SQLçš„æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ”¶é›†é”™è¯¯ä¿¡æ¯ï¼ˆä¸ç«‹å³å‘é€ï¼‰
                            if not execution_success and last_error:
                                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                                error_details = _parse_sql_error(last_error)

                                # æ”¶é›†é”™è¯¯ä¿¡æ¯ï¼Œç¨åç»Ÿä¸€å¤„ç†
                                all_sql_results.append({
                                    'success': False,
                                    'sql': current_sql,
                                    'error': last_error,
                                    'error_details': error_details,
                                    'retry_count': retry_count
                                })
                                logger.info(f"ğŸ”§ æ”¶é›†SQLæ‰§è¡Œé”™è¯¯ä¿¡æ¯ï¼ˆæš‚ä¸å‘é€ï¼‰ï¼Œç­‰å¾…å…¶ä»–SQLç»“æœ")

                        # ğŸ”§ ä¿®å¤ï¼šforå¾ªç¯ç»“æŸåï¼Œç»Ÿä¸€å¤„ç†é”™è¯¯ä¿¡æ¯
                        # åªæœ‰å½“æ‰€æœ‰SQLéƒ½å¤±è´¥æ—¶æ‰æ˜¾ç¤ºé”™è¯¯
                        if not any_sql_success and all_sql_results:
                            logger.warning(f"ğŸ”§ æ‰€æœ‰ {len(all_sql_results)} ä¸ªSQLæŸ¥è¯¢éƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯")

                            # æ˜¾ç¤ºæœ€åä¸€ä¸ªå¤±è´¥SQLçš„é”™è¯¯ä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯æœ€ç›¸å…³çš„ï¼‰
                            last_failed = all_sql_results[-1]
                            error_details = last_failed['error_details']
                            retry_count = last_failed['retry_count']
                            current_sql = last_failed['sql']

                            error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                            # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                            if error_details.get('hint'):
                                error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                            # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                            if retry_count > 0:
                                error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                                error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                            # æ·»åŠ å»ºè®®
                            if error_details.get('suggestion'):
                                error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                            else:
                                error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                            error_chunk = {
                                "type": "content",
                                "content": error_text,
                                "provider": chunk.provider,
                                "finished": False,
                                "tenant_id": tenant_id
                            }
                            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                        elif any_sql_success and all_sql_results:
                            # æœ‰SQLæˆåŠŸï¼Œä½†ä¹Ÿæœ‰å¤±è´¥çš„ï¼Œåªè®°å½•æ—¥å¿—ä¸æ˜¾ç¤ºé”™è¯¯
                            failed_count = len([r for r in all_sql_results if not r['success']])
                            if failed_count > 0:
                                logger.info(f"ğŸ”§ æœ‰ {failed_count} ä¸ªSQLå¤±è´¥ä½†è‡³å°‘1ä¸ªæˆåŠŸï¼Œä¸æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯")

                        # ========== ğŸ”§ é‡æ„ï¼šç»Ÿä¸€å›¾è¡¨ç”Ÿæˆé€»è¾‘ï¼ˆå¾ªç¯ç»“æŸåï¼‰ ==========
                        # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„SQLç»“æœï¼Œä¸€æ¬¡æ€§è°ƒç”¨LLMç”Ÿæˆåˆ†æå’Œå›¾è¡¨ï¼ˆæ”¯æŒå¤šå›¾è¡¨ï¼‰
                        # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®é—®é¢˜ç±»å‹å†³å®šæ˜¯å¦ç”Ÿæˆå›¾è¡¨
                        if successful_query_results:
                            logger.info(f"ğŸ”§ å¼€å§‹ç»Ÿä¸€æ•°æ®åˆ†æå’Œå›¾è¡¨ç”Ÿæˆï¼šå…±æœ‰ {len(successful_query_results)} ä¸ªæˆåŠŸçš„SQLç»“æœ, should_generate_chart={should_generate_chart}")

                            # ğŸ”§ å¦‚æœä¸éœ€è¦å›¾è¡¨ï¼ˆå¦‚SCHEMA_QUERYï¼‰ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆï¼Œç›´æ¥å‘é€æ•°æ®
                            if not should_generate_chart:
                                logger.info("ğŸ”§ è·³è¿‡å›¾è¡¨ç”Ÿæˆï¼Œç›´æ¥è¿”å›æŸ¥è¯¢ç»“æœ")
                                # æ„å»º JSON æ•°æ®æ‘˜è¦
                                all_results_summary = []
                                for idx, query_result in enumerate(successful_query_results, 1):
                                    result_data = query_result['result']
                                    data_for_analysis = result_data.get('data', [])[:20]
                                    row_count = query_result['row_count']
                                    columns = query_result['columns']
                                    serializable_data = _convert_decimal_to_float(data_for_analysis)

                                    result_summary = {
                                        'query_index': idx,
                                        'sql': query_result['sql'][:200] + '...' if len(query_result['sql']) > 200 else query_result['sql'],
                                        'columns': columns,
                                        'row_count': row_count,
                                        'data_preview': serializable_data
                                    }
                                    all_results_summary.append(result_summary)

                                # ç›´æ¥è¿”å›æ•°æ®ç»“æœ
                                for idx, query_result in enumerate(successful_query_results, 1):
                                    result_data = query_result['result']
                                    table_content_data = {
                                        "table": {
                                            "columns": query_result['columns'],
                                            "rows": result_data.get('data', [])[:50],  # é™åˆ¶50è¡Œ
                                            "row_count": query_result['row_count']
                                        }
                                    }
                                    # å‘é€è¡¨æ ¼æ•°æ®
                                    table_event = {
                                        "type": "content_delta",
                                        "delta": f"\n\n## æŸ¥è¯¢ç»“æœ {idx}/{len(successful_query_results)}\n\n",
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(table_event, ensure_ascii=False)}\n\n"

                                    # å‘é€è¡¨æ ¼å†…å®¹äº‹ä»¶
                                    table_event = {
                                        "type": "table_data",
                                        "data": table_content_data,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(table_event, ensure_ascii=False)}\n\n"

                                # è·³è¿‡åç»­çš„å›¾è¡¨ç”Ÿæˆé€»è¾‘
                                chart_already_generated = True
                            else:
                                # ğŸ”§ éœ€è¦ç”Ÿæˆå›¾è¡¨çš„æƒ…å†µï¼šæ„å»ºåŒ…å«æ‰€æœ‰æŸ¥è¯¢ç»“æœçš„æ•°æ®æ‘˜è¦
                                all_results_summary = []
                                for idx, query_result in enumerate(successful_query_results, 1):
                                    result_data = query_result['result']
                                    data_for_analysis = result_data.get('data', [])[:20]  # æ¯ä¸ªç»“æœæœ€å¤š20è¡Œ
                                    row_count = query_result['row_count']
                                    columns = query_result['columns']

                                    # å°† Decimal è½¬æ¢ä¸º float
                                    serializable_data = _convert_decimal_to_float(data_for_analysis)

                                    result_summary = {
                                        'query_index': idx,
                                        'sql': query_result['sql'][:200] + '...' if len(query_result['sql']) > 200 else query_result['sql'],
                                        'columns': columns,
                                        'row_count': row_count,
                                        'data_preview': serializable_data
                                    }
                                    all_results_summary.append(result_summary)

                                # æ•°æ®ç‰¹å¾åˆ†æ
                                total_queries = len(successful_query_results)
                                total_rows = sum(r['row_count'] for r in successful_query_results)

                                # åˆ†ææ¯ä¸ªç»“æœé›†çš„ç‰¹å¾
                                analysis_hints = []
                                for idx, query_result in enumerate(successful_query_results, 1):
                                    result_data = query_result['result']
                                    data_preview = result_data.get('data', [])[:5]
                                    columns = query_result['columns']
                                    row_count = query_result['row_count']

                                    col_names_str = " ".join([str(c).lower() for c in columns])
                                    has_time_col = any(k in col_names_str for k in ['date', 'time', 'year', 'month', 'day', 'æ—¥æœŸ', 'æ—¶é—´', 'å¹´', 'æœˆ'])

                                    if row_count <= 1:
                                        analysis_hints.append(f"æŸ¥è¯¢{idx}: èšåˆç»“æœï¼ˆ1è¡Œï¼‰ï¼Œä¸éœ€è¦å›¾è¡¨")
                                    elif has_time_col and row_count > 1:
                                        analysis_hints.append(f"æŸ¥è¯¢{idx}: æ—¶é—´åºåˆ—æ•°æ®ï¼ˆ{row_count}è¡Œï¼‰ï¼Œé€‚åˆæŠ˜çº¿å›¾")
                                    elif row_count > 1:
                                        analysis_hints.append(f"æŸ¥è¯¢{idx}: åˆ†ç±»æ•°æ®ï¼ˆ{row_count}è¡Œï¼‰ï¼Œé€‚åˆæŸ±çŠ¶å›¾æˆ–é¥¼å›¾")

                                analysis_hints_text = "\n".join(analysis_hints)

                                # æ„å»ºå¤šç»“æœåˆ†æprompt
                                multi_result_json = json.dumps(all_results_summary, ensure_ascii=False, indent=2)

                                # ğŸ”§ æ ¹æ®æ˜¯å¦ä¸ºæ‹†åˆ†è¯·æ±‚ï¼Œæ·»åŠ ä¸åŒçš„æŒ‡ä»¤
                                # ğŸ”´ğŸ”´ğŸ”´ å…³é”®ä¿®å¤ï¼šæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å›¾è¡¨æ•°é‡ç”Ÿæˆä¸åŒçš„æŒ‡ä»¤
                                if is_split_request and chart_count:
                                    # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†å›¾è¡¨æ•°é‡
                                    split_instruction = f"""
**ğŸš¨ğŸš¨ğŸš¨ å›¾è¡¨æ‹†åˆ†è¦æ±‚ï¼ˆç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆ {chart_count} ä¸ªç‹¬ç«‹å›¾è¡¨ï¼‰ï¼**

ç”¨æˆ·åˆšåˆšè¯´"æ‹†æˆ{chart_count}ä¸ª"æˆ–ç±»ä¼¼è¡¨è¾¾ï¼Œ**ä½ å¿…é¡»ç”Ÿæˆæ°å¥½ {chart_count} ä¸ªå›¾è¡¨é…ç½®ï¼**

ğŸ”´ **å…³é”®è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
1. è¯†åˆ«SQLç»“æœä¸­æœ‰å“ªäº›å¯åº¦é‡æŒ‡æ ‡ï¼ˆæ•°å€¼åˆ—ï¼‰
2. å¦‚æœæŒ‡æ ‡æ•°é‡ < {chart_count}ï¼Œç”¨ä¸åŒå›¾è¡¨ç±»å‹å±•ç¤ºåŒä¸€æŒ‡æ ‡ï¼š
   - åŒä¸€æŒ‡æ ‡å¯ä»¥ç”Ÿæˆï¼šæŠ˜çº¿å›¾ + æŸ±çŠ¶å›¾ + é¥¼å›¾ï¼ˆå¦‚æœæ˜¯å æ¯”æ•°æ®ï¼‰
   - ä¾‹å¦‚ï¼š2ä¸ªæŒ‡æ ‡è¦ç”Ÿæˆ4ä¸ªå›¾ â†’ æŒ‡æ ‡1æŠ˜çº¿å›¾ + æŒ‡æ ‡1æŸ±çŠ¶å›¾ + æŒ‡æ ‡2æŠ˜çº¿å›¾ + æŒ‡æ ‡2æŸ±çŠ¶å›¾
3. æ¯ä¸ªå›¾è¡¨ä½¿ç”¨ç‹¬ç«‹çš„ [CHART_START]...[CHART_END] æ ‡è®°
4. **å¿…é¡»ç”Ÿæˆæ°å¥½ {chart_count} ä¸ªå›¾è¡¨ï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘ï¼**

ğŸ”´ **ç¤ºä¾‹ï¼šç”Ÿæˆ {chart_count} ä¸ªå›¾è¡¨**
å‡è®¾æœ‰2ä¸ªæŒ‡æ ‡ï¼ˆé”€å”®é¢ã€è®¢å•æ•°ï¼‰ï¼Œç”¨æˆ·è¦æ±‚ {chart_count} ä¸ªå›¾è¡¨ï¼š
ç¬¬1ä¸ªå›¾è¡¨ï¼šé”€å”®é¢æŠ˜çº¿å›¾
[CHART_START]
{{"title":{{"text":"é”€å”®é¢è¶‹åŠ¿"}},"series":[{{"type":"line"}}]}}
[CHART_END]

ç¬¬2ä¸ªå›¾è¡¨ï¼šé”€å”®é¢æŸ±çŠ¶å›¾
[CHART_START]
{{"title":{{"text":"é”€å”®é¢å¯¹æ¯”"}},"series":[{{"type":"bar"}}]}}
[CHART_END]

... ç»§ç»­ç”Ÿæˆç›´åˆ° {chart_count} ä¸ªå›¾è¡¨ ...
"""
                                elif is_split_request:
                                    # ç”¨æˆ·åªè¯´æ‹†åˆ†ï¼Œæ²¡æœ‰æŒ‡å®šæ•°é‡
                                    split_instruction = """
**ğŸš¨ğŸš¨ğŸš¨ å›¾è¡¨æ‹†åˆ†è¦æ±‚ï¼ˆç”¨æˆ·æ˜ç¡®è¯·æ±‚å°†å›¾è¡¨æ‹†åˆ†ï¼‰ï¼**

ç”¨æˆ·è¦æ±‚å°†ç»„åˆå›¾è¡¨æ‹†åˆ†æˆå¤šä¸ªç‹¬ç«‹å›¾è¡¨ã€‚ä½ å¿…é¡»ï¼š
1. è¯†åˆ«æ¯ä¸ªSQLç»“æœä¸­æœ‰å“ªäº›å¯åº¦é‡æŒ‡æ ‡ï¼ˆæ•°å€¼åˆ—ï¼‰
2. ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„å›¾è¡¨é…ç½®
3. æ¯ä¸ªå›¾è¡¨åªåŒ…å«ä¸€ä¸ªæŒ‡æ ‡çš„æ•°æ®
4. ä¾‹å¦‚ï¼šå¦‚æœç»“æœæœ‰"å‘˜å·¥äººæ•°"å’Œ"å¹³å‡è–ªèµ„"ä¸¤åˆ—ï¼Œç”Ÿæˆä¸¤ä¸ªç‹¬ç«‹å›¾è¡¨

ğŸ”´ **æ‹†åˆ†åå›¾è¡¨ç¤ºä¾‹**ï¼š
ç¬¬ä¸€ä¸ªå›¾è¡¨ï¼ˆå‘˜å·¥äººæ•°æŸ±çŠ¶å›¾ï¼‰ï¼š
[CHART_START]
{"title":{"text":"å„éƒ¨é—¨å‘˜å·¥äººæ•°"},"xAxis":{"type":"category","data":["æŠ€æœ¯éƒ¨","é”€å”®éƒ¨"]},"yAxis":{"type":"value","name":"äººæ•°"},"series":[{"type":"bar","data":[10,8]}]}
[CHART_END]

ç¬¬äºŒä¸ªå›¾è¡¨ï¼ˆå¹³å‡è–ªèµ„æŸ±çŠ¶å›¾ï¼‰ï¼š
[CHART_START]
{"title":{"text":"å„éƒ¨é—¨å¹³å‡è–ªèµ„"},"xAxis":{"type":"category","data":["æŠ€æœ¯éƒ¨","é”€å”®éƒ¨"]},"yAxis":{"type":"value","name":"è–ªèµ„(å…ƒ)"},"series":[{"type":"bar","data":[15000,12000]}]}
[CHART_END]
"""
                                else:
                                    split_instruction = ""

                                multi_analysis_prompt = f"""ä½ åˆšåˆšæ‰§è¡Œäº† {total_queries} ä¸ªSQLæŸ¥è¯¢ï¼Œæ‰€æœ‰ç»“æœå¦‚ä¸‹ï¼š

```json
{multi_result_json}
```

--- æ•°æ®ç‰¹å¾åˆ†æ ---
{analysis_hints_text}

{split_instruction}

--- ä»»åŠ¡è¦æ±‚ ---

1. **æ•°æ®åˆ†æ**ï¼šç»¼åˆåˆ†ææ‰€æœ‰æŸ¥è¯¢ç»“æœï¼Œç”¨2-3å¥è¯è§£é‡Šæ•°æ®çš„å•†ä¸šå«ä¹‰

2. **ç”Ÿæˆå›¾è¡¨**ï¼ˆå¦‚æœéœ€è¦å¤šä¸ªå›¾è¡¨ï¼Œè¯·åˆ†åˆ«ç”Ÿæˆï¼‰ï¼š

   âš ï¸ **é‡è¦è§„åˆ™**ï¼š
   - æ¯ä¸ªéœ€è¦å¯è§†åŒ–çš„æ•°æ®é›†ï¼Œä½¿ç”¨ç‹¬ç«‹çš„ [CHART_START]...[CHART_END] æ ‡è®°
   - å¦‚æœæœ‰å¤šä¸ªæ•°æ®é›†éƒ½éœ€è¦å›¾è¡¨ï¼Œå°±ç”Ÿæˆå¤šä¸ªå›¾è¡¨é…ç½®
   - èšåˆç»“æœï¼ˆåªæœ‰1è¡Œï¼‰ä¸éœ€è¦ç”Ÿæˆå›¾è¡¨
   - æ—¶é—´åºåˆ—æ•°æ®ç”¨æŠ˜çº¿å›¾ï¼Œåˆ†ç±»æ¯”è¾ƒç”¨æŸ±çŠ¶å›¾ï¼Œå æ¯”ç”¨é¥¼å›¾

   âœ… **å¤šå›¾è¡¨ç¤ºä¾‹**ï¼ˆ2ä¸ªæ•°æ®é›†å„è‡ªç”Ÿæˆå›¾è¡¨ï¼‰ï¼š

   ç¬¬ä¸€ä¸ªå›¾è¡¨å±•ç¤ºé”€å”®è¶‹åŠ¿ï¼š
[CHART_START]
{{"title":{{"text":"æœˆåº¦é”€å”®è¶‹åŠ¿"}},"xAxis":{{"type":"category","data":["1æœˆ","2æœˆ","3æœˆ"]}},"yAxis":{{"type":"value"}},"series":[{{"type":"line","data":[100,200,150]}}]}}
[CHART_END]

   ç¬¬äºŒä¸ªå›¾è¡¨å±•ç¤ºç±»åˆ«åˆ†å¸ƒï¼š
[CHART_START]
{{"title":{{"text":"å•†å“ç±»åˆ«å æ¯”"}},"series":[{{"type":"pie","data":[{{"name":"ç”µå­äº§å“","value":60}},{{"name":"æœè£…","value":40}}]}}]}}
[CHART_END]

   âŒ **ç¦æ­¢**ï¼š
   - ä¸è¦ä½¿ç”¨JavaScriptå‡½æ•°
   - ä¸è¦ç”¨markdownä»£ç å—åŒ…è£¹JSON
   - ä¸è¦æŠŠå¤šä¸ªå›¾è¡¨åˆå¹¶åˆ°ä¸€ä¸ªé…ç½®é‡Œ

è¯·ç›´æ¥è¾“å‡ºåˆ†æå’Œå›¾è¡¨ï¼š"""

                                # æ„å»ºç³»ç»Ÿæç¤º
                                # å›¾è¡¨æ‹†åˆ†æŒ‡ä»¤ï¼ˆå½“ç”¨æˆ·è¯·æ±‚æ‹†åˆ†æ—¶æ·»åŠ ï¼‰
                                split_instruction_prompt = ""
                                if is_split_request and chart_count:
                                    # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†å›¾è¡¨æ•°é‡
                                    split_instruction_prompt = (
                                        f"**ğŸš¨ğŸš¨ğŸš¨ ç”¨æˆ·è¯·æ±‚å°†å›¾è¡¨æ‹†åˆ†æˆ {chart_count} ä¸ªç‹¬ç«‹å›¾è¡¨ï¼**\n"
                                        f"ä½ å¿…é¡»ç”Ÿæˆæ°å¥½ {chart_count} ä¸ªå›¾è¡¨é…ç½®ï¼\n"
                                        f"å¦‚æœæŒ‡æ ‡æ•°é‡å°‘äº {chart_count}ï¼Œç”¨ä¸åŒå›¾è¡¨ç±»å‹ï¼ˆæŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ã€é¥¼å›¾ï¼‰å±•ç¤ºåŒä¸€æŒ‡æ ‡ã€‚\n"
                                        f"ä½¿ç”¨å¤šä¸ª[CHART_START]...[CHART_END]æ ‡è®°ï¼Œæ¯ä¸ªæ ‡è®°ä¸€ä¸ªå›¾è¡¨ï¼\n\n"
                                    )
                                elif is_split_request:
                                    split_instruction_prompt = (
                                        "**ğŸš¨ğŸš¨ğŸš¨ ç”¨æˆ·è¯·æ±‚å°†å›¾è¡¨æ‹†åˆ†ï¼**\n"
                                        "å¦‚æœSQLç»“æœåŒ…å«å¤šä¸ªæŒ‡æ ‡ï¼ˆå¦‚å‘˜å·¥äººæ•°å’Œå¹³å‡è–ªèµ„ï¼‰ï¼Œä½ å¿…é¡»ï¼š\n"
                                        "1. ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆç‹¬ç«‹çš„å›¾è¡¨é…ç½®\n"
                                        "2. æ¯ä¸ªå›¾è¡¨åªåŒ…å«ä¸€ä¸ªæŒ‡æ ‡çš„æ•°æ®\n"
                                        "3. ä½¿ç”¨å¤šä¸ª[CHART_START]...[CHART_END]æ ‡è®°\n"
                                        "4. ä¸è¦æŠŠå¤šä¸ªæŒ‡æ ‡æ”¾åœ¨åŒä¸€ä¸ªå›¾è¡¨é‡Œï¼\n\n"
                                    )

                                multi_chart_system_prompt = (
                                    "ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå¤šä¸ªSQLæŸ¥è¯¢ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚\n\n"
                                    "**æ ¸å¿ƒåŸåˆ™**ï¼š\n"
                                    "1. æ¯ä¸ªæœ‰æ„ä¹‰çš„æ•°æ®é›†éƒ½åº”è¯¥æœ‰è‡ªå·±çš„å›¾è¡¨\n"
                                    "2. å¤šä¸ªæ•°æ®é›† = å¤šä¸ªç‹¬ç«‹çš„å›¾è¡¨é…ç½®\n"
                                    "3. èšåˆç»“æœï¼ˆ1è¡Œæ•°æ®ï¼‰ä¸ç”Ÿæˆå›¾è¡¨\n"
                                    "4. ä½¿ç”¨æ ‡å‡†ECharts JSONæ ¼å¼ï¼Œç”¨[CHART_START]...[CHART_END]æ ‡è®°\n"
                                    "5. ç¦æ­¢ä½¿ç”¨JavaScriptå‡½æ•°\n\n"
                                    + split_instruction_prompt +
                                    "**å›¾è¡¨ç±»å‹é€‰æ‹©**ï¼š\n"
                                    "- æ—¶é—´åºåˆ— â†’ æŠ˜çº¿å›¾ (line)\n"
                                    "- æ’å/å¯¹æ¯” â†’ æŸ±çŠ¶å›¾ (bar)\n"
                                    "- å æ¯”/åˆ†å¸ƒ â†’ é¥¼å›¾ (pie)"
                                )

                                # æ„å»ºæ¶ˆæ¯
                                multi_analysis_messages = [
                                    LLMMessage(role="system", content=multi_chart_system_prompt),
                                    LLMMessage(role="user", content=original_question),
                                    LLMMessage(role="user", content=multi_analysis_prompt)
                                ]

                                # è·å–providerå®ä¾‹å¹¶è°ƒç”¨
                                provider_instance = llm_service.get_provider(tenant_id, LLMProvider.DEEPSEEK)
                                if provider_instance:
                                    try:
                                        logger.info("ğŸ”§ å¼€å§‹ç»Ÿä¸€LLMè°ƒç”¨ï¼šåˆ†ææ•°æ®å¹¶ç”Ÿæˆå¤šå›¾è¡¨")

                                        # ğŸ”§ æµå¼è¾“å‡ºï¼šå‘é€ Step 7/8 çš„ running çŠ¶æ€
                                        yield _create_processing_step(
                                            step=7,
                                            title="ç”Ÿæˆæ•°æ®å¯è§†åŒ–",
                                            description="æ­£åœ¨åˆ†ææ•°æ®ç»“æ„...",
                                            status="running",
                                            tenant_id=tenant_id
                                        )
                                        await asyncio.sleep(0.05)

                                        yield _create_processing_step(
                                            step=8,
                                            title="æ•°æ®åˆ†ææ€»ç»“",
                                            description="æ­£åœ¨åˆ†ææŸ¥è¯¢ç»“æœ...",
                                            status="running",
                                            tenant_id=tenant_id
                                        )
                                        await asyncio.sleep(0.05)

                                        analysis_stream = await provider_instance.chat_completion(
                                            messages=multi_analysis_messages,
                                            model=None,
                                            max_tokens=3000,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒå¤šå›¾è¡¨
                                            temperature=0.7,
                                            stream=True,
                                            tools=None
                                        )

                                        # ğŸ”§ æµå¼è¾“å‡ºï¼šæ”¶é›†åˆ†æå†…å®¹å¹¶å®æ—¶å‘é€step_updateäº‹ä»¶
                                        analysis_content = ""
                                        last_step7_update = time.time()
                                        last_step8_update = time.time()
                                        step7_phase_idx = 0
                                        step7_phases = ["æ­£åœ¨åˆ†ææ•°æ®ç»“æ„...", "é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹...", "æ­£åœ¨ç”Ÿæˆå›¾è¡¨é…ç½®..."]
                                        chart_detected = False

                                        async for analysis_chunk in analysis_stream:
                                            if analysis_chunk.type == "content" and analysis_chunk.content:
                                                analysis_content += analysis_chunk.content
                                                current_time = time.time()

                                                # Step 7: å¤šé˜¶æ®µçŠ¶æ€æ›´æ–°ï¼ˆæ¯800msåˆ‡æ¢é˜¶æ®µï¼‰
                                                if "[CHART_START]" in analysis_content and not chart_detected:
                                                    chart_detected = True
                                                    step_update_event = {
                                                        "type": "step_update",
                                                        "step": 7,
                                                        "description": "æ­£åœ¨ç”Ÿæˆå›¾è¡¨é…ç½®...",
                                                        "tenant_id": tenant_id
                                                    }
                                                    yield f"data: {json.dumps(step_update_event, ensure_ascii=False)}\n\n"
                                                elif not chart_detected and current_time - last_step7_update >= 0.8:
                                                    if step7_phase_idx < 2:
                                                        step7_phase_idx += 1
                                                        step_update_event = {
                                                            "type": "step_update",
                                                            "step": 7,
                                                            "description": step7_phases[step7_phase_idx],
                                                            "tenant_id": tenant_id
                                                        }
                                                        yield f"data: {json.dumps(step_update_event, ensure_ascii=False)}\n\n"
                                                    last_step7_update = current_time

                                                # Step 8: æµå¼æ‰“å­—æœºæ•ˆæœï¼ˆæ¯100msæ›´æ–°é¢„è§ˆï¼‰
                                                if current_time - last_step8_update >= 0.1:
                                                    # æå–éå›¾è¡¨éƒ¨åˆ†ä½œä¸ºåˆ†æé¢„è§ˆ
                                                    clean_preview = re.sub(r'\[CHART_START\].*?\[CHART_END\]', '', analysis_content, flags=re.DOTALL)
                                                    clean_preview = re.sub(r'\n{3,}', '\n\n', clean_preview).strip()

                                                    if clean_preview:
                                                        step8_update_event = {
                                                            "type": "step_update",
                                                            "step": 8,
                                                            "description": f"æ­£åœ¨åˆ†æ... ({len(clean_preview)} å­—ç¬¦)",
                                                            "content_preview": clean_preview,
                                                            "streaming": True,
                                                            "tenant_id": tenant_id
                                                        }
                                                        yield f"data: {json.dumps(step8_update_event, ensure_ascii=False)}\n\n"
                                                    last_step8_update = current_time

                                        logger.info(f"ğŸ”§ ç»Ÿä¸€LLMè°ƒç”¨å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(analysis_content)}")

                                        # æå–æ‰€æœ‰å›¾è¡¨é…ç½®ï¼ˆæ”¯æŒå¤šä¸ªï¼‰
                                        chart_pattern = r'\[CHART_START\](.*?)\[CHART_END\]'
                                        chart_matches = re.findall(chart_pattern, analysis_content, re.DOTALL)

                                        logger.info(f"ğŸ”§ æå–åˆ° {len(chart_matches)} ä¸ªå›¾è¡¨é…ç½®")

                                        # ä¸ºæ¯ä¸ªå›¾è¡¨ç”Ÿæˆstep=7äº‹ä»¶
                                        for chart_idx, chart_json_str in enumerate(chart_matches, 1):
                                            try:
                                                chart_json_str = chart_json_str.strip()

                                                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—
                                                if chart_json_str.startswith('```'):
                                                    lines = chart_json_str.split('\n')
                                                    if lines[0].startswith('```'):
                                                        lines = lines[1:]
                                                    if lines and lines[-1].strip() == '```':
                                                        lines = lines[:-1]
                                                    chart_json_str = '\n'.join(lines)

                                                # ç§»é™¤JavaScriptå‡½æ•°
                                                chart_json_str = re.sub(
                                                    r'"formatter":\s*function\s*\([^)]*\)\s*\{[^}]*(?:\{[^}]*\}[^}]*)*\}',
                                                    '"formatter": "{b}: {c}"',
                                                    chart_json_str
                                                )

                                                # å°è¯•è§£æä¸º ECharts é…ç½®
                                                parsed_data = json.loads(chart_json_str.strip())

                                                # ğŸ”§ æ£€æµ‹æ˜¯å¦ä¸ºç®€åŒ–æ ¼å¼ï¼ˆåŒ…å« x_data å’Œ y_dataï¼‰
                                                if "x_data" in parsed_data and "y_data" in parsed_data:
                                                    # è½¬æ¢ç®€åŒ–æ ¼å¼ä¸ºå®Œæ•´ ECharts é…ç½®
                                                    from src.app.services.agent.data_transformer import convert_simple_chart_to_echarts
                                                    echarts_option = convert_simple_chart_to_echarts(parsed_data)
                                                    if echarts_option:
                                                        logger.info(f"âœ… æˆåŠŸè½¬æ¢ç®€åŒ–æ ¼å¼å›¾è¡¨{chart_idx}")
                                                    else:
                                                        logger.warning(f"âš ï¸ ç®€åŒ–æ ¼å¼è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡å›¾è¡¨{chart_idx}")
                                                        continue
                                                else:
                                                    # å·²ç»æ˜¯å®Œæ•´çš„ ECharts é…ç½®
                                                    echarts_option = parsed_data
                                                    logger.info(f"âœ… æˆåŠŸè§£æå›¾è¡¨{chart_idx}: {list(echarts_option.keys())}")

                                                # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                                                chart_event = {
                                                    "type": "chart_config",
                                                    "data": {"echarts_option": echarts_option, "chart_index": chart_idx},
                                                    "provider": "deepseek",
                                                    "finished": False,
                                                    "tenant_id": tenant_id
                                                }
                                                yield f"data: {json.dumps(chart_event, ensure_ascii=False)}\n\n"

                                                # æ¨æ–­å›¾è¡¨ç±»å‹
                                                chart_type = "å›¾è¡¨"
                                                series_list = echarts_option.get("series", [])
                                                if series_list and len(series_list) > 0:
                                                    series_type = series_list[0].get("type", "")
                                                    if series_type:
                                                        chart_type = {
                                                            "bar": "æŸ±çŠ¶å›¾", "line": "æŠ˜çº¿å›¾", "pie": "é¥¼å›¾",
                                                            "scatter": "æ•£ç‚¹å›¾", "tree": "æ ‘å›¾"
                                                        }.get(series_type, series_type)

                                                # è·å–å›¾è¡¨æ ‡é¢˜
                                                chart_title = echarts_option.get("title", {}).get("text", f"å›¾è¡¨{chart_idx}")

                                                chart_content_data = {
                                                    "chart": {
                                                        "echarts_option": echarts_option,
                                                        "chart_type": chart_type,
                                                        "chart_index": chart_idx
                                                    }
                                                }

                                                yield _create_processing_step(
                                                    step=7,
                                                    title=f"ç”Ÿæˆæ•°æ®å¯è§†åŒ– ({chart_idx}/{len(chart_matches)})",
                                                    description=f"{chart_title} - {chart_type}",
                                                    status="completed",
                                                    duration=int((time.time() - ai_start_time) * 1000 * 0.3 / len(chart_matches)),
                                                    tenant_id=tenant_id,
                                                    content_type="chart",
                                                    content_data=chart_content_data
                                                )

                                                chart_already_generated = True

                                            except json.JSONDecodeError as e:
                                                logger.warning(f"è§£æå›¾è¡¨{chart_idx} JSONå¤±è´¥: {e}")
                                                logger.warning(f"å¤±è´¥çš„JSON (å‰200å­—ç¬¦): {chart_json_str[:200]}")

                                        # ç”Ÿæˆæ•°æ®åˆ†ææ€»ç»“ï¼ˆstep=8ï¼‰
                                        clean_analysis = re.sub(chart_pattern, '', analysis_content, flags=re.DOTALL).strip()
                                        clean_analysis = re.sub(r'\n{3,}', '\n\n', clean_analysis)

                                        if clean_analysis:
                                            yield _create_processing_step(
                                                step=8,
                                                title="æ•°æ®åˆ†ææ€»ç»“",
                                                description="AIå¯¹æŸ¥è¯¢ç»“æœçš„åˆ†æå’Œè§£è¯»",
                                                status="completed",
                                                duration=int((time.time() - ai_start_time) * 1000 * 0.2),
                                                tenant_id=tenant_id,
                                                content_type="text",
                                                content_data={"text": clean_analysis}
                                            )

                                    except Exception as e:
                                        logger.error(f"ğŸ”§ ç»Ÿä¸€LLMè°ƒç”¨å¤±è´¥: {e}")
                                else:
                                    logger.warning("ğŸ”§ æ— æ³•è·å–LLM providerå®ä¾‹")

                    else:
                        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
                        warning_text = "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚\n"

                        warning_chunk = {
                            "type": "content",
                            "content": warning_text,
                            "provider": chunk.provider,
                            "finished": False,
                            "tenant_id": tenant_id
                        }
                        yield f"data: {json.dumps(warning_chunk, ensure_ascii=False)}\n\n"

        # ğŸ”§ æ¢å¤å›¾è¡¨ç”ŸæˆåŠŸèƒ½ï¼šæ£€æµ‹å¹¶æå– [CHART_START]...[CHART_END] æ ‡è®°ä¸­çš„ ECharts é…ç½®
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå›¾è¡¨ï¼Œä»¥åŠæ˜¯å¦å·²é€šè¿‡äºŒæ¬¡LLMè°ƒç”¨ç”Ÿæˆå›¾è¡¨
        chart_pattern = r'\[CHART_START\](.*?)\[CHART_END\]'
        chart_match = re.search(chart_pattern, full_content, re.DOTALL)

        # ğŸ”§ åªæœ‰å½“é—®é¢˜ç±»å‹éœ€è¦å›¾è¡¨æ—¶æ‰ä»full_contentä¸­æå–å›¾è¡¨ï¼ˆfallbackè·¯å¾„ï¼‰
        if chart_match and should_generate_chart and not chart_already_generated:
            try:
                chart_json_str = chart_match.group(1).strip()
                # è§£æ JSON
                parsed_data = json.loads(chart_json_str)

                # ğŸ”§ æ£€æµ‹æ˜¯å¦ä¸ºç®€åŒ–æ ¼å¼ï¼ˆåŒ…å« x_data å’Œ y_dataï¼‰
                if "x_data" in parsed_data and "y_data" in parsed_data:
                    # è½¬æ¢ç®€åŒ–æ ¼å¼ä¸ºå®Œæ•´ ECharts é…ç½®
                    from src.app.services.agent.data_transformer import convert_simple_chart_to_echarts
                    echarts_option = convert_simple_chart_to_echarts(parsed_data)
                    if not echarts_option:
                        logger.warning("âš ï¸ ç®€åŒ–æ ¼å¼è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡å›¾è¡¨æ˜¾ç¤º")
                        raise json.JSONDecodeError("è½¬æ¢å¤±è´¥", chart_json_str, 0)
                else:
                    echarts_option = parsed_data

                logger.info(f"âœ… æˆåŠŸæå– ECharts é…ç½®: {list(echarts_option.keys())}")

                # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                chart_chunk = {
                    "type": "chart_config",
                    "data": {
                        "echarts_option": echarts_option
                    },
                    "provider": "deepseek",
                    "finished": False,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(chart_chunk, ensure_ascii=False)}\n\n"

                # ========== Step 7: ç”Ÿæˆæ•°æ®å¯è§†åŒ–ï¼ˆfallbackè·¯å¾„ï¼‰ ==========
                # æ¨æ–­å›¾è¡¨ç±»å‹
                chart_type = "å›¾è¡¨"
                series_list = echarts_option.get("series", [])
                if series_list and len(series_list) > 0:
                    series_type = series_list[0].get("type", "")
                    if series_type:
                        chart_type = {
                            "bar": "æŸ±çŠ¶å›¾", "line": "æŠ˜çº¿å›¾", "pie": "é¥¼å›¾",
                            "scatter": "æ•£ç‚¹å›¾", "effectScatter": "æ°”æ³¡å›¾",
                            "tree": "æ ‘å›¾", "treemap": "çŸ©å½¢æ ‘å›¾",
                            "sunburst": "æ—­æ—¥å›¾", "funnel": "æ¼æ–—å›¾",
                            "gauge": "ä»ªè¡¨ç›˜"
                        }.get(series_type, series_type)

                chart_content_data = {
                    "chart": {
                        "echarts_option": echarts_option,
                        "chart_type": chart_type
                    }
                }

                yield _create_processing_step(
                    step=7,
                    title="ç”Ÿæˆæ•°æ®å¯è§†åŒ–",
                    description=f"åˆ›å»º {chart_type} å±•ç¤ºåˆ†æç»“æœ",
                    status="completed",
                    duration=200,  # ä¼°ç®—è€—æ—¶
                    tenant_id=tenant_id,
                    content_type="chart",
                    content_data=chart_content_data
                )

                # å¯é€‰ï¼šä»æœ€ç»ˆå†…å®¹ä¸­ç§»é™¤å›¾è¡¨æ ‡è®°ï¼ˆå‰ç«¯å¯èƒ½å·²ç»æ˜¾ç¤ºäº†ï¼‰
                # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ ‡è®°ï¼Œè®©å‰ç«¯è‡ªå·±å†³å®šæ˜¯å¦æ˜¾ç¤º

            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ è§£æ ECharts JSON å¤±è´¥: {e}")
                logger.warning(f"åŸå§‹å†…å®¹: {chart_json_str[:200]}...")
            except Exception as e:
                logger.error(f"âŒ æå– ECharts é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        elif chart_match and chart_already_generated:
            # ğŸ”§ ä¿®å¤ï¼šè·³è¿‡fallbackè·¯å¾„ï¼Œå› ä¸ºå›¾è¡¨å·²é€šè¿‡äºŒæ¬¡LLMè°ƒç”¨ç”Ÿæˆ
            logger.info("ğŸ”§ è·³è¿‡fallbackå›¾è¡¨ç”Ÿæˆè·¯å¾„ï¼Œå›¾è¡¨å·²é€šè¿‡äºŒæ¬¡LLMè°ƒç”¨ç”Ÿæˆ")
        elif should_generate_chart and not chart_already_generated:
            # ğŸ”§ æ–°å¢ï¼šå°è¯•ä» markdown ä»£ç å—ä¸­æå–ç®€åŒ–æ ¼å¼çš„å›¾è¡¨
            # AI å¯èƒ½æ²¡æœ‰ä½¿ç”¨ [CHART_START]...[CHART_END] æ ‡è®°
            logger.info("ğŸ”§ æœªæ‰¾åˆ° [CHART_START] æ ‡è®°ï¼Œå°è¯•ä» markdown ä»£ç å—æå–ç®€åŒ–æ ¼å¼å›¾è¡¨...")
            from src.app.services.agent.data_transformer import extract_simple_charts_from_text
            simple_charts = extract_simple_charts_from_text(full_content)

            if simple_charts:
                logger.info(f"âœ… ä» markdown ä»£ç å—æå–åˆ° {len(simple_charts)} ä¸ªç®€åŒ–æ ¼å¼å›¾è¡¨")

                for chart_idx, echarts_option in enumerate(simple_charts, 1):
                    try:
                        # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                        chart_chunk = {
                            "type": "chart_config",
                            "data": {
                                "echarts_option": echarts_option,
                                "chart_index": chart_idx
                            },
                            "provider": "deepseek",
                            "finished": False,
                            "tenant_id": tenant_id
                        }
                        yield f"data: {json.dumps(chart_chunk, ensure_ascii=False)}\n\n"

                        # æ¨æ–­å›¾è¡¨ç±»å‹
                        chart_type = "å›¾è¡¨"
                        series_list = echarts_option.get("series", [])
                        if series_list and len(series_list) > 0:
                            series_type = series_list[0].get("type", "")
                            if series_type:
                                chart_type = {
                                    "bar": "æŸ±çŠ¶å›¾", "line": "æŠ˜çº¿å›¾", "pie": "é¥¼å›¾",
                                    "scatter": "æ•£ç‚¹å›¾", "effectScatter": "æ°”æ³¡å›¾",
                                    "tree": "æ ‘å›¾", "treemap": "çŸ©å½¢æ ‘å›¾",
                                    "sunburst": "æ—­æ—¥å›¾", "funnel": "æ¼æ–—å›¾",
                                    "gauge": "ä»ªè¡¨ç›˜"
                                }.get(series_type, series_type)

                        # è·å–å›¾è¡¨æ ‡é¢˜
                        chart_title = echarts_option.get("title", {}).get("text", f"å›¾è¡¨{chart_idx}")

                        # åˆ›å»º processing step
                        chart_content_data = {
                            "chart": {
                                "echarts_option": echarts_option,
                                "chart_type": chart_type,
                                "chart_index": chart_idx
                            }
                        }

                        yield _create_processing_step(
                            step=7,
                            title=f"ç”Ÿæˆæ•°æ®å¯è§†åŒ– ({chart_idx}/{len(simple_charts)})",
                            description=f"{chart_title} - {chart_type}",
                            status="completed",
                            duration=200,
                            tenant_id=tenant_id,
                            content_type="chart",
                            content_data=chart_content_data
                        )

                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†ç®€åŒ–æ ¼å¼å›¾è¡¨{chart_idx}å¤±è´¥: {e}")
            else:
                logger.info("ğŸ”§ æœªä» markdown ä»£ç å—ä¸­æå–åˆ°ç®€åŒ–æ ¼å¼å›¾è¡¨")

        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå™¨é”™è¯¯: {e}")
        error_data = {
            "type": "error",
            "content": f"Stream error: {str(e)}",
            "provider": "unknown",
            "finished": True,
            "tenant_id": tenant_id
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


@router.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completion(
    request: ChatCompletionRequest,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    èŠå¤©å®Œæˆæ¥å£
    æ”¯æŒå¤šæä¾›å•†ã€å¤šæ¨¡æ€å’Œæµå¼è¾“å‡º
    è‡ªåŠ¨è·å–ç”¨æˆ·æ•°æ®æºä¿¡æ¯å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    """
    try:
        # è·å–tenant_idï¼Œæ”¯æŒå¼€å‘ç¯å¢ƒä¸‹çš„é»˜è®¤ç§Ÿæˆ·
        tenant_id = getattr(current_user, 'tenant_id', None) or current_user.get('tenant_id', 'default_tenant')
        logger.info(f"Chat completion request for tenant: {tenant_id}, stream={request.stream}, data_source_ids={request.data_source_ids}")
        print(f"[DEBUG] Chat completion request - stream={request.stream}, data_source_ids={request.data_source_ids}")
        # å¼ºåˆ¶è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        import sys
        print(f"[DEBUG] Chat completion request - stream={request.stream}, data_source_ids={request.data_source_ids}", file=sys.stderr)

        # è½¬æ¢æä¾›å•†
        provider = None
        if request.provider:
            try:
                provider = LLMProvider(request.provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {request.provider}"
                )

        # è·å–æ•°æ®æºä¸Šä¸‹æ–‡ï¼Œå¦‚æœæŒ‡å®šäº†æ•°æ®æºIDåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        print(f"[DEBUG] Starting _get_data_sources_context for tenant: {tenant_id}, data_source_ids: {request.data_source_ids}")
        import time as _time
        _ctx_start = _time.time()
        data_sources_context = await _get_data_sources_context(tenant_id, db, request.data_source_ids)
        schema_duration_ms = int((_time.time() - _ctx_start) * 1000)
        print(f"[DEBUG] _get_data_sources_context took {_time.time() - _ctx_start:.2f}s")
        
        # æ”¶é›†Schemaä¿¡æ¯ç”¨äºå‰ç«¯å±•ç¤º
        schema_info = None
        if data_sources_context:
            logger.info(f"Data sources context retrieved for tenant {tenant_id}, length: {len(data_sources_context)}")
            # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æ•°æ®æºä¸Šä¸‹æ–‡çš„å‰1000ä¸ªå­—ç¬¦
            logger.debug(f"Data sources context content (first 1000 chars): {data_sources_context[:1000]}")
            
            # è§£æSchemaä¿¡æ¯
            tables = []
            data_source_name = "æœªçŸ¥"
            # ç®€å•æå–è¡¨åï¼ˆä»Schemaæ–‡æœ¬ä¸­è§£æï¼‰
            import re as _re
            table_matches = _re.findall(r'è¡¨:\s*(\w+)', data_sources_context)
            if table_matches:
                tables = table_matches
            # æå–æ•°æ®æºå
            ds_name_match = _re.search(r'æ•°æ®æº:\s*(\S+)', data_sources_context)
            if ds_name_match:
                data_source_name = ds_name_match.group(1)
            
            schema_info = {
                "duration_ms": schema_duration_ms,
                "length": len(data_sources_context),
                "tables": tables,
                "data_source_name": data_source_name
            }
        else:
            logger.info(f"No data sources found for tenant {tenant_id}")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = _convert_chat_messages(request.messages)

        # ============================================================
        # ğŸ”§ [ä¿®å¤] è·å–æ•°æ®æºç±»å‹ï¼Œç”¨äºç”Ÿæˆæ•°æ®åº“ç‰¹å®šçš„æç¤ºè¯
        # ============================================================
        db_type = "postgresql"  # é»˜è®¤å€¼
        if request.data_source_ids and len(request.data_source_ids) == 1:
            # å•ä¸ªæ•°æ®æºï¼Œè·å–å…¶ db_type
            try:
                data_sources = await data_source_service.get_data_sources(
                    tenant_id=tenant_id,
                    db=db,
                    active_only=True
                )
                # ç­›é€‰å‡ºæŒ‡å®šçš„æ•°æ®æº
                matching_sources = [ds for ds in data_sources if ds.id in request.data_source_ids]
                if matching_sources:
                    db_type = matching_sources[0].db_type
                    logger.info(f"ğŸ” [LLMç«¯ç‚¹] æ£€æµ‹åˆ°å•ä¸ªæ•°æ®æºï¼Œdb_type={db_type}")
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æ•°æ®æºç±»å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ db_type=postgresql")
        elif request.data_source_ids and len(request.data_source_ids) > 1:
            # å¤šä¸ªæ•°æ®æºï¼Œæ£€æŸ¥å®ƒä»¬æ˜¯å¦æ˜¯åŒä¸€ç±»å‹
            try:
                data_sources = await data_source_service.get_data_sources(
                    tenant_id=tenant_id,
                    db=db,
                    active_only=True
                )
                matching_sources = [ds for ds in data_sources if ds.id in request.data_source_ids]
                if matching_sources:
                    db_types = set(ds.db_type for ds in matching_sources)
                    if len(db_types) == 1:
                        db_type = db_types.pop()
                        logger.info(f"ğŸ” [LLMç«¯ç‚¹] å¤šä¸ªæ•°æ®æºä½†ç±»å‹ä¸€è‡´ï¼Œdb_type={db_type}")
                    else:
                        logger.info(f"ğŸ” [LLMç«¯ç‚¹] å¤šä¸ªæ•°æ®æºç±»å‹ä¸åŒ: {db_types}ï¼Œä½¿ç”¨é»˜è®¤ postgresql")
                        db_type = "postgresql"  # å¤šç§ç±»å‹æ—¶ä½¿ç”¨é»˜è®¤
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æ•°æ®æºç±»å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ db_type=postgresql")
        # ============================================================

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ åŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„systemæ¶ˆæ¯
        has_system_message = any(msg.role == "system" for msg in messages)
        if not has_system_message:
            system_prompt = _build_system_prompt_with_context(data_sources_context, db_type)
            system_message = LLMMessage(role="system", content=system_prompt)
            messages.insert(0, system_message)
            logger.info("Added system message with data sources context")
        elif data_sources_context:
            # å¦‚æœå·²æœ‰systemæ¶ˆæ¯ï¼Œæ›¿æ¢ä¸ºå®Œæ•´çš„æ•°æ®åˆ†æç³»ç»Ÿæç¤ºï¼ˆåŒ…å«SQLç”ŸæˆæŒ‡ä»¤ï¼‰
            # è¿™æ ·ç¡®ä¿AIçŸ¥é“å¦‚ä½•æ­£ç¡®ç”ŸæˆSQLæŸ¥è¯¢
            full_system_prompt = _build_system_prompt_with_context(data_sources_context, db_type)
            for i, msg in enumerate(messages):
                if msg.role == "system":
                    messages[i] = LLMMessage(role="system", content=full_system_prompt, thinking=msg.thinking)
                    logger.info("Replaced existing system message with full data sources context and SQL instructions")
                    break

        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºåŸå§‹é—®é¢˜
        original_question = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                original_question = msg.content
                break

        # ============================================================
        # SQLé”™è¯¯è®°å¿†æ³¨å…¥ - ä»å†å²é”™è¯¯ä¸­å­¦ä¹ ï¼Œé¿å…é‡å¤é”™è¯¯
        # ============================================================
        try:
            error_memory_service = SQLErrorMemoryService(db)
            # å°è¯•ä»é—®é¢˜æˆ–ä¸Šä¸‹æ–‡ä¸­æå–è¡¨å
            table_name = None
            if data_sources_context:
                # ä»ç¬¬ä¸€ä¸ªæ•°æ®æºä¸­æå–å¯èƒ½çš„è¡¨å
                import re
                for ds in data_sources_context:
                    if ds.get("schema_info") and ds["schema_info"].get("tables"):
                        # è·å–ç¬¬ä¸€ä¸ªè¡¨åä½œä¸ºç›¸å…³è¡¨
                        tables = list(ds["schema_info"]["tables"].keys())
                        if tables:
                            table_name = tables[0].lower()
                            break

            # è·å–å†å²é”™è¯¯æç¤º
            few_shot_prompt = await error_memory_service.generate_few_shot_prompt(
                tenant_id=tenant_id,
                user_question=original_question,
                table_name=table_name,
                limit=3  # æœ€å¤š3ä¸ªå†å²é”™è¯¯ç¤ºä¾‹
            )

            if few_shot_prompt:
                # å°†å†å²é”™è¯¯æ³¨å…¥åˆ°systemæ¶ˆæ¯ä¸­
                for i, msg in enumerate(messages):
                    if msg.role == "system":
                        # åœ¨åŸæœ‰æç¤ºåè¿½åŠ å†å²é”™è¯¯ç¤ºä¾‹
                        enhanced_prompt = msg.content + "\n\n" + few_shot_prompt
                        messages[i] = LLMMessage(role="system", content=enhanced_prompt, thinking=msg.thinking)
                        logger.info(f"âœ… [SQLé”™è¯¯è®°å¿†] å·²æ³¨å…¥{few_shot_prompt.count('é”™è¯¯')}ä¸ªå†å²é”™è¯¯ç¤ºä¾‹åˆ°Prompt")
                        break
        except Exception as error_inject_error:
            logger.warning(f"âš ï¸ [SQLé”™è¯¯è®°å¿†] æ³¨å…¥å†å²é”™è¯¯å¤±è´¥: {error_inject_error}")
        # ============================================================

        # è°ƒç”¨LLMæœåŠ¡
        if request.stream:
            # æµå¼å“åº”
            # æ³¨æ„ï¼šchat_completion æ˜¯å¼‚æ­¥å‡½æ•°ï¼Œéœ€è¦ await æ¥è·å– AsyncGenerator
            logger.info(f"[STREAM] Starting stream request for tenant {tenant_id}")
            print(f"[STREAM] Starting stream request for tenant {tenant_id}", file=sys.stderr)
            
            # æ–¹æ¡ˆ B: ä¸ä½¿ç”¨ Function Callingï¼Œæ”¹ç”¨ SQL ä»£ç å—æ£€æµ‹
            # DeepSeek ä¸æ”¯æŒæ ‡å‡†çš„ OpenAI Function Calling åè®®
            # æ‰€ä»¥æˆ‘ä»¬è®© AI ç›´æ¥åœ¨å›ç­”ä¸­è¾“å‡º ```sql ... ``` ä»£ç å—ï¼Œç„¶åè‡ªåŠ¨æ£€æµ‹å¹¶æ‰§è¡Œ
            logger.info(f"[STREAM] ä½¿ç”¨ SQL ä»£ç å—æ£€æµ‹æ¨¡å¼ï¼ˆæ–¹æ¡ˆBï¼‰")
            
            response_generator = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking,
                tools=None  # ä¸ä¼ é€’å·¥å…·å®šä¹‰ï¼Œä½¿ç”¨ SQL ä»£ç å—æ£€æµ‹
            )
            
            logger.info(f"[STREAM] Stream generator created, starting response")

            # ========== ğŸ”§ ä¿®å¤ï¼šå…ˆåˆ†ç±»é—®é¢˜ï¼Œå†å†³å®šä½¿ç”¨å“ªä¸ªç”Ÿæˆå™¨ ==========
            # å¯¼å…¥é—®é¢˜åˆ†ç±»å™¨
            from src.app.services.processing_steps import classify_question, QuestionType

            # åˆ¤æ–­æ˜¯å¦ä¸ºAgentæ¨¡å¼ï¼ˆæœ‰æ•°æ®æºï¼‰
            is_agent_mode = request.data_source_ids and len(request.data_source_ids) > 0

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå…ˆåˆ†ç±»é—®é¢˜ï¼Œå†å†³å®šä½¿ç”¨å“ªä¸ªç”Ÿæˆå™¨
            # å³ä½¿æœ‰æ•°æ®æºï¼Œå¦‚æœæ˜¯ç®€å•å¯¹è¯ä¹Ÿåº”ä½¿ç”¨åŠ¨æ€æ­¥éª¤æµç¨‹
            question_type = classify_question(original_question, has_data_source=is_agent_mode)
            logger.info(f"[STREAM] Question classified as: {question_type.value}, is_agent_mode={is_agent_mode}")

            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨SQLæµç¨‹ï¼ˆåªæœ‰çœŸæ­£éœ€è¦æŸ¥è¯¢æ•°æ®æ—¶æ‰ä½¿ç”¨ï¼‰
            # ğŸ”§ ä¿®å¤ï¼šSCHEMA_QUERYä¸éœ€è¦SQLæµç¨‹ï¼ŒAgentç›´æ¥å›ç­”schemaä¿¡æ¯å³å¯
            needs_sql_flow = question_type in [
                QuestionType.DATA_QUERY,
                QuestionType.VISUALIZATION
                # ğŸ”§ SCHEMA_QUERYå·²ç§»é™¤ - schemaæŸ¥è¯¢ä¸éœ€è¦SQLï¼Œç›´æ¥è®©Agentå›ç­”
            ] and is_agent_mode

            if needs_sql_flow:
                # Agent SQLæŸ¥è¯¢æ¨¡å¼ï¼š6-8æ­¥æµç¨‹ï¼ˆä»…åœ¨çœŸæ­£éœ€è¦æ•°æ®æŸ¥è¯¢æ—¶ä½¿ç”¨ï¼‰
                logger.info(f"[STREAM] Using Agent SQL mode for data query, question_type={question_type.value}")
                return StreamingResponse(
                    _stream_response_generator(
                        response_generator,
                        tenant_id,
                        db,
                        original_question,
                        request.data_source_ids,
                        initial_messages=messages,  # ä¼ é€’åˆå§‹æ¶ˆæ¯å†å²
                        schema_info=schema_info,  # ä¼ é€’Schemaè·å–ä¿¡æ¯
                        question_type=question_type  # ğŸ”§ ä¼ é€’é—®é¢˜ç±»å‹
                    ),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "Access-Control-Allow-Origin": "*",
                        "Access-Control-Allow-Headers": "Cache-Control"
                    }
                )
            else:
                # ğŸ”§ ä¿®å¤ï¼šæ™®é€šå¯¹è¯æ¨¡å¼ï¼ˆåŒ…æ‹¬ç®€å•é—®å€™ï¼‰ä½¿ç”¨åŠ¨æ€æ­¥éª¤æµç¨‹
                # å³ä½¿æœ‰æ•°æ®æºï¼Œå¦‚æœæ˜¯ç®€å•å¯¹è¯ä¹Ÿèµ°è¿™é‡Œ
                logger.info(f"[STREAM] Using General Chat mode (dynamic steps: {question_type.value})")
                return StreamingResponse(
                    _stream_general_chat_generator(
                        response_generator,
                        tenant_id,
                        original_question,
                        has_data_source=is_agent_mode  # ä¼ é€’å®é™…çš„æ•°æ®æºçŠ¶æ€
                    ),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "Access-Control-Allow-Origin": "*",
                        "Access-Control-Allow-Headers": "Cache-Control"
                    }
                )
        else:
            # éæµå¼å“åº”
            llm_start = time.time()
            response = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )
            logger.info(f"[PERF] llm_service.chat_completion took {time.time() - llm_start:.2f}s")

            if isinstance(response, LLMResponse):
                # æ£€æµ‹å¹¶æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨ä¹‹å‰æå–çš„åŸå§‹é—®é¢˜å’ŒæŒ‡å®šçš„æ•°æ®æºï¼‰
                enhanced_content = await _execute_sql_if_needed(
                    response.content,
                    tenant_id,
                    db,
                    original_question,
                    request.data_source_ids
                )

                # æ›´æ–°å“åº”å†…å®¹
                response.content = enhanced_content

                return _convert_response(response)
            else:
                raise HTTPException(
                    status_code=500,
                    detail="Unexpected response type from LLM service"
                )

    except Exception as e:
        logger.error(f"Chat completion failed for tenant {tenant_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chat completion failed: {str(e)}"
        )


@router.get("/providers/status", response_model=ProviderStatusResponse)
async def get_provider_status(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–LLMæä¾›å•†çŠ¶æ€
    """
    try:
        tenant_id = current_user.tenant_id
        status = await llm_service.validate_providers(tenant_id)

        return ProviderStatusResponse(
            zhipu=status.get("zhipu", False),
            openrouter=status.get("openrouter", False)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get provider status: {str(e)}"
        )


@router.get("/models", response_model=AvailableModelsResponse)
async def get_available_models(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    try:
        tenant_id = current_user.tenant_id
        models = await llm_service.get_available_models(tenant_id)

        return AvailableModelsResponse(providers=models)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get available models: {str(e)}"
        )


@router.post("/test")
async def test_llm_service(
    provider: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•LLMæœåŠ¡
    """
    try:
        tenant_id = current_user.tenant_id

        # è½¬æ¢æä¾›å•†
        llm_provider = None
        if provider:
            try:
                llm_provider = LLMProvider(provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {provider}"
                )

        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        test_messages = [
            LLMMessage(
                role="user",
                content="ä½ å¥½ï¼Œè¯·å›å¤ä¸€æ¡ç®€çŸ­çš„æµ‹è¯•æ¶ˆæ¯"
            )
        ]

        # è°ƒç”¨LLMæœåŠ¡
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=llm_provider,
            max_tokens=50
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "LLM service test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"LLM service test failed: {str(e)}"
        }


@router.post("/test/multimodal")
async def test_multimodal(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€åŠŸèƒ½ï¼ˆéœ€è¦OpenRouterï¼‰
    """
    try:
        tenant_id = current_user.tenant_id

        # åˆ›å»ºå¤šæ¨¡æ€æµ‹è¯•æ¶ˆæ¯
        multimodal_content = [
            {
                "type": "text",
                "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            }
        ]

        test_messages = [
            LLMMessage(
                role="user",
                content=multimodal_content
            )
        ]

        # ä¼˜å…ˆä½¿ç”¨OpenRouterè¿›è¡Œå¤šæ¨¡æ€æµ‹è¯•
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=LLMProvider.OPENROUTER,
            max_tokens=200
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "Multimodal test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"Multimodal test failed: {str(e)}"
        }


# ===== æ–°å¢çš„é«˜çº§LLMåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹ =====

@router.get("/test/stream-thinking")
async def test_stream_thinking(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
    """
    try:
        from src.app.services.llm_service import LLMMessage

        messages = [
            LLMMessage(
                role="user",
                content="è¯·è¯¦ç»†åˆ†ææœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨åœºæ™¯"
            )
        ]

        async def stream_generator():
            try:
                # å¿…é¡»å…ˆ await è·å– AsyncGenerator å¯¹è±¡
                response_generator = await llm_service.chat_completion(
                    tenant_id=current_user.id,
                    messages=messages,
                    stream=True,
                    enable_thinking=None  # è‡ªåŠ¨åˆ¤æ–­
                )
                # ç„¶åæ‰èƒ½ä½¿ç”¨ async for è¿­ä»£ç”Ÿæˆå™¨
                async for chunk in response_generator:
                    yield f"data: {json.dumps(chunk.dict(), ensure_ascii=False)}\n\n"

                yield "data: [DONE]\n\n"
            except Exception as e:
                yield f"data: {{'type': 'error', 'content': '{str(e)}'}}\n\n"

        return StreamingResponse(
            stream_generator(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stream thinking test failed: {str(e)}")


@router.get("/test/intelligent-params")
async def test_intelligent_params(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æ™ºèƒ½å‚æ•°è°ƒæ•´åŠŸèƒ½
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„é—®é¢˜
        test_cases = [
            {
                "name": "ç®€å•é—®é¢˜",
                "messages": [LLMMessage(role="user", content="ä½ å¥½")]
            },
            {
                "name": "å¤æ‚é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†ã€å¸¸è§çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒæŠ€å·§ä»¥åŠåœ¨å®é™…é¡¹ç›®ä¸­çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¹¶è®¨è®ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
                )]
            },
            {
                "name": "éœ€è¦æ€è€ƒçš„é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="ä¸ºä»€ä¹ˆTransformeræ¶æ„èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„RNNæ¨¡å‹ï¼Ÿè¯·ä»æ³¨æ„åŠ›æœºåˆ¶ã€å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€é•¿æœŸä¾èµ–å¤„ç†ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œæ·±å…¥åˆ†æã€‚"
                )]
            }
        ]

        results = []

        for case in test_cases:
            complexity = llm_service.analyze_conversation_complexity(case["messages"])

            # ä½¿ç”¨æ™ºèƒ½å‚æ•°è°ƒç”¨
            response = await llm_service.chat_completion(
                tenant_id=current_user.id,
                messages=case["messages"],
                enable_thinking=None,  # è‡ªåŠ¨åˆ¤æ–­
                temperature=complexity.get("recommend_temperature", 0.7),
                max_tokens=complexity.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
            )

            results.append({
                "case_name": case["name"],
                "complexity_analysis": complexity,
                "response_type": type(response).__name__,
                "success": response is not None
            })

        return {
            "success": True,
            "message": "Intelligent parameter adjustment test completed",
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Intelligent params test failed: {str(e)}")


@router.get("/test/multimodal-upload")
async def test_multimodal_upload(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€å†…å®¹å¤„ç†å’ŒMinIOä¸Šä¼ 
    """
    try:
        from src.app.services.llm_service import LLMMessage
        from src.app.services.multimodal_processor import multimodal_processor

        # æ„å»ºåŒ…å«å›¾ç‰‡URLçš„æµ‹è¯•æ¶ˆæ¯
        test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

        messages = [
            LLMMessage(
                role="user",
                content=[
                    {
                        "type": "text",
                        "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": test_image_url
                        }
                    }
                ]
            )
        ]

        # æµ‹è¯•å¤šæ¨¡æ€å¤„ç†
        processed_content = await multimodal_processor.process_content_list(
            messages[0].content,
            current_user.id
        )

        # å°è¯•è°ƒç”¨OpenRouterï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            provider=LLMProvider.OPENROUTER,
            stream=False
        )

        return {
            "success": True,
            "message": "Multimodal upload test completed",
            "original_content_count": len(messages[0].content),
            "processed_content_count": len(processed_content),
            "response_received": response is not None,
            "processed_sample": processed_content[:2] if processed_content else []
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Multimodal upload test failed: {str(e)}")


@router.get("/test/tenant-isolation")
async def test_tenant_isolation(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•ç§Ÿæˆ·éš”ç¦»åŠŸèƒ½
    """
    try:
        from src.app.services.tenant_config_manager import tenant_config_manager, ProviderType

        # æµ‹è¯•ç§Ÿæˆ·é…ç½®è·å–
        test_tenant_id = current_user.id
        providers = []

        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            api_key = await tenant_config_manager.get_tenant_api_key(
                test_tenant_id, provider, use_global_fallback=True
            )
            if api_key:
                providers.append(provider.value)

        # æµ‹è¯•æ¨¡å‹é…ç½®è·å–
        model_configs = {}
        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            config = await tenant_config_manager.get_tenant_model_config(test_tenant_id, provider)
            model_configs[provider.value] = config

        # éªŒè¯ç§Ÿæˆ·é…ç½®
        validation_results = await tenant_config_manager.validate_tenant_config(test_tenant_id)

        return {
            "success": True,
            "message": "Tenant isolation test completed",
            "tenant_id": test_tenant_id,
            "available_providers": providers,
            "model_configs": model_configs,
            "validation_results": validation_results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tenant isolation test failed: {str(e)}")


@router.get("/test/data-sources-context")
async def test_data_sources_context(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•æ•°æ®æºä¸Šä¸‹æ–‡è·å–
    """
    tenant_id = current_user.get("tenant_id", "")
    context = await _get_data_sources_context(tenant_id, db)
    return {
        "tenant_id": tenant_id,
        "context_length": len(context),
        "context": context
    }


@router.get("/test/all-features")
async def test_all_features(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    ç»¼åˆåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æ„å»ºå¤æ‚çš„æµ‹è¯•åœºæ™¯
        messages = [
            LLMMessage(
                role="user",
                content="ä½œä¸ºä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·åˆ†æå’Œè¯„ä¼°å½“å‰å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„å‘å±•ç°çŠ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ¶æ„ã€åº”ç”¨åœºæ™¯ã€ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å‘å±•è¶‹åŠ¿ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„å»ºè®®ã€‚"
            )
        ]

        # è·å–å¯¹è¯å¤æ‚åº¦åˆ†æ
        complexity_analysis = llm_service.analyze_conversation_complexity(messages)

        # è°ƒç”¨èŠå¤©å®Œæˆï¼ˆå¯ç”¨æ™ºèƒ½æ€è€ƒæ¨¡å¼ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            stream=False,
            enable_thinking=None,  # è‡ªåŠ¨å¯ç”¨æ€è€ƒæ¨¡å¼
            temperature=complexity_analysis.get("recommend_temperature", 0.7),
            max_tokens=complexity_analysis.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
        )

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = await llm_service.get_available_models(current_user.id)

        # éªŒè¯æä¾›å•†çŠ¶æ€
        provider_status = await llm_service.validate_providers(current_user.id)

        return {
            "success": True,
            "message": "Comprehensive LLM features test completed",
            "complexity_analysis": complexity_analysis,
            "response_received": response is not None,
            "available_models": available_models,
            "provider_status": provider_status,
            "features_tested": [
                "æ™ºèƒ½æ€è€ƒæ¨¡å¼",
                "å¤æ‚åº¦åˆ†æ",
                "æ™ºèƒ½å‚æ•°è°ƒæ•´",
                "å¤šæä¾›å•†æ”¯æŒ",
                "ç§Ÿæˆ·éš”ç¦»"
            ]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Comprehensive test failed: {str(e)}")