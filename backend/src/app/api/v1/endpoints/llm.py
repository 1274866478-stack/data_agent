"""
LLM APIç«¯ç‚¹
æä¾›ç»Ÿä¸€çš„èŠå¤©å®Œæˆã€æµå¼è¾“å‡ºå’Œå¤šæ¨¡æ€æ”¯æŒ
"""

import json
import asyncio
import logging
import io
import os
import sys
import time
from decimal import Decimal
from typing import Dict, Any, Optional, List, Union
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import pandas as pd

from src.app.services.llm_service import (
    llm_service,
    LLMProvider,
    LLMMessage,
    LLMResponse,
    LLMStreamChunk
)
from src.app.core.auth import get_current_user_with_tenant
from src.app.data.models import Tenant, DataSourceConnection, DataSourceConnectionStatus
from src.app.data.database import get_db
from src.app.services.data_source_service import data_source_service
from src.app.services.minio_client import minio_service
from src.app.services.database_interface import PostgreSQLAdapter
from src.app.services.zhipu_client import zhipu_service
import re
import duckdb

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["LLM"])


def _convert_decimal_to_float(data: Any) -> Any:
    """
    é€’å½’åœ°å°†æ•°æ®ä¸­çš„ Decimal ç±»å‹è½¬æ¢ä¸º floatï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
    
    Args:
        data: éœ€è¦è½¬æ¢çš„æ•°æ®ï¼ˆå¯ä»¥æ˜¯ dict, list, æˆ–å…¶ä»–ç±»å‹ï¼‰
    
    Returns:
        è½¬æ¢åçš„æ•°æ®ï¼Œå…¶ä¸­æ‰€æœ‰ Decimal éƒ½å˜æˆäº† float
    """
    if isinstance(data, Decimal):
        return float(data)
    elif isinstance(data, dict):
        return {k: _convert_decimal_to_float(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [_convert_decimal_to_float(item) for item in data]
    else:
        return data


# ============================================================
# å·¥å…·å®šä¹‰ (Tool Definitions) - OpenAI Function Calling æ ¼å¼
# ============================================================

# SQL æ‰§è¡Œå·¥å…· Schema
SQL_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "execute_sql",
        "description": "æ‰§è¡Œ SQL SELECT æŸ¥è¯¢ä»¥è·å–æ•°æ®åº“æ•°æ®ã€‚åªèƒ½æ‰§è¡Œ SELECT æŸ¥è¯¢ï¼Œç¦æ­¢æ‰§è¡Œ INSERTã€UPDATEã€DELETE ç­‰ä¿®æ”¹æ“ä½œã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "sql_query": {
                    "type": "string",
                    "description": "è¦æ‰§è¡Œçš„ SQL SELECT æŸ¥è¯¢è¯­å¥"
                }
            },
            "required": ["sql_query"]
        }
    }
}

# å›¾è¡¨ç”Ÿæˆå·¥å…· Schema
CHART_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "generate_chart",
        "description": "æ ¹æ®æ•°æ®ç”Ÿæˆ ECharts å›¾è¡¨é…ç½®ã€‚å½“æ•°æ®åŒ…å«æ•°å­—ã€è¶‹åŠ¿æˆ–åˆ†ç±»å¯¹æ¯”æ—¶å¿…é¡»è°ƒç”¨æ­¤å·¥å…·è¿›è¡Œå¯è§†åŒ–ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "chart_type": {
                    "type": "string",
                    "enum": ["bar", "line", "pie", "scatter"],
                    "description": "å›¾è¡¨ç±»å‹ï¼šbar(æŸ±çŠ¶å›¾-åˆ†ç±»å¯¹æ¯”), line(æŠ˜çº¿å›¾-è¶‹åŠ¿å˜åŒ–), pie(é¥¼å›¾-å æ¯”åˆ†å¸ƒ), scatter(æ•£ç‚¹å›¾-ç›¸å…³æ€§)"
                },
                "title": {
                    "type": "string",
                    "description": "å›¾è¡¨æ ‡é¢˜"
                },
                "x_data": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Xè½´æ•°æ®ï¼ˆåˆ†ç±»åç§°æˆ–æ—¶é—´ï¼‰"
                },
                "y_data": {
                    "type": "array",
                    "items": {"type": "number"},
                    "description": "Yè½´æ•°æ®ï¼ˆæ•°å€¼ï¼‰"
                },
                "series_name": {
                    "type": "string",
                    "description": "æ•°æ®ç³»åˆ—åç§°ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "required": ["chart_type", "title", "x_data", "y_data"]
        }
    }
}


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser, assistant, system")
    content: Union[str, List[Dict[str, Any]]] = Field(
        ...,
        description="æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€å†…å®¹"
    )
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹ï¼ˆä»…assistantè§’è‰²ï¼‰")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹"""
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    provider: Optional[str] = Field(
        None,
        description="LLMæä¾›å•†ï¼šzhipu, openrouterï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©"
    )
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§è¾“å‡ºtokens")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•°(0-1)")
    stream: bool = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
    enable_thinking: bool = Field(False, description="æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä»…Zhipuæ”¯æŒï¼‰")
    data_source_ids: Optional[List[str]] = Field(None, description="æŒ‡å®šä½¿ç”¨çš„æ•°æ®æºIDåˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰æ´»è·ƒæ•°æ®æº")


class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”æ¨¡å‹"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨æƒ…å†µ")
    model: Optional[str] = Field(None, description="ä½¿ç”¨çš„æ¨¡å‹")
    provider: Optional[str] = Field(None, description="ä½¿ç”¨çš„æä¾›å•†")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")
    created_at: Optional[str] = Field(None, description="åˆ›å»ºæ—¶é—´")


class ProviderStatusResponse(BaseModel):
    """æä¾›å•†çŠ¶æ€å“åº”æ¨¡å‹"""
    zhipu: bool = Field(..., description="æ™ºè°±AIå¯ç”¨æ€§")
    openrouter: bool = Field(..., description="OpenRouterå¯ç”¨æ€§")


class AvailableModelsResponse(BaseModel):
    """å¯ç”¨æ¨¡å‹å“åº”æ¨¡å‹"""
    providers: Dict[str, List[str]] = Field(..., description="å„æä¾›å•†çš„å¯ç”¨æ¨¡å‹")


def _convert_chat_messages(messages: List[ChatMessage]) -> List[LLMMessage]:
    """è½¬æ¢èŠå¤©æ¶ˆæ¯æ ¼å¼"""
    llm_messages = []
    for msg in messages:
        llm_messages.append(LLMMessage(
            role=msg.role,
            content=msg.content,
            thinking=msg.thinking
        ))
    return llm_messages


def _get_column_type(dtype_str: str) -> str:
    """å°†pandasæ•°æ®ç±»å‹è½¬æ¢ä¸ºå‹å¥½çš„ç±»å‹æè¿°"""
    if 'int' in dtype_str:
        return 'integer'
    elif 'float' in dtype_str:
        return 'float'
    elif 'datetime' in dtype_str:
        return 'datetime'
    elif 'bool' in dtype_str:
        return 'boolean'
    else:
        return 'text'


def _build_table_schema(df: pd.DataFrame, table_name: str) -> Dict[str, Any]:
    """ä»DataFrameæ„å»ºå•ä¸ªè¡¨çš„schemaä¿¡æ¯"""
    columns = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        columns.append({
            "name": str(col),
            "type": _get_column_type(dtype),
            "nullable": df[col].isnull().any()
        })

    # è·å–ç¤ºä¾‹æ•°æ®ï¼ˆå‰5è¡Œï¼‰
    sample_rows = []
    for _, row in df.head(5).iterrows():
        row_data = {}
        for col in df.columns[:10]:  # é™åˆ¶åˆ—æ•°
            value = row[col]
            if pd.isna(value):
                row_data[str(col)] = None
            else:
                row_data[str(col)] = str(value)
        sample_rows.append(row_data)

    return {
        "table_info": {
            "name": table_name,
            "columns": columns,
            "row_count": len(df)
        },
        "sample_data": {
            "columns": [str(c) for c in df.columns[:10]],
            "data": sample_rows
        }
    }


async def _get_file_schema(connection_string: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶æ•°æ®æºè·å–schemaä¿¡æ¯ï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼ˆæ ¼å¼: file://data-sources/{tenant_id}/{file_id}.xlsx æˆ– /app/uploads/...ï¼‰
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsç­‰ï¼‰
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¡¨ï¼ˆSheetï¼‰çš„åˆ—åã€ç±»å‹å’Œç¤ºä¾‹æ•°æ®
    """
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„è·¯å¾„è§£æé€»è¾‘ï¼Œä¼˜å…ˆå°è¯•æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
        from src.app.services.agent.path_extractor import resolve_file_path_with_fallback
        
        # é¦–å…ˆå°è¯•è§£æè·¯å¾„ï¼ˆåŒ…å«æœ¬åœ°å›é€€é€»è¾‘ï¼‰
        local_file_path = resolve_file_path_with_fallback(connection_string)
        file_data = None
        use_local_file = False
        
        # å¦‚æœæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
        if local_file_path and os.path.exists(local_file_path):
            logger.info(f"ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶: {local_file_path}")
            use_local_file = True
            file_path_for_read = local_file_path
        else:
            # å°è¯•ä»MinIOä¸‹è½½
            storage_path = connection_string[7:] if connection_string.startswith("file://") else connection_string
            logger.info(f"å°è¯•ä»MinIOä¸‹è½½æ–‡ä»¶: {storage_path}")
            file_data = minio_service.download_file(
                bucket_name="data-sources",
                object_name=storage_path
            )
            
            if not file_data:
                logger.warning(f"æ— æ³•ä»MinIOè·å–æ–‡ä»¶: {storage_path}")
                # æœ€åå°è¯•æœ¬åœ°å›é€€
                if local_file_path:
                    logger.info(f"å°è¯•ä½¿ç”¨è§£æçš„æœ¬åœ°è·¯å¾„: {local_file_path}")
                    if os.path.exists(local_file_path):
                        use_local_file = True
                        file_path_for_read = local_file_path
                    else:
                        return {}
                else:
                    return {}

        tables = []
        sample_data = {}

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheet
            try:
                if use_local_file:
                    # ä»æœ¬åœ°æ–‡ä»¶è¯»å–
                    excel_file = pd.ExcelFile(file_path_for_read, engine='openpyxl')
                else:
                    # ä»MinIOä¸‹è½½çš„æ•°æ®è¯»å–
                    excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                return {}
            except Exception as e:
                logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                return {}
            
            sheet_names = excel_file.sheet_names
            logger.info(f"Excelæ–‡ä»¶åŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨å
                    table_schema = _build_table_schema(df, sheet_name)
                    tables.append(table_schema["table_info"])
                    sample_data[sheet_name] = table_schema["sample_data"]
                    logger.info(f"Sheet '{sheet_name}': {len(df)}è¡Œ, {len(df.columns)}åˆ—")
                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            if use_local_file:
                # ä»æœ¬åœ°æ–‡ä»¶è¯»å–
                for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                    try:
                        df = pd.read_csv(file_path_for_read, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
            else:
                # ä»MinIOä¸‹è½½çš„æ•°æ®è¯»å–
                for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                    try:
                        df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue

            if df is not None:
                table_schema = _build_table_schema(df, data_source_name)
                tables.append(table_schema["table_info"])
                sample_data[data_source_name] = table_schema["sample_data"]

        if not tables:
            path_info = file_path_for_read if use_local_file else (connection_string[7:] if connection_string.startswith("file://") else connection_string)
            logger.warning(f"æ— æ³•ä»æ–‡ä»¶è§£æä»»ä½•è¡¨: {path_info}")
            return {}

        schema_info = {
            "tables": tables,
            "sample_data": sample_data
        }

        total_rows = sum(t.get("row_count", 0) for t in tables)
        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
        return schema_info

    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶schemaå¤±è´¥: {e}")
        return {}


async def _try_find_file_in_minio(tenant_id: str, data_source_id: str, db_type: str) -> Optional[str]:
    """
    å°è¯•åœ¨MinIOä¸­æŸ¥æ‰¾æ•°æ®æºå¯¹åº”çš„æ–‡ä»¶

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹

    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
        prefix = f"{tenant_id}/{data_source_id}"
        ext = f".{db_type}"

        # åˆ—å‡ºMinIOä¸­çš„æ–‡ä»¶
        objects = minio_service.list_files(
            bucket_name="data-sources",
            prefix=prefix
        )

        for obj in objects:
            obj_name = obj.object_name if hasattr(obj, 'object_name') else str(obj)
            if obj_name.endswith(ext):
                return f"file://{obj_name}"

        logger.warning(f"åœ¨MinIOä¸­æœªæ‰¾åˆ°æ•°æ®æº {data_source_id} çš„æ–‡ä»¶")
        return None

    except Exception as e:
        logger.error(f"åœ¨MinIOä¸­æœç´¢æ–‡ä»¶å¤±è´¥: {e}")
        return None


async def _try_get_file_schema_fallback(tenant_id: str, data_source_id: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ä»MinIOç›´æ¥è·å–æ–‡ä»¶schemaï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
        ext = f".{db_type}"
        possible_paths = [
            f"{tenant_id}/{data_source_id}{ext}",
            f"{tenant_id}/{data_source_name}{ext}",
        ]

        for path in possible_paths:
            try:
                logger.info(f"å°è¯•ä»MinIOè·å–æ–‡ä»¶: {path}")
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=path
                )

                if file_data:
                    tables = []
                    sample_data = {}

                    if db_type in ["xlsx", "xls"]:
                        # è¯»å–æ‰€æœ‰Sheet
                        try:
                            # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                            excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
                        except ImportError as e:
                            logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        except Exception as e:
                            logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        
                        sheet_names = excel_file.sheet_names
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆ: ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

                        for sheet_name in sheet_names:
                            try:
                                # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                                df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                                if df.empty:
                                    continue
                                table_schema = _build_table_schema(df, sheet_name)
                                tables.append(table_schema["table_info"])
                                sample_data[sheet_name] = table_schema["sample_data"]
                            except Exception as e:
                                logger.debug(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                                continue

                    elif db_type == "csv":
                        df = None
                        for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                            try:
                                df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                                break
                            except UnicodeDecodeError:
                                continue

                        if df is not None:
                            table_schema = _build_table_schema(df, data_source_name)
                            tables.append(table_schema["table_info"])
                            sample_data[data_source_name] = table_schema["sample_data"]

                    if tables:
                        total_rows = sum(t.get("row_count", 0) for t in tables)
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆæˆåŠŸè·å–schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
                        return {
                            "tables": tables,
                            "sample_data": sample_data
                        }

            except Exception as e:
                logger.debug(f"å°è¯•è·¯å¾„ {path} å¤±è´¥: {e}")
                continue

        logger.warning(f"å¤‡é€‰æ–¹æ¡ˆæœªèƒ½è·å–æ•°æ®æº {data_source_name} çš„schema")
        return {}

    except Exception as e:
        logger.error(f"å¤‡é€‰æ–¹æ¡ˆè·å–schemaå¤±è´¥: {e}")
        return {}


async def _get_data_sources_context(tenant_id: str, db: Session, data_source_ids: Optional[List[str]] = None) -> str:
    """
    è·å–ç§Ÿæˆ·æ•°æ®æºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬schemaï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰æ´»è·ƒæ•°æ®æº

    Returns:
        æ•°æ®æºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    start_time = time.time()
    try:
        # è·å–ç§Ÿæˆ·çš„æ‰€æœ‰æ´»è·ƒæ•°æ®æº
        t1 = time.time()
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )
        perf_msg = f"[PERF] get_data_sources took {time.time() - t1:.2f}s, found {len(data_sources) if data_sources else 0} sources"
        print(perf_msg)  # ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
        logger.info(perf_msg)

        if not data_sources:
            return ""

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        if data_source_ids:
            original_count = len(data_sources)
            data_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            logger.info(f"ğŸ¯ [æ•°æ®æºç­›é€‰] æŒ‡å®šæ•°æ®æº: {data_source_ids}, ä» {original_count} ä¸ªä¸­ç­›é€‰å‡º {len(data_sources)} ä¸ªåŒ¹é…çš„æ•°æ®æº")
            for ds in data_sources:
                logger.info(f"  âœ… ä½¿ç”¨æ•°æ®æº: {ds.name} (ID: {ds.id}, ç±»å‹: {ds.db_type})")
            if not data_sources:
                logger.warning(f"âš ï¸ [æ•°æ®æºç­›é€‰] æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æºï¼è¯·æ±‚çš„ID: {data_source_ids}")
                return ""
        else:
            logger.warning(f"âš ï¸ [æ•°æ®æºç­›é€‰] æœªæŒ‡å®š data_source_idsï¼Œå°†ä½¿ç”¨æ‰€æœ‰ {len(data_sources)} ä¸ªæ´»è·ƒæ•°æ®æº:")
            for ds in data_sources:
                logger.info(f"  ğŸ“¦ æ´»è·ƒæ•°æ®æº: {ds.name} (ID: {ds.id}, ç±»å‹: {ds.db_type})")

        context_parts = []
        context_parts.append("## å¯ç”¨æ•°æ®æº\n")

        for ds in data_sources:
            try:
                ds_start = time.time()
                schema_info = None
                connection_string = None

                # å°è¯•è·å–è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
                try:
                    t2 = time.time()
                    connection_string = await data_source_service.get_decrypted_connection_string(
                        data_source_id=ds.id,
                        tenant_id=tenant_id,
                        db=db
                    )
                    print(f"[PERF] get_decrypted_connection_string for {ds.name} took {time.time() - t2:.2f}s")
                except Exception as decrypt_error:
                    print(f"[PERF] è§£å¯†æ•°æ®æº {ds.name} è¿æ¥å­—ç¬¦ä¸²å¤±è´¥: {decrypt_error}")
                    # å¯¹äºæ–‡ä»¶ç±»å‹æ•°æ®æºï¼Œå°è¯•ä»MinIOç›´æ¥æœç´¢æ–‡ä»¶
                    if ds.db_type in ["xlsx", "xls", "csv"]:
                        connection_string = await _try_find_file_in_minio(tenant_id, ds.id, ds.db_type)

                # æ ¹æ®æ•°æ®æºç±»å‹è·å–schema
                if ds.db_type == "postgresql" and connection_string:
                    t3 = time.time()
                    adapter = PostgreSQLAdapter(connection_string)
                    try:
                        await adapter.connect()
                        schema_result = await adapter.get_schema_info()
                        # å°†SchemaInfoå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        schema_info = {
                            "database_type": schema_result.database_type.value if schema_result.database_type else "postgresql",
                            "tables": [
                                {
                                    "name": table.name,
                                    "columns": [
                                        {
                                            "name": col.name,
                                            "type": col.data_type,
                                            "nullable": col.is_nullable
                                        }
                                        for col in table.columns
                                    ]
                                }
                                for table in schema_result.tables.values()
                            ] if schema_result.tables else []
                        }
                    finally:
                        await adapter.disconnect()
                    print(f"[PERF] PostgreSQL get_schema for {ds.name} took {time.time() - t3:.2f}s")

                elif ds.db_type in ["xlsx", "xls", "csv"]:
                    # ğŸ”§ ä¿®å¤ï¼šä»connection_configæˆ–connection_stringæå–æ–‡ä»¶è·¯å¾„
                    file_path = connection_string
                    if hasattr(ds, 'connection_config') and ds.connection_config:
                        # å¦‚æœå­˜åœ¨connection_configå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
                        from src.app.services.agent.path_extractor import extract_file_path_from_config
                        extracted_path = extract_file_path_from_config(ds.connection_config, connection_string)
                        if extracted_path:
                            file_path = extracted_path
                    
                    if file_path:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä»æ–‡ä»¶è¯»å–å¹¶è§£æschema
                        t4 = time.time()
                        schema_info = await _get_file_schema(file_path, ds.db_type, ds.name)
                        print(f"[PERF] _get_file_schema for {ds.name} took {time.time() - t4:.2f}s")
                    else:
                        # è¿æ¥å­—ç¬¦ä¸²è·å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
                        print(f"[PERF] å°è¯•å¤‡é€‰æ–¹æ¡ˆè·å–æ•°æ®æº {ds.name} çš„schema")
                        schema_info = await _try_get_file_schema_fallback(tenant_id, ds.id, ds.db_type, ds.name)

                print(f"[PERF] Total processing for data source {ds.name} took {time.time() - ds_start:.2f}s")

                if schema_info and schema_info.get("tables"):
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ–‡ä»¶/æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")

                    # æ·»åŠ è¡¨ä¿¡æ¯
                    context_parts.append("\n#### è¡¨ç»“æ„:")
                    for table in schema_info["tables"][:20]:  # é™åˆ¶è¡¨æ•°é‡é¿å…tokenè¿‡å¤š
                        table_name = table.get("name", "unknown")
                        row_count = table.get("row_count", "æœªçŸ¥")
                        context_parts.append(f"\n**è¡¨: {table_name}** (å…±{row_count}è¡Œ)")

                        columns = table.get("columns", [])
                        if columns:
                            col_info = []
                            for col in columns[:30]:  # é™åˆ¶åˆ—æ•°é‡
                                col_name = col.get("name", "unknown")
                                col_type = col.get("type", "unknown")
                                nullable = "å¯ç©º" if col.get("nullable") else "éç©º"
                                col_info.append(f"  - {col_name} ({col_type}, {nullable})")
                            context_parts.append("\n".join(col_info))

                        # æ·»åŠ ä¸»é”®ä¿¡æ¯
                        if table.get("primary_key"):
                            context_parts.append(f"  - ä¸»é”®: {', '.join(table['primary_key'])}")

                        # æ·»åŠ å¤–é”®ä¿¡æ¯
                        if table.get("foreign_keys"):
                            for fk in table["foreign_keys"]:
                                context_parts.append(
                                    f"  - å¤–é”®: {fk['column']} -> {fk['references_table']}.{fk['references_column']}"
                                )

                    # æ·»åŠ è¡¨å…³ç³»ä¿¡æ¯ï¼ˆå¤–é”®å…³è”ï¼‰
                    relationships = schema_info.get("relationships", [])
                    if relationships:
                        context_parts.append("\n#### è¡¨å…³ç³»ï¼ˆå¤–é”®ï¼‰:")
                        context_parts.append("**é‡è¦ï¼š** ä»¥ä¸‹æ˜¯è¡¨ä¹‹é—´çš„å…³è”å…³ç³»ï¼ŒæŸ¥è¯¢æ—¶å¿…é¡»é€šè¿‡è¿™äº›å¤–é”®è¿›è¡ŒJOINï¼š")
                        for rel in relationships:
                            from_table = rel.get("from_table", "unknown")
                            from_column = rel.get("from_column", "unknown")
                            to_table = rel.get("to_table", "unknown")
                            to_column = rel.get("to_column", "unknown")
                            context_parts.append(
                                f"  - {from_table}.{from_column} -> {to_table}.{to_column}"
                            )

                    # æ·»åŠ ç¤ºä¾‹æ•°æ®
                    sample_data = schema_info.get("sample_data", {})
                    if sample_data:
                        context_parts.append("\n#### ç¤ºä¾‹æ•°æ®:")
                        for table_name, samples in list(sample_data.items())[:5]:  # é™åˆ¶è¡¨æ•°é‡
                            if samples.get("data"):
                                context_parts.append(f"\n**{table_name}** (å‰5è¡Œ):")
                                for row in samples["data"][:3]:  # é™åˆ¶è¡Œæ•°
                                    row_str = ", ".join([f"{k}={v}" for k, v in list(row.items())[:5]])
                                    context_parts.append(f"  {row_str}")
                else:
                    # å…¶ä»–æ•°æ®åº“ç±»å‹æš‚æ—¶åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")
                    context_parts.append("- æ³¨: æ­¤æ•°æ®åº“ç±»å‹çš„schemaå‘ç°åŠŸèƒ½å¼€å‘ä¸­")

            except Exception as e:
                logger.warning(f"è·å–æ•°æ®æº {ds.name} çš„schemaå¤±è´¥: {e}")
                context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                context_parts.append(f"- ç±»å‹: {ds.db_type}")
                context_parts.append(f"- æ³¨: æ— æ³•è·å–schemaä¿¡æ¯ ({str(e)[:50]})")

        total_time = time.time() - start_time
        logger.info(f"[PERF] _get_data_sources_context TOTAL took {total_time:.2f}s")
        return "\n".join(context_parts)

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return ""


def _build_system_prompt_with_context(data_sources_context: str) -> str:
    """
    æ„å»ºåŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨ SQL ä»£ç å—æ ¼å¼ï¼‰

    Args:
        data_sources_context: æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯

    Returns:
        ç³»ç»Ÿæç¤ºè¯
    """
    if data_sources_context:
        # æœ‰æ•°æ®æºæ—¶çš„å®Œæ•´æç¤ºï¼Œè¦æ±‚ä½¿ç”¨ ```sql ä»£ç å—æ ¼å¼
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼ŒæŸ¥è¯¢æ•°æ®åº“å¹¶ç»™å‡ºåˆ†æç»“æœã€‚

## ğŸ”´ é‡è¦è§„åˆ™ ğŸ”´

**ä½ å¿…é¡»ä½¿ç”¨ SQL ä»£ç å—æ¥æŸ¥è¯¢æ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š**

```sql
SELECT * FROM è¡¨å WHERE æ¡ä»¶;
```

**ç¦æ­¢ä½¿ç”¨ä»»ä½•å·¥å…·è°ƒç”¨æˆ–å‡½æ•°è°ƒç”¨æ ¼å¼ï¼åªéœ€è¦åœ¨å›ç­”ä¸­ç›´æ¥å†™ SQL ä»£ç å—å³å¯ã€‚**

---

## å·¥ä½œæµç¨‹

### æ­¥éª¤ 1: åˆ†æé—®é¢˜
- ç†è§£ç”¨æˆ·æƒ³è¦æŸ¥è¯¢ä»€ä¹ˆæ•°æ®
- æ ¹æ®ä¸‹æ–¹çš„æ•°æ®åº“ Schema ç¡®å®šéœ€è¦æŸ¥è¯¢çš„è¡¨å’Œå­—æ®µ

### æ­¥éª¤ 2: ç”Ÿæˆ SQL
- åœ¨å›ç­”ä¸­ä½¿ç”¨ ```sql ... ``` ä»£ç å—æ ¼å¼è¾“å‡º SQL æŸ¥è¯¢è¯­å¥
- å¿…é¡»ä½¿ç”¨ä¸‹æ–¹ Schema ä¸­çš„**å®é™…è¡¨åå’Œåˆ—å**ï¼Œç¦æ­¢çŒœæµ‹
- ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œä½ çš„ SQL å¹¶è¿”å›ç»“æœ

### æ­¥éª¤ 3: åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰
- å¦‚æœä½ å·²ç»çŸ¥é“æ•°æ®ç‰¹å¾ï¼Œå¯ä»¥ç®€è¦è¯´æ˜é¢„æœŸçš„åˆ†ææ–¹å‘
- ä¾‹å¦‚ï¼š"è®©æˆ‘æŸ¥è¯¢ä¸€ä¸‹å„äº§å“çš„é”€é‡æ•°æ®..."

---

## æ•°æ®åº“ Schema

{data_sources_context}

---

## ğŸ”¥ æ—¥æœŸå¤„ç†é‡è¦è¯´æ˜ï¼ˆé’ˆå¯¹ Excel/CSV æ–‡ä»¶æ•°æ®æºï¼‰

**å¯¹äº Excel/CSV æ–‡ä»¶æ•°æ®æºï¼Œæ—¥æœŸå­—æ®µé€šå¸¸å­˜å‚¨ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å¤„ç†ï¼š**

### æ–¹å¼1ï¼šä½¿ç”¨ CAST è½¬æ¢åæ¯”è¾ƒï¼ˆæ¨èï¼‰
```sql
SELECT * FROM è®¢å•è¡¨ 
WHERE CAST(created_at AS DATE) >= '2023-01-01' 
  AND CAST(created_at AS DATE) < '2024-01-01';
```

### æ–¹å¼2ï¼šä½¿ç”¨ LIKE è¿›è¡Œæ–‡æœ¬åŒ¹é…
```sql
-- ç­›é€‰2023å¹´çš„æ•°æ®
SELECT * FROM è®¢å•è¡¨ WHERE created_at LIKE '2023%';

-- ç­›é€‰2023å¹´æŸæœˆçš„æ•°æ®
SELECT * FROM è®¢å•è¡¨ WHERE created_at LIKE '2023-06%';
```

### æ–¹å¼3ï¼šæŒ‰å¹´æœˆåˆ†ç»„ç»Ÿè®¡
```sql
-- ä½¿ç”¨ strftime éœ€è¦å…ˆè½¬æ¢ä¸ºæ—¥æœŸç±»å‹
SELECT 
    strftime(CAST(created_at AS DATE), '%Y-%m') as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡
FROM è®¢å•è¡¨ 
WHERE created_at LIKE '2023%'
GROUP BY strftime(CAST(created_at AS DATE), '%Y-%m')
ORDER BY æœˆä»½;
```

### æ–¹å¼4ï¼šä½¿ç”¨ SUBSTRING æå–å¹´æœˆï¼ˆæ›´é€šç”¨ï¼‰
```sql
SELECT 
    SUBSTRING(created_at, 1, 7) as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡,
    SUM(final_amount) as æ€»é”€å”®é¢
FROM è®¢å•è¡¨ 
WHERE SUBSTRING(created_at, 1, 4) = '2023'
GROUP BY SUBSTRING(created_at, 1, 7)
ORDER BY æœˆä»½;
```

---

## SQL æ ¼å¼ç¤ºä¾‹

ç”¨æˆ·é—®ï¼š"åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘å¸®ä½ æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯ï¼š

```sql
SELECT * FROM ç”¨æˆ·è¡¨;
```

ç”¨æˆ·é—®ï¼š"å“ªä¸ªäº§å“å–å¾—æœ€å¥½ï¼Ÿ"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘æŸ¥è¯¢å„äº§å“çš„é”€é‡æ’åï¼š

```sql
SELECT äº§å“åç§°, SUM(é”€é‡) as æ€»é”€é‡ 
FROM è®¢å•è¡¨ 
GROUP BY äº§å“åç§° 
ORDER BY æ€»é”€é‡ DESC 
LIMIT 10;
```

ç”¨æˆ·é—®ï¼š"2023å¹´çš„é”€å”®è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
ä½ çš„å›ç­”ï¼š
è®©æˆ‘æŸ¥è¯¢2023å¹´æŒ‰æœˆçš„é”€å”®è¶‹åŠ¿ï¼š

```sql
SELECT 
    SUBSTRING(created_at, 1, 7) as æœˆä»½,
    COUNT(*) as è®¢å•æ•°é‡,
    SUM(final_amount) as æ€»é”€å”®é¢
FROM è®¢å•è¡¨ 
WHERE SUBSTRING(created_at, 1, 4) = '2023'
GROUP BY SUBSTRING(created_at, 1, 7)
ORDER BY æœˆä»½;
```

---

**è®°ä½ï¼šåªéœ€è¦åœ¨å›ç­”ä¸­å†™ SQL ä»£ç å—ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœï¼**"""
    else:
        # æ²¡æœ‰æ•°æ®æºæ—¶çš„æç¤º
        return """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚

å½“å‰ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰è¿æ¥ä»»ä½•æ•°æ®æºã€‚

å¦‚æœç”¨æˆ·è¯¢é—®æ•°æ®ç›¸å…³é—®é¢˜ï¼Œè¯·å‘Šè¯‰ä»–ä»¬éœ€è¦å…ˆåœ¨"æ•°æ®æºç®¡ç†"é¡µé¢æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚

ä¸è¦å‡è®¾æˆ–çŒœæµ‹æ•°æ®åº“ç»“æ„ï¼Œä¸è¦ç”Ÿæˆä»»ä½•SQLæŸ¥è¯¢ã€‚"""


def _parse_sql_error(error_message: str) -> Dict[str, str]:
    """
    è§£æSQLé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯

    Args:
        error_message: å®Œæ•´çš„é”™è¯¯ä¿¡æ¯

    Returns:
        åŒ…å«main_error, hint, suggestionçš„å­—å…¸
    """
    result = {
        'main_error': error_message,
        'hint': None,
        'suggestion': None
    }

    try:
        # æå–ä¸»è¦é”™è¯¯ä¿¡æ¯
        lines = error_message.split('\n')

        # é¦–å…ˆå°è¯•ä»å®Œæ•´é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ—/è¡¨ä¸å­˜åœ¨çš„é”™è¯¯
        column_match = re.search(r'column "([^"]+)" does not exist', error_message, re.IGNORECASE)
        table_match = re.search(r'table "([^"]+)" does not exist', error_message, re.IGNORECASE)
        relation_match = re.search(r'relation "([^"]+)" does not exist', error_message, re.IGNORECASE)

        if column_match:
            wrong_column = column_match.group(1)
            result['main_error'] = f'column "{wrong_column}" does not exist'
        elif table_match:
            wrong_table = table_match.group(1)
            result['main_error'] = f'table "{wrong_table}" does not exist'
        elif relation_match:
            wrong_relation = relation_match.group(1)
            result['main_error'] = f'relation "{wrong_relation}" does not exist'
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”™è¯¯ï¼Œå°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
            for line in lines:
                if 'psycopg2.errors' in line:
                    # æå–æ‹¬å·åçš„å†…å®¹
                    match = re.search(r'\)\s*(.+?)(?:\n|$)', line)
                    if match:
                        result['main_error'] = match.group(1).strip()
                        break
                elif line.strip() and not line.startswith('LINE') and not line.startswith('[SQL') and not line.startswith('HINT'):
                    result['main_error'] = line.strip()
                    break

        # æå–HINTä¿¡æ¯
        hint_match = re.search(r'HINT:\s*(.+?)(?:\n|$)', error_message, re.IGNORECASE)
        if hint_match:
            result['hint'] = hint_match.group(1).strip()

            # æ ¹æ®HINTç”Ÿæˆå»ºè®®
            if 'Perhaps you meant to reference the column' in result['hint']:
                # æå–å»ºè®®çš„åˆ—å
                column_match = re.search(r'column "([^"]+)"', result['hint'])
                if column_match:
                    suggested_column = column_match.group(1)
                    # æå–ç®€å•åˆ—åï¼ˆå»æ‰è¡¨åå‰ç¼€ï¼‰
                    simple_column = suggested_column.split('.')[-1] if '.' in suggested_column else suggested_column
                    result['suggestion'] = f"è¯·ä½¿ç”¨åˆ—å `{simple_column}` è€Œä¸æ˜¯é”™è¯¯çš„åˆ—åã€‚"

        # å¦‚æœæ˜¯åˆ—ä¸å­˜åœ¨é”™è¯¯ä½†æ²¡æœ‰HINTå»ºè®®
        if column_match and not result['suggestion']:
            wrong_column = column_match.group(1)
            result['suggestion'] = f"åˆ— `{wrong_column}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…åˆ—åã€‚"

        # å¦‚æœæ˜¯è¡¨ä¸å­˜åœ¨é”™è¯¯
        if table_match and not result['suggestion']:
            wrong_table = table_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_table}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

        # å¦‚æœæ˜¯relationä¸å­˜åœ¨é”™è¯¯ï¼ˆPostgreSQLçš„è¡¨ä¸å­˜åœ¨é”™è¯¯ï¼‰
        if relation_match and not result['suggestion']:
            wrong_relation = relation_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_relation}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

    except Exception as e:
        logger.warning(f"è§£æSQLé”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

    return result


async def _fix_sql_with_ai(
    original_sql: str,
    error_message: str,
    schema_context: str,
    original_question: str
) -> Optional[str]:
    """
    ä½¿ç”¨AIä¿®å¤å¤±è´¥çš„SQLæŸ¥è¯¢

    Args:
        original_sql: åŸå§‹SQLæŸ¥è¯¢
        error_message: é”™è¯¯ä¿¡æ¯
        schema_context: æ•°æ®åº“schemaä¸Šä¸‹æ–‡
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜

    Returns:
        ä¿®å¤åçš„SQLï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
    """
    try:
        # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
        error_details = _parse_sql_error(error_message)

        # æ„å»ºæ›´ç²¾ç¡®çš„ä¿®å¤æç¤º
        fix_prompt = f"""ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚ç”¨æˆ·çš„æŸ¥è¯¢æ‰§è¡Œå¤±è´¥äº†ï¼Œè¯·å¸®åŠ©ä¿®å¤SQLè¯­å¥ã€‚

# ç”¨æˆ·åŸå§‹é—®é¢˜
{original_question}

# å¤±è´¥çš„SQLæŸ¥è¯¢
```sql
{original_sql}
```

# é”™è¯¯ä¿¡æ¯
{error_details['main_error']}

# PostgreSQLæ•°æ®åº“æç¤º
{error_details.get('hint', 'æ— ')}

# ğŸ”´ğŸ”´ğŸ”´ æ•°æ®åº“Schemaä¿¡æ¯ï¼ˆå¿…é¡»ä½¿ç”¨è¿™é‡Œçš„å®é™…è¡¨åå’Œåˆ—åï¼‰
{schema_context}

# ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®å¤è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

## ç¬¬1æ­¥ï¼šç†è§£é”™è¯¯
- **ä¸»è¦é”™è¯¯**: {error_details['main_error']}
- **æ•°æ®åº“æç¤º**: {error_details.get('hint', 'æ— æç¤º')}

## ç¬¬2æ­¥ï¼šæŸ¥æ‰¾æ­£ç¡®çš„è¡¨å/åˆ—å
**ğŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šSQLä¸­ä½¿ç”¨äº†ä¸å­˜åœ¨çš„è¡¨åæˆ–åˆ—åï¼**

1. **å¦‚æœé”™è¯¯æ˜¯"Table does not exist"**ï¼š
   - è¿™é€šå¸¸æ„å‘³ç€SQLä¸­ä½¿ç”¨äº†é”™è¯¯çš„è¡¨åï¼ˆå¯èƒ½æ˜¯ç”¨æˆ·æƒ³è±¡çš„ä¸­æ–‡åï¼‰
   - å¿…é¡»ä»ä¸Šé¢çš„Schemaä¿¡æ¯ä¸­æ‰¾åˆ°å®é™…å­˜åœ¨çš„è¡¨å
   - ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"å®¢æˆ·"ï¼Œä½†Schemaä¸­å®é™…çš„è¡¨å¯èƒ½å« `customers`
   - ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"è®¢å•"ï¼Œä½†Schemaä¸­å®é™…çš„è¡¨å¯èƒ½å« `orders`

2. **å¦‚æœé”™è¯¯æ˜¯"Column does not exist"**ï¼š
   - æŸ¥çœ‹PostgreSQLçš„HINTæç¤º
   - åœ¨Schemaä¸­æ‰¾åˆ°æ­£ç¡®çš„åˆ—å

3. **å¸¸è§é”™è¯¯æ¨¡å¼ï¼ˆä¸­æ–‡è¡¨åâ†’è‹±æ–‡å®é™…è¡¨åï¼‰ï¼š**
   - âŒ `FROM å®¢æˆ·` â†’ âœ… `FROM customers`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM è®¢å•` â†’ âœ… `FROM orders`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM äº§å“` â†’ âœ… `FROM products`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM å‘˜å·¥` â†’ âœ… `FROM employees`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰

## ç¬¬3æ­¥ï¼šä¿®å¤SQL
1. ä»”ç»†é˜…è¯»ä¸Šé¢çš„Schemaä¿¡æ¯ï¼Œæ‰¾åˆ°å¯¹åº”çš„**å®é™…è¡¨åå’Œåˆ—å**
2. å°†SQLä¸­é”™è¯¯çš„è¡¨å/åˆ—åæ›¿æ¢ä¸ºSchemaä¸­çš„å®é™…åç§°
3. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®
4. åªä½¿ç”¨SELECTæŸ¥è¯¢
5. ğŸ”´ æå€¼æŸ¥è¯¢å¿…é¡»ä½¿ç”¨ LIMIT 1ï¼šå¦‚æœåŸå§‹é—®é¢˜æ¶‰åŠ"æœ€å¤§"ã€"æœ€å°"ã€"æœ€é•¿"ã€"æœ€çŸ­"ç­‰æå€¼ï¼Œç¡®ä¿SQLä½¿ç”¨ ORDER BY + LIMIT 1

## ç¬¬4æ­¥ï¼šè¿”å›ç»“æœ
- **åªè¿”å›ä¿®å¤åçš„SQLè¯­å¥** - ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–markdownæ ‡è®°
- **å¦‚æœSchemaä¸­æ²¡æœ‰ç›¸å…³çš„è¡¨æˆ–åˆ—** - è¿”å›"CANNOT_FIX"
- **ä¸è¦æ·»åŠ ```sqlæ ‡è®°** - ç›´æ¥è¿”å›çº¯SQLè¯­å¥

# ä¿®å¤ç¤ºä¾‹

**é”™è¯¯SQL:**
```sql
SELECT å®¢æˆ·.name, SUM(è®¢å•.total_amount) FROM å®¢æˆ· JOIN è®¢å• ON å®¢æˆ·.id = è®¢å•.customer_id
```

**é”™è¯¯ä¿¡æ¯:**
Table with name å®¢æˆ· does not exist

**Schemaä¿¡æ¯ä¸­æ˜¾ç¤ºå®é™…è¡¨åæ˜¯ï¼šcustomers, orders**

**ä¿®å¤åçš„SQL:**
SELECT customers.name, SUM(orders.total_amount) as total_spent FROM customers JOIN orders ON customers.id = orders.customer_id GROUP BY customers.name

---

ç°åœ¨è¯·ä¿®å¤ä¸Šè¿°å¤±è´¥çš„SQLæŸ¥è¯¢ï¼Œç›´æ¥è¿”å›ä¿®å¤åçš„SQLè¯­å¥ï¼š"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¿®å¤ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®é”™è¯¯ä¿¡æ¯å’Œschemaä¿®å¤SQLæŸ¥è¯¢ã€‚"},
            {"role": "user", "content": fix_prompt}
        ]

        # è°ƒç”¨æ™ºè°±AIä¿®å¤SQLï¼ˆè·³è¿‡å®‰å…¨æ£€æŸ¥ï¼Œå› ä¸ºè¿™æ˜¯å†…éƒ¨è°ƒç”¨ï¼‰
        response = await zhipu_service.chat_completion(
            messages=messages,
            max_tokens=1000,
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            stream=False,
            skip_security_check=True  # å†…éƒ¨SQLä¿®å¤è°ƒç”¨ï¼Œè·³è¿‡å®‰å…¨æ£€æŸ¥
        )

        if response and response.get("content"):
            fixed_sql = response["content"].strip()

            # æ¸…ç†è¿”å›çš„SQL
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            fixed_sql = re.sub(r'```sql\s*', '', fixed_sql)
            fixed_sql = re.sub(r'```\s*', '', fixed_sql)
            fixed_sql = fixed_sql.strip()

            # æ£€æŸ¥æ˜¯å¦æ— æ³•ä¿®å¤
            if "CANNOT_FIX" in fixed_sql.upper():
                logger.warning("AIè¡¨ç¤ºæ— æ³•ä¿®å¤æ­¤SQL")
                return None

            # éªŒè¯æ˜¯å¦æ˜¯SELECTæŸ¥è¯¢
            if not fixed_sql.upper().startswith('SELECT'):
                logger.warning("ä¿®å¤åçš„SQLä¸æ˜¯SELECTæŸ¥è¯¢")
                return None

            logger.info(f"AIæˆåŠŸä¿®å¤SQL: {fixed_sql[:100]}...")
            return fixed_sql

        return None

    except Exception as e:
        logger.error(f"AIä¿®å¤SQLå¤±è´¥: {e}")
        return None


async def _execute_sql_on_file_datasource(
    connection_string: str,
    db_type: str,
    sql_query: str,
    data_source_name: str
) -> Dict[str, Any]:
    """
    åœ¨æ–‡ä»¶ç±»å‹æ•°æ®æºä¸Šæ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨duckdbï¼Œæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsï¼‰
        sql_query: SQLæŸ¥è¯¢è¯­å¥
        data_source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºè¡¨åï¼‰

    Returns:
        æŸ¥è¯¢ç»“æœå­—å…¸
    """
    try:
        # è§£æå­˜å‚¨è·¯å¾„
        if connection_string.startswith("file://"):
            storage_path = connection_string[7:]
        else:
            storage_path = connection_string

        file_data = None
        file_path = None

        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œåªæœ‰æœ¬åœ°ä¸å­˜åœ¨æ—¶æ‰ä» MinIO ä¸‹è½½
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆé€šå¸¸åœ¨ /app/uploads/ ç›®å½•ä¸‹ï¼‰
        if storage_path.startswith("/app/uploads/") or storage_path.startswith("/app/data/"):
            file_path = storage_path
            if os.path.exists(file_path):
                logger.info(f"ç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: {file_path}")
                try:
                    with open(file_path, 'rb') as f:
                        file_data = f.read()
                except Exception as e:
                    logger.warning(f"è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä» MinIO ä¸‹è½½")
                    file_data = None
            else:
                logger.info(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œå°è¯•ä» MinIO ä¸‹è½½")
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œä» MinIO ä¸‹è½½
        if not file_data:
            # ä»è·¯å¾„ä¸­æå–æ­£ç¡®çš„ object_nameï¼ˆå»æ‰ /app/uploads/ å‰ç¼€ï¼‰
            if storage_path.startswith("/app/uploads/"):
                # æå–ç›¸å¯¹äº uploads çš„è·¯å¾„ä½œä¸º object_name
                object_name = storage_path.replace("/app/uploads/", "", 1)
            elif storage_path.startswith("/app/data/"):
                # æå–ç›¸å¯¹äº data çš„è·¯å¾„ä½œä¸º object_name
                object_name = storage_path.replace("/app/data/", "", 1)
            else:
                # å¦‚æœè·¯å¾„ä¸åŒ…å« /app/uploads/ æˆ– /app/data/ï¼Œç›´æ¥ä½¿ç”¨åŸè·¯å¾„
                object_name = storage_path.lstrip("/")
            
            logger.info(f"ä»MinIOä¸‹è½½æ–‡ä»¶ç”¨äºSQLæ‰§è¡Œ: bucket=data-sources, object_name={object_name}")
            try:
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=object_name
                )
            except Exception as e:
                logger.error(f"ä»MinIOä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                file_data = None

        if not file_data:
            return {
                "success": False,
                "error": f"æ— æ³•è·å–æ–‡ä»¶: {storage_path} (æœ¬åœ°è·¯å¾„: {file_path if file_path else 'N/A'})",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        # åˆ›å»ºduckdbè¿æ¥
        conn = duckdb.connect(':memory:')
        registered_tables = []

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheetå¹¶æ³¨å†Œä¸ºä¸åŒçš„è¡¨
            try:
                # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"System Error: Missing dependency 'openpyxl'. Please install it: pip install openpyxl. Original error: {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            except Exception as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"Execution Error: Failed to read Excel file. {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            
            sheet_names = excel_file.sheet_names
            logger.info(f"ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨åï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
                    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œå› ä¸ºDuckDBæ”¯æŒä¸­æ–‡è¡¨å
                    clean_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', sheet_name)
                    if not clean_table_name or clean_table_name[0].isdigit():
                        clean_table_name = f"sheet_{clean_table_name}"

                    conn.register(clean_table_name, df)
                    registered_tables.append(clean_table_name)
                    logger.info(f"æ³¨å†Œè¡¨ '{clean_table_name}' (æ¥è‡ªSheet '{sheet_name}'): {len(df)}è¡Œ")

                    # åŒæ—¶ç”¨åŸå§‹Sheetåæ³¨å†Œï¼ˆå¦‚æœä¸åŒï¼‰
                    if sheet_name != clean_table_name:
                        try:
                            conn.register(sheet_name, df)
                            registered_tables.append(sheet_name)
                        except Exception:
                            pass  # å¦‚æœåŸåæ³¨å†Œå¤±è´¥ï¼Œå¿½ç•¥

                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

            # å¦‚æœç¬¬ä¸€ä¸ªSheetå­˜åœ¨ï¼Œä¹Ÿç”¨æ•°æ®æºåç§°æ³¨å†Œï¼ˆå‘åå…¼å®¹ï¼‰
            if sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    first_df = pd.read_excel(excel_file, sheet_name=0, engine='openpyxl')
                    if not first_df.empty:
                        ds_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                        if ds_table_name not in registered_tables:
                            conn.register(ds_table_name, first_df)
                            registered_tables.append(ds_table_name)
                except Exception as e:
                    logger.warning(f"æ³¨å†Œæ•°æ®æºåç§°è¡¨å¤±è´¥: {e}")
                    pass

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                try:
                    df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if df is not None:
                table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                if not table_name or table_name[0].isdigit():
                    table_name = f"data_{table_name}"
                conn.register(table_name, df)
                registered_tables.append(table_name)
                # åŒæ—¶æ³¨å†Œä¸º 'data' ä½œä¸ºå¤‡ç”¨
                conn.register('data', df)

        if not registered_tables:
            conn.close()
            return {
                "success": False,
                "error": f"æ— æ³•ä»æ–‡ä»¶è¯»å–ä»»ä½•æ•°æ®: {storage_path}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        logger.info(f"æˆåŠŸæ³¨å†Œ {len(registered_tables)} ä¸ªè¡¨: {registered_tables}")

        # æ‰§è¡ŒSQLæŸ¥è¯¢
        try:
            result_df = conn.execute(sql_query).fetchdf()
        except Exception as sql_error:
            error_msg = str(sql_error)
            logger.warning(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«å¯ç”¨çš„è¡¨å
            conn.close()
            return {
                "success": False,
                "error": f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}\n\nå¯ç”¨çš„è¡¨: {', '.join(registered_tables)}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        conn.close()

        # è½¬æ¢ç»“æœä¸ºå­—å…¸åˆ—è¡¨
        columns = list(result_df.columns)
        data = result_df.to_dict('records')

        logger.info(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(data)} è¡Œ")

        return {
            "success": True,
            "data": data,
            "columns": columns,
            "row_count": len(data)
        }

    except Exception as e:
        logger.error(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡Œå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": [],
            "columns": [],
            "row_count": 0
        }


async def _execute_sql_if_needed(
    content: str,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None
) -> str:
    """
    æ£€æµ‹AIå›å¤ä¸­çš„SQLæŸ¥è¯¢å¹¶æ‰§è¡Œï¼Œå°†ç»“æœæ’å…¥å›å¤ä¸­ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰

    Args:
        content: AIç”Ÿæˆçš„å›å¤å†…å®¹
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº

    Returns:
        å¢å¼ºåçš„å›å¤å†…å®¹ï¼ˆåŒ…å«æŸ¥è¯¢ç»“æœï¼‰
    """
    try:
        # æ£€æµ‹SQLä»£ç å—
        sql_pattern = r'```sql\s*(.*?)\s*```'
        sql_matches = re.findall(sql_pattern, content, re.DOTALL | re.IGNORECASE)

        if not sql_matches:
            return content

        # å»é‡ï¼šå¦‚æœæœ‰å¤šä¸ªç›¸åŒçš„SQLï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
        seen_sqls = set()
        unique_sql_matches = []
        for sql in sql_matches:
            normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
            if normalized_sql not in seen_sqls:
                seen_sqls.add(normalized_sql)
                unique_sql_matches.append(sql)
            else:
                logger.warning(f"æ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

        sql_matches = unique_sql_matches
        logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

        # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )

        if not data_sources:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
            return content + "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚"

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
        if data_source_ids:
            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            if matching_sources:
                data_source = matching_sources[0]
                logger.info(f"ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
            else:
                data_source = data_sources[0]
                logger.warning(f"æœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
        else:
            data_source = data_sources[0]

        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
        connection_string = await data_source_service.get_decrypted_connection_string(
            data_source_id=data_source.id,
            tenant_id=tenant_id,
            db=db
        )

        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢
        enhanced_content = content
        for sql_query in sql_matches:
            current_sql = sql_query.strip()
            retry_count = 0
            max_retries = 2
            last_error = None
            execution_success = False

            while retry_count <= max_retries and not execution_success:
                try:
                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢ï¼ˆåŒ…æ‹¬WITH...SELECTçš„CTEæŸ¥è¯¢ï¼‰
                    sql_upper = current_sql.upper().strip()
                    if not (sql_upper.startswith('SELECT') or sql_upper.startswith('WITH')):
                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                        break

                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                        logger.info(f"ä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                        result = await _execute_sql_on_file_datasource(
                            connection_string=connection_string,
                            db_type=data_source.db_type,
                            sql_query=current_sql,
                            data_source_name=data_source.name
                        )
                        if not result.get("success", False) and result.get("error"):
                            raise Exception(result["error"])
                    else:
                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                        adapter = PostgreSQLAdapter(connection_string)
                        try:
                            await adapter.connect()
                            query_result = await adapter.execute_query(current_sql)
                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            result = {
                                "data": query_result.data,
                                "columns": query_result.columns,
                                "row_count": query_result.row_count
                            }
                        finally:
                            await adapter.disconnect()

                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                    row_count = len(result.get("data", []))

                    if result.get("data") and row_count > 0:
                        # è·å–åˆ—åï¼šä¼˜å…ˆä½¿ç”¨columnså­—æ®µï¼Œå¦åˆ™ä»æ•°æ®ä¸­æå–
                        columns = result.get("columns") or list(result["data"][0].keys())
                        # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                        result_text = "\n\n| " + " | ".join(columns) + " |\n"
                        result_text += "|" + "|".join(["---" for _ in columns]) + "|\n"

                        for row in result["data"][:10]:
                            row_values = [str(row.get(col, "")) for col in columns]
                            result_text += "| " + " | ".join(row_values) + " |\n"

                        # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                        if row_count > 10:
                            result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                    else:
                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ›¿æ¢ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                    if retry_count > 0:
                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                        # å®Œå…¨æ›¿æ¢åŸå§‹SQLå—ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                        sql_block = f"```sql\n{sql_query}\n```"
                        fixed_sql_block = f"**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            fixed_sql_block + result_text
                        )
                    else:
                        # æ²¡æœ‰é‡è¯•ï¼Œç›´æ¥å°†ç»“æœæ’å…¥åˆ°SQLä»£ç å—åé¢
                        sql_block = f"```sql\n{sql_query}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            sql_block + result_text
                        )

                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result.get('data', []))} è¡Œ")
                    execution_success = True

                except Exception as e:
                    last_error = str(e)
                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                    if retry_count < max_retries:
                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                        fixed_sql = await _fix_sql_with_ai(
                            original_sql=current_sql,
                            error_message=last_error,
                            schema_context=schema_context,
                            original_question=original_question
                        )

                        if fixed_sql:
                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                            current_sql = fixed_sql
                            retry_count += 1
                        else:
                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                            break
                    else:
                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                        break

            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if not execution_success and last_error:
                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                error_details = _parse_sql_error(last_error)

                # æ„å»ºé”™è¯¯ä¿¡æ¯
                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                if error_details.get('hint'):
                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                if retry_count > 0:
                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                # æ·»åŠ å»ºè®®
                if error_details.get('suggestion'):
                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                else:
                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                # æ›¿æ¢åŸå§‹SQLå—
                sql_block = f"```sql\n{sql_query}\n```"
                if retry_count > 0:
                    # å¦‚æœç»è¿‡é‡è¯•ï¼Œå®Œå…¨æ›¿æ¢ä¸ºé”™è¯¯ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºåŸå§‹SQLï¼‰
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        f"**âš ï¸ åŸå§‹SQLæœ‰è¯¯ï¼Œå°è¯•ä¿®å¤åä»ç„¶å¤±è´¥ï¼š**\n{error_text}"
                    )
                else:
                    # æ²¡æœ‰é‡è¯•ï¼Œåœ¨åŸå§‹SQLåæ·»åŠ é”™è¯¯ä¿¡æ¯
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        sql_block + error_text
                    )

        return enhanced_content

    except Exception as e:
        logger.error(f"SQLæ‰§è¡Œå¤„ç†å¤±è´¥: {e}")
        return content


def _convert_response(response: LLMResponse) -> ChatCompletionResponse:
    """è½¬æ¢å“åº”æ ¼å¼"""
    return ChatCompletionResponse(
        content=response.content,
        thinking=response.thinking,
        usage=response.usage,
        model=response.model,
        provider=response.provider,
        finish_reason=response.finish_reason,
        created_at=response.created_at
    )


async def _execute_tool_call(
    tool_call: Dict[str, Any],
    tenant_id: str,
    db: Session,
    data_source_ids: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆç›®å‰åªæ”¯æŒ execute_sqlï¼‰
    
    Returns:
        Dict with keys: success, result, error
    """
    try:
        tool_name = tool_call.get("function", {}).get("name", "")
        tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
        
        # è§£æå·¥å…·å‚æ•°
        try:
            tool_args = json.loads(tool_args_str)
        except json.JSONDecodeError:
            return {
                "success": False,
                "result": None,
                "error": f"æ— æ³•è§£æå·¥å…·å‚æ•°: {tool_args_str}"
            }
        
        if tool_name == "execute_sql":
            sql_query = tool_args.get("sql_query", "")
            if not sql_query:
                return {
                    "success": False,
                    "result": None,
                    "error": "SQLæŸ¥è¯¢è¯­å¥ä¸ºç©º"
                }
            
            # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢
            if not sql_query.strip().upper().startswith('SELECT'):
                return {
                    "success": False,
                    "result": None,
                    "error": "åªå…è®¸æ‰§è¡Œ SELECT æŸ¥è¯¢ï¼Œç¦æ­¢æ‰§è¡Œä¿®æ”¹æ“ä½œ"
                }
            
            # è·å–æ•°æ®æº
            data_sources = await data_source_service.get_data_sources(
                tenant_id=tenant_id,
                db=db,
                active_only=True
            )
            
            if not data_sources:
                return {
                    "success": False,
                    "result": None,
                    "error": "æœªæ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æº"
                }
            
            # é€‰æ‹©æ•°æ®æº
            if data_source_ids:
                matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
                if matching_sources:
                    data_source = matching_sources[0]
                else:
                    data_source = data_sources[0]
            else:
                data_source = data_sources[0]
            
            # è·å–è¿æ¥å­—ç¬¦ä¸²
            connection_string = await data_source_service.get_decrypted_connection_string(
                data_source_id=data_source.id,
                tenant_id=tenant_id,
                db=db
            )
            
            # æ‰§è¡ŒSQL
            try:
                if data_source.db_type in ["xlsx", "xls", "csv"]:
                    # æ–‡ä»¶ç±»å‹æ•°æ®æº
                    result = await _execute_sql_on_file_datasource(
                        connection_string=connection_string,
                        db_type=data_source.db_type,
                        sql_query=sql_query,
                        data_source_name=data_source.name
                    )
                    if not result.get("success", False):
                        return {
                            "success": False,
                            "result": None,
                            "error": result.get("error", "æ‰§è¡Œå¤±è´¥")
                        }
                else:
                    # æ•°æ®åº“ç±»å‹æ•°æ®æº
                    adapter = PostgreSQLAdapter(connection_string)
                    try:
                        await adapter.connect()
                        query_result = await adapter.execute_query(sql_query)
                        result = {
                            "data": query_result.data,
                            "columns": query_result.columns,
                            "row_count": query_result.row_count
                        }
                    finally:
                        await adapter.disconnect()
                
                # æ ¼å¼åŒ–ç»“æœä¸ºJSONå­—ç¬¦ä¸²
                result_json = json.dumps(result, ensure_ascii=False, default=str)
                return {
                    "success": True,
                    "result": result_json,
                    "error": None
                }
            except Exception as e:
                logger.error(f"æ‰§è¡ŒSQLå¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "result": None,
                    "error": str(e)
                }
        
        elif tool_name == "generate_chart":
            # å¤„ç†å›¾è¡¨ç”Ÿæˆå·¥å…·è°ƒç”¨
            try:
                chart_type = tool_args.get("chart_type", "bar")
                title = tool_args.get("title", "æ•°æ®å›¾è¡¨")
                x_data = tool_args.get("x_data", [])
                y_data = tool_args.get("y_data", [])
                series_name = tool_args.get("series_name", "æ•°æ®")
                
                logger.info(f"ç”Ÿæˆå›¾è¡¨: type={chart_type}, title={title}, x_data_len={len(x_data)}, y_data_len={len(y_data)}")
                
                # æ ¹æ®å›¾è¡¨ç±»å‹ç”Ÿæˆ ECharts é…ç½®
                if chart_type == "pie":
                    # é¥¼å›¾éœ€è¦ç‰¹æ®Šçš„æ•°æ®æ ¼å¼
                    pie_data = [{"name": x, "value": y} for x, y in zip(x_data, y_data)]
                    echarts_option = {
                        "title": {"text": title, "left": "center"},
                        "tooltip": {"trigger": "item", "formatter": "{b}: {c} ({d}%)"},
                        "legend": {"orient": "vertical", "left": "left"},
                        "series": [{
                            "type": "pie",
                            "radius": "50%",
                            "data": pie_data,
                            "emphasis": {
                                "itemStyle": {
                                    "shadowBlur": 10,
                                    "shadowOffsetX": 0,
                                    "shadowColor": "rgba(0, 0, 0, 0.5)"
                                }
                            }
                        }]
                    }
                else:
                    # æŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾
                    echarts_option = {
                        "title": {"text": title, "left": "center"},
                        "tooltip": {"trigger": "axis"},
                        "xAxis": {"type": "category", "data": x_data},
                        "yAxis": {"type": "value"},
                        "series": [{
                            "name": series_name,
                            "type": chart_type,
                            "data": y_data,
                            "smooth": True if chart_type == "line" else False
                        }]
                    }
                
                # è¿”å› ECharts é…ç½®
                result = {
                    "chart_generated": True,
                    "echarts_option": echarts_option
                }
                
                return {
                    "success": True,
                    "result": json.dumps(result, ensure_ascii=False),
                    "error": None,
                    "echarts_option": echarts_option  # é¢å¤–è¿”å›é…ç½®ï¼Œæ–¹ä¾¿ç›´æ¥ä½¿ç”¨
                }
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "result": None,
                    "error": f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}"
                }
        
        else:
            return {
                "success": False,
                "result": None,
                "error": f"æœªçŸ¥çš„å·¥å…·: {tool_name}"
            }
    except Exception as e:
        logger.error(f"æ‰§è¡Œå·¥å…·è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "result": None,
            "error": str(e)
        }


async def _stream_response_generator(
    stream_generator,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None,
    initial_messages: Optional[List[LLMMessage]] = None
):
    """
    æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆæ–¹æ¡ˆBï¼šSQL ä»£ç å—æ£€æµ‹æ¨¡å¼ï¼‰
    
    ä¸ä½¿ç”¨ Function Callingï¼Œè€Œæ˜¯æ£€æµ‹ AI è¾“å‡ºä¸­çš„ ```sql ... ``` ä»£ç å—ï¼Œ
    è‡ªåŠ¨æ‰§è¡Œ SQL æŸ¥è¯¢å¹¶è¿›è¡Œç¬¬äºŒæ¬¡ LLM è°ƒç”¨ã€‚
    """
    try:
        # æ”¶é›†å®Œæ•´çš„å“åº”å†…å®¹
        full_content = ""
        thinking_content = ""
        
        # æ¶ˆæ¯å†å²ï¼ˆç”¨äºäºŒæ¬¡è°ƒç”¨ï¼‰
        messages = initial_messages or []

        async for chunk in stream_generator:
            # å¤„ç†æ™®é€šå†…å®¹
            if chunk.type == "content":
                # å‘é€åŸå§‹chunk
                chunk_data = {
                    "type": chunk.type,
                    "content": chunk.content,
                    "provider": chunk.provider,
                    "finished": chunk.finished,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                full_content += chunk.content
            
            elif chunk.type == "thinking":
                thinking_content += chunk.content
                # å‘é€thinking chunk
                chunk_data = {
                    "type": chunk.type,
                    "content": chunk.content,
                    "provider": chunk.provider,
                    "finished": chunk.finished,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            
            # å¦‚æœæµç»“æŸï¼Œæ£€æµ‹å¹¶æ‰§è¡Œ SQLï¼ˆæ–¹æ¡ˆBï¼‰
            if chunk.finished:
                logger.info(f"æµå¼å“åº”å®Œæˆï¼Œæ£€æµ‹SQLæŸ¥è¯¢ã€‚å†…å®¹é•¿åº¦: {len(full_content)}")

                # æ£€æµ‹SQLä»£ç å—
                sql_pattern = r'```sql\s*(.*?)\s*```'
                sql_matches = re.findall(sql_pattern, full_content, re.DOTALL | re.IGNORECASE)

                if sql_matches:
                    # å»é‡ï¼šå¦‚æœæœ‰å¤šä¸ªç›¸åŒçš„SQLï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
                    seen_sqls = set()
                    unique_sql_matches = []
                    for sql in sql_matches:
                        normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
                        if normalized_sql not in seen_sqls:
                            seen_sqls.add(normalized_sql)
                            unique_sql_matches.append(sql)
                        else:
                            logger.warning(f"æµå¼å“åº”ï¼šæ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

                    sql_matches = unique_sql_matches
                    logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

                    # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
                    data_sources = await data_source_service.get_data_sources(
                        tenant_id=tenant_id,
                        db=db,
                        active_only=True
                    )

                    if data_sources:
                        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                        if data_source_ids:
                            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
                            if matching_sources:
                                data_source = matching_sources[0]
                                logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
                            else:
                                data_source = data_sources[0]
                                logger.warning(f"æµå¼å“åº”ï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
                        else:
                            data_source = data_sources[0]

                        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
                        connection_string = await data_source_service.get_decrypted_connection_string(
                            data_source_id=data_source.id,
                            tenant_id=tenant_id,
                            db=db
                        )

                        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
                        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

                        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
                        for sql_query in sql_matches:
                            current_sql = sql_query.strip()
                            retry_count = 0
                            max_retries = 2
                            last_error = None
                            execution_success = False

                            while retry_count <= max_retries and not execution_success:
                                try:
                                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢ï¼ˆåŒ…æ‹¬WITH...SELECTçš„CTEæŸ¥è¯¢ï¼‰
                                    sql_upper = current_sql.upper().strip()
                                    if not (sql_upper.startswith('SELECT') or sql_upper.startswith('WITH')):
                                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                                        break

                                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                                        logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                                        result = await _execute_sql_on_file_datasource(
                                            connection_string=connection_string,
                                            db_type=data_source.db_type,
                                            sql_query=current_sql,
                                            data_source_name=data_source.name
                                        )
                                        if not result.get("success", False) and result.get("error"):
                                            raise Exception(result["error"])
                                    else:
                                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                                        adapter = PostgreSQLAdapter(connection_string)
                                        try:
                                            await adapter.connect()
                                            query_result = await adapter.execute_query(current_sql)
                                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                                            result = {
                                                "data": query_result.data,
                                                "columns": query_result.columns,
                                                "row_count": query_result.row_count
                                            }
                                        finally:
                                            await adapter.disconnect()

                                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                                    row_count = result.get('row_count', 0)

                                    if result.get('data'):
                                        data = result['data'][:10]
                                        if data:
                                            headers = list(data[0].keys())
                                            # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                                            result_text = "\n\n| " + " | ".join(headers) + " |\n"
                                            result_text += "|" + "|".join(["---"] * len(headers)) + "|\n"

                                            for row in data:
                                                values = [str(row.get(h, '')) for h in headers]
                                                result_text += "| " + " | ".join(values) + " |\n"

                                            # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                                            if row_count > 10:
                                                result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                                        else:
                                            result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"
                                    else:
                                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œå‘é€ä¿®å¤è¯´æ˜
                                    if retry_count > 0:
                                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                                        # å‘é€ä¿®å¤åçš„SQL
                                        fix_info = f"\n\n**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```\n"
                                        fix_chunk = {
                                            "type": "content",
                                            "content": fix_info,
                                            "provider": chunk.provider,
                                            "finished": False,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(fix_chunk, ensure_ascii=False)}\n\n"

                                    # å‘é€æŸ¥è¯¢ç»“æœä½œä¸ºæ–°çš„content chunk
                                    result_chunk = {
                                        "type": "content",
                                        "content": result_text,
                                        "provider": chunk.provider,
                                        "finished": False,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(result_chunk, ensure_ascii=False)}\n\n"

                                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {result.get('row_count', 0)} è¡Œ")
                                    execution_success = True
                                    
                                    # ğŸ”§ æ–¹æ¡ˆBå¢å¼ºï¼šäºŒæ¬¡LLMè°ƒç”¨ï¼Œåˆ†ææ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨
                                    if execution_success and result.get('data'):
                                        logger.info("å¼€å§‹äºŒæ¬¡LLMè°ƒç”¨ï¼šåˆ†ææ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨")
                                        
                                        # --- ğŸ§  æ•°æ®ç‰¹å¾åˆ†æä¸å†³ç­–æ³¨å…¥ ---
                                        data_for_analysis = result['data'][:20]  # æœ€å¤šå–20è¡Œç”¨äºåˆ†æ
                                        analysis_row_count = len(result['data'])
                                        
                                        # è·å–åˆ—ä¿¡æ¯
                                        columns = result.get('columns', [])
                                        if not columns and data_for_analysis:
                                            columns = list(data_for_analysis[0].keys())
                                        col_count = len(columns)
                                        
                                        # ç®€å•çš„ç±»å‹æ¨æ–­ï¼šæ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¶é—´å…³é”®è¯
                                        col_names_str = " ".join([str(c).lower() for c in columns])
                                        has_time_col = any(k in col_names_str for k in ['date', 'time', 'year', 'month', 'day', 'quarter', 'week', 'æ—¥æœŸ', 'æ—¶é—´', 'å¹´', 'æœˆ', 'æ—¥'])
                                        has_metric_col = col_count >= 2  # å‡è®¾é™¤äº†ç»´åº¦è¿˜æœ‰æŒ‡æ ‡
                                        
                                        analysis_directive = ""
                                        
                                        # è§„åˆ™ 1: å•è¡Œæ•°æ®æˆ–çº¯æ–‡æœ¬ -> ç¦æ­¢ç”»å›¾
                                        if analysis_row_count <= 1:
                                            analysis_directive = (
                                                "ğŸ›‘ **CONSTRAINT**: The result contains only 1 row.\n"
                                                "- **DO NOT** call `generate_chart`. Visualization is useless for a single number.\n"
                                                "- Focus on explaining the value directly."
                                            )
                                        elif not has_metric_col and analysis_row_count < 50:
                                            analysis_directive = (
                                                "ğŸ›‘ **CONSTRAINT**: This appears to be a text list without numerical metrics.\n"
                                                "- **DO NOT** call `generate_chart`.\n"
                                                "- Summarize the list content (e.g., total count, examples)."
                                            )
                                        # è§„åˆ™ 2: å¤§æ•°æ®é‡ -> å¼ºåˆ¶ Top N
                                        elif analysis_row_count > 20 and not has_time_col:
                                            analysis_directive = (
                                                f"âš ï¸ **CONSTRAINT**: The result has {analysis_row_count} rows, which is too many for a clean chart.\n"
                                                "- **ACTION**: Use `generate_chart` but ONLY include the **Top 10** data points in the `data` parameter.\n"
                                                "- In your text analysis, mention that you are showing the top performers."
                                            )
                                        # è§„åˆ™ 3: æ—¶é—´åºåˆ— -> å¼ºåˆ¶æŠ˜çº¿å›¾
                                        elif has_time_col and analysis_row_count > 1:
                                            analysis_directive = (
                                                "âœ… **STRATEGY**: This is time-series data.\n"
                                                "- **ACTION**: You MUST call `generate_chart` with `chart_type='line'`.\n"
                                                "- **Analysis**: Focus on the trend (upward/downward), seasonality, or spikes."
                                            )
                                        # è§„åˆ™ 4: åˆ†ç±»å¯¹æ¯” -> å»ºè®®æŸ±çŠ¶å›¾æˆ–é¥¼å›¾
                                        else:
                                            chart_suggestion = "pie" if analysis_row_count <= 8 else "bar"
                                            analysis_directive = (
                                                f"âœ… **STRATEGY**: This is categorical comparison data.\n"
                                                f"- **ACTION**: You SHOULD call `generate_chart` with `chart_type='{chart_suggestion}'`.\n"
                                                "- **Analysis**: Compare the magnitudes. Identify the leader and the laggard."
                                            )
                                        
                                        logger.info(f"æ•°æ®ç‰¹å¾åˆ†æ: rows={analysis_row_count}, cols={col_count}, has_time={has_time_col}, has_metric={has_metric_col}")
                                        
                                        # æ„å»ºåˆ†ææç¤ºï¼ˆåŒ…å«å†³ç­–æŒ‡ä»¤ï¼‰
                                        # å°† Decimal ç±»å‹è½¬æ¢ä¸º floatï¼Œé¿å… JSON åºåˆ—åŒ–å¤±è´¥
                                        serializable_data = _convert_decimal_to_float(data_for_analysis)
                                        data_json = json.dumps(serializable_data, ensure_ascii=False, indent=2)
                                        
                                        analysis_prompt = f"""ä½ åˆšåˆšæŸ¥è¯¢äº†æ•°æ®ï¼Œç»“æœå¦‚ä¸‹ï¼š

```json
{data_json}
```

--- ANALYSIS INSTRUCTIONS ---
{analysis_directive}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **æ•°æ®åˆ†æ**ï¼šç”¨ 2-3 å¥è¯åˆ†ææ•°æ®çš„å…³é”®æ´å¯Ÿï¼Œè§£é‡Šæ•°æ®çš„å•†ä¸šå«ä¹‰ï¼ˆä¸è¦åªé‡å¤æ•°å­—ï¼‰

2. **ç”Ÿæˆ ECharts å›¾è¡¨é…ç½®**ï¼ˆå¦‚æœä¸Šè¿°æŒ‡ä»¤å…è®¸ï¼‰ï¼š
   
   âš ï¸ **æ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
   - å¿…é¡»ä½¿ç”¨ `[CHART_START]` å¼€å§‹ï¼Œ`[CHART_END]` ç»“æŸ
   - ä¸­é—´æ˜¯**æ ‡å‡† ECharts JSON é…ç½®**ï¼ˆä¸æ˜¯è‡ªå®šä¹‰æ ¼å¼ï¼ï¼‰
   - ä¸è¦ä½¿ç”¨ markdown ä»£ç å—åŒ…è£¹ JSON
   
   âœ… **æ­£ç¡®ç¤ºä¾‹ï¼ˆæŸ±çŠ¶å›¾ï¼‰**ï¼š
[CHART_START]
{{"title":{{"text":"å•†å“åº“å­˜æ’å"}},"tooltip":{{"trigger":"axis"}},"xAxis":{{"type":"category","data":["åä¸ºMateBook","iPhone 15","å°ç±³ç”µè§†"]}},"yAxis":{{"type":"value","name":"åº“å­˜æ•°é‡"}},"series":[{{"name":"åº“å­˜","type":"bar","data":[100,80,50]}}]}}
[CHART_END]

   âœ… **æ­£ç¡®ç¤ºä¾‹ï¼ˆé¥¼å›¾ï¼‰**ï¼š
[CHART_START]
{{"title":{{"text":"é”€å”®å æ¯”"}},"tooltip":{{"trigger":"item"}},"series":[{{"name":"é”€å”®é¢","type":"pie","radius":"50%","data":[{{"value":1048,"name":"äº§å“A"}},{{"value":735,"name":"äº§å“B"}}]}}]}}
[CHART_END]

   âŒ **é”™è¯¯æ ¼å¼ï¼ˆä¸è¦è¿™æ ·å†™ï¼‰**ï¼š
   - {{"chartType": "bar", "xAxis": {{"field": "name"}}}} â† è¿™ä¸æ˜¯ ECharts æ ¼å¼ï¼

è¯·ç›´æ¥è¾“å‡ºåˆ†æå’Œå›¾è¡¨ï¼š"""

                                        # æ„å»ºä¸“å®¶æ•°æ®åˆ†æå¸ˆçš„ç³»ç»Ÿæç¤º
                                        expert_system_prompt = (
                                            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æ•°æ®ä¸­æå–æ´å¯Ÿå¹¶æœ‰æ•ˆåœ°å¯è§†åŒ–å®ƒä»¬ã€‚\n\n"
                                            "**æ ¸å¿ƒåè®®ï¼š**\n"
                                            "1. **éµå¾ªæŒ‡ä»¤**ï¼šç³»ç»Ÿä¼šåˆ†ææ•°æ®å½¢æ€å¹¶ç»™å‡ºå…·ä½“çº¦æŸï¼ˆå¦‚'ç¦æ­¢ç”»å›¾'æˆ–'åªç”» Top 10'ï¼‰ã€‚ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆã€‚\n"
                                            "2. **æ•°æ®åˆ†æ**ï¼šä¸è¦åªé‡å¤æ•°å­—ã€‚è§£é‡Šæ•°æ®çš„æ„ä¹‰ï¼ˆä¾‹å¦‚ï¼Œä¸è¦è¯´'Aæ˜¯100ï¼ŒBæ˜¯50'ï¼Œè€Œè¦è¯´'Açš„è¡¨ç°æ˜¯Bçš„2å€'ï¼‰ã€‚\n"
                                            "3. **å›¾è¡¨æ ¼å¼**ï¼šå½“éœ€è¦ç”Ÿæˆå›¾è¡¨æ—¶ï¼Œå¿…é¡»ä½¿ç”¨æ ‡å‡†çš„ ECharts JSON é…ç½®æ ¼å¼ï¼Œç”¨ [CHART_START] å’Œ [CHART_END] æ ‡è®°åŒ…è£¹ã€‚\n\n"
                                            "**é‡è¦æé†’ï¼š**\n"
                                            "- å›¾è¡¨é…ç½®å¿…é¡»æ˜¯æ ‡å‡† ECharts æ ¼å¼ï¼ŒåŒ…å« titleã€xAxisã€yAxisã€series ç­‰å­—æ®µ\n"
                                            "- ä¸è¦ä½¿ç”¨è‡ªå®šä¹‰çš„ç®€åŒ–æ ¼å¼å¦‚ {chartType: 'bar', xAxis: {field: 'name'}}\n"
                                            "- ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹"
                                        )
                                        
                                        # æ„å»ºæ¶ˆæ¯å†å²
                                        analysis_messages = [
                                            LLMMessage(role="system", content=expert_system_prompt),
                                            LLMMessage(role="user", content=original_question),
                                            LLMMessage(role="assistant", content=f"è®©æˆ‘æŸ¥è¯¢ç›¸å…³æ•°æ®ï¼š\n\n```sql\n{current_sql}\n```\n\n{result_text}"),
                                            LLMMessage(role="user", content=analysis_prompt)
                                        ]
                                        
                                        # è·å–providerå®ä¾‹
                                        provider_instance = llm_service.get_provider(tenant_id, LLMProvider.DEEPSEEK)
                                        if provider_instance:
                                            try:
                                                # å‘é€åˆ†æçŠ¶æ€
                                                analysis_status = {
                                                    "type": "content",
                                                    "content": "\n\nğŸ“Š **æ•°æ®åˆ†æä¸­...**\n\n",
                                                    "provider": chunk.provider,
                                                    "finished": False,
                                                    "tenant_id": tenant_id
                                                }
                                                yield f"data: {json.dumps(analysis_status, ensure_ascii=False)}\n\n"
                                                
                                                # äºŒæ¬¡è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
                                                analysis_stream = await provider_instance.chat_completion(
                                                    messages=analysis_messages,
                                                    model=None,
                                                    max_tokens=2000,
                                                    temperature=0.7,
                                                    stream=True,
                                                    tools=None
                                                )
                                                
                                                # æµå¼è¾“å‡ºåˆ†æç»“æœ
                                                analysis_content = ""
                                                async for analysis_chunk in analysis_stream:
                                                    if analysis_chunk.type == "content" and analysis_chunk.content:
                                                        analysis_content += analysis_chunk.content
                                                        analysis_data = {
                                                            "type": "content",
                                                            "content": analysis_chunk.content,
                                                            "provider": analysis_chunk.provider,
                                                            "finished": False,
                                                            "tenant_id": tenant_id
                                                        }
                                                        yield f"data: {json.dumps(analysis_data, ensure_ascii=False)}\n\n"
                                                
                                                logger.info(f"äºŒæ¬¡LLMè°ƒç”¨å®Œæˆï¼Œåˆ†æå†…å®¹é•¿åº¦: {len(analysis_content)}")
                                                
                                                # æ£€æµ‹å¹¶æå–å›¾è¡¨é…ç½®
                                                chart_pattern = r'\[CHART_START\](.*?)\[CHART_END\]'
                                                chart_match = re.search(chart_pattern, analysis_content, re.DOTALL)
                                                
                                                if chart_match:
                                                    try:
                                                        chart_json_str = chart_match.group(1).strip()
                                                        echarts_option = json.loads(chart_json_str)
                                                        logger.info(f"âœ… æˆåŠŸæå– ECharts é…ç½®: {list(echarts_option.keys())}")
                                                        
                                                        # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                                                        chart_event = {
                                                            "type": "chart_config",
                                                            "data": {"echarts_option": echarts_option},
                                                            "provider": "deepseek",
                                                            "finished": False,
                                                            "tenant_id": tenant_id
                                                        }
                                                        yield f"data: {json.dumps(chart_event, ensure_ascii=False)}\n\n"
                                                    except json.JSONDecodeError as e:
                                                        logger.warning(f"è§£æ ECharts JSON å¤±è´¥: {e}")
                                                
                                            except Exception as e:
                                                logger.error(f"äºŒæ¬¡LLMè°ƒç”¨å¤±è´¥: {e}")

                                except Exception as e:
                                    last_error = str(e)
                                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                                    if retry_count < max_retries:
                                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                                        fixed_sql = await _fix_sql_with_ai(
                                            original_sql=current_sql,
                                            error_message=last_error,
                                            schema_context=schema_context,
                                            original_question=original_question
                                        )

                                        if fixed_sql:
                                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                                            current_sql = fixed_sql
                                            retry_count += 1
                                        else:
                                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                                            break
                                    else:
                                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                                        break

                            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                            if not execution_success and last_error:
                                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                                error_details = _parse_sql_error(last_error)

                                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                                if error_details.get('hint'):
                                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                                if retry_count > 0:
                                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                                # æ·»åŠ å»ºè®®
                                if error_details.get('suggestion'):
                                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                                else:
                                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                                error_chunk = {
                                    "type": "content",
                                    "content": error_text,
                                    "provider": chunk.provider,
                                    "finished": False,
                                    "tenant_id": tenant_id
                                }
                                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                    else:
                        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
                        warning_text = "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚\n"

                        warning_chunk = {
                            "type": "content",
                            "content": warning_text,
                            "provider": chunk.provider,
                            "finished": False,
                            "tenant_id": tenant_id
                        }
                        yield f"data: {json.dumps(warning_chunk, ensure_ascii=False)}\n\n"

        # ğŸ”§ æ¢å¤å›¾è¡¨ç”ŸæˆåŠŸèƒ½ï¼šæ£€æµ‹å¹¶æå– [CHART_START]...[CHART_END] æ ‡è®°ä¸­çš„ ECharts é…ç½®
        chart_pattern = r'\[CHART_START\](.*?)\[CHART_END\]'
        chart_match = re.search(chart_pattern, full_content, re.DOTALL)
        
        if chart_match:
            try:
                chart_json_str = chart_match.group(1).strip()
                # è§£æ JSON
                echarts_option = json.loads(chart_json_str)
                logger.info(f"âœ… æˆåŠŸæå– ECharts é…ç½®: {list(echarts_option.keys())}")
                
                # å‘é€å›¾è¡¨é…ç½®äº‹ä»¶
                chart_chunk = {
                    "type": "chart_config",
                    "data": {
                        "echarts_option": echarts_option
                    },
                    "provider": "deepseek",
                    "finished": False,
                    "tenant_id": tenant_id
                }
                yield f"data: {json.dumps(chart_chunk, ensure_ascii=False)}\n\n"
                
                # å¯é€‰ï¼šä»æœ€ç»ˆå†…å®¹ä¸­ç§»é™¤å›¾è¡¨æ ‡è®°ï¼ˆå‰ç«¯å¯èƒ½å·²ç»æ˜¾ç¤ºäº†ï¼‰
                # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ ‡è®°ï¼Œè®©å‰ç«¯è‡ªå·±å†³å®šæ˜¯å¦æ˜¾ç¤º
                
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ è§£æ ECharts JSON å¤±è´¥: {e}")
                logger.warning(f"åŸå§‹å†…å®¹: {chart_json_str[:200]}...")
            except Exception as e:
                logger.error(f"âŒ æå– ECharts é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå™¨é”™è¯¯: {e}")
        error_data = {
            "type": "error",
            "content": f"Stream error: {str(e)}",
            "provider": "unknown",
            "finished": True,
            "tenant_id": tenant_id
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


@router.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completion(
    request: ChatCompletionRequest,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    èŠå¤©å®Œæˆæ¥å£
    æ”¯æŒå¤šæä¾›å•†ã€å¤šæ¨¡æ€å’Œæµå¼è¾“å‡º
    è‡ªåŠ¨è·å–ç”¨æˆ·æ•°æ®æºä¿¡æ¯å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    """
    try:
        # è·å–tenant_idï¼Œæ”¯æŒå¼€å‘ç¯å¢ƒä¸‹çš„é»˜è®¤ç§Ÿæˆ·
        tenant_id = getattr(current_user, 'tenant_id', None) or current_user.get('tenant_id', 'default_tenant')
        logger.info(f"Chat completion request for tenant: {tenant_id}, stream={request.stream}, data_source_ids={request.data_source_ids}")
        print(f"[DEBUG] Chat completion request - stream={request.stream}, data_source_ids={request.data_source_ids}")
        # å¼ºåˆ¶è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        import sys
        print(f"[DEBUG] Chat completion request - stream={request.stream}, data_source_ids={request.data_source_ids}", file=sys.stderr)

        # è½¬æ¢æä¾›å•†
        provider = None
        if request.provider:
            try:
                provider = LLMProvider(request.provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {request.provider}"
                )

        # è·å–æ•°æ®æºä¸Šä¸‹æ–‡ï¼Œå¦‚æœæŒ‡å®šäº†æ•°æ®æºIDåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        print(f"[DEBUG] Starting _get_data_sources_context for tenant: {tenant_id}, data_source_ids: {request.data_source_ids}")
        import time as _time
        _ctx_start = _time.time()
        data_sources_context = await _get_data_sources_context(tenant_id, db, request.data_source_ids)
        print(f"[DEBUG] _get_data_sources_context took {_time.time() - _ctx_start:.2f}s")
        if data_sources_context:
            logger.info(f"Data sources context retrieved for tenant {tenant_id}, length: {len(data_sources_context)}")
            # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æ•°æ®æºä¸Šä¸‹æ–‡çš„å‰1000ä¸ªå­—ç¬¦
            logger.debug(f"Data sources context content (first 1000 chars): {data_sources_context[:1000]}")
        else:
            logger.info(f"No data sources found for tenant {tenant_id}")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = _convert_chat_messages(request.messages)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ åŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„systemæ¶ˆæ¯
        has_system_message = any(msg.role == "system" for msg in messages)
        if not has_system_message:
            system_prompt = _build_system_prompt_with_context(data_sources_context)
            system_message = LLMMessage(role="system", content=system_prompt)
            messages.insert(0, system_message)
            logger.info("Added system message with data sources context")
        elif data_sources_context:
            # å¦‚æœå·²æœ‰systemæ¶ˆæ¯ï¼Œæ›¿æ¢ä¸ºå®Œæ•´çš„æ•°æ®åˆ†æç³»ç»Ÿæç¤ºï¼ˆåŒ…å«SQLç”ŸæˆæŒ‡ä»¤ï¼‰
            # è¿™æ ·ç¡®ä¿AIçŸ¥é“å¦‚ä½•æ­£ç¡®ç”ŸæˆSQLæŸ¥è¯¢
            full_system_prompt = _build_system_prompt_with_context(data_sources_context)
            for i, msg in enumerate(messages):
                if msg.role == "system":
                    messages[i] = LLMMessage(role="system", content=full_system_prompt, thinking=msg.thinking)
                    logger.info("Replaced existing system message with full data sources context and SQL instructions")
                    break

        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºåŸå§‹é—®é¢˜
        original_question = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                original_question = msg.content
                break

        # è°ƒç”¨LLMæœåŠ¡
        if request.stream:
            # æµå¼å“åº”
            # æ³¨æ„ï¼šchat_completion æ˜¯å¼‚æ­¥å‡½æ•°ï¼Œéœ€è¦ await æ¥è·å– AsyncGenerator
            logger.info(f"[STREAM] Starting stream request for tenant {tenant_id}")
            print(f"[STREAM] Starting stream request for tenant {tenant_id}", file=sys.stderr)
            
            # æ–¹æ¡ˆ B: ä¸ä½¿ç”¨ Function Callingï¼Œæ”¹ç”¨ SQL ä»£ç å—æ£€æµ‹
            # DeepSeek ä¸æ”¯æŒæ ‡å‡†çš„ OpenAI Function Calling åè®®
            # æ‰€ä»¥æˆ‘ä»¬è®© AI ç›´æ¥åœ¨å›ç­”ä¸­è¾“å‡º ```sql ... ``` ä»£ç å—ï¼Œç„¶åè‡ªåŠ¨æ£€æµ‹å¹¶æ‰§è¡Œ
            logger.info(f"[STREAM] ä½¿ç”¨ SQL ä»£ç å—æ£€æµ‹æ¨¡å¼ï¼ˆæ–¹æ¡ˆBï¼‰")
            
            response_generator = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking,
                tools=None  # ä¸ä¼ é€’å·¥å…·å®šä¹‰ï¼Œä½¿ç”¨ SQL ä»£ç å—æ£€æµ‹
            )
            
            logger.info(f"[STREAM] Stream generator created, starting response")
            return StreamingResponse(
                _stream_response_generator(
                    response_generator, 
                    tenant_id, 
                    db, 
                    original_question, 
                    request.data_source_ids,
                    initial_messages=messages  # ä¼ é€’åˆå§‹æ¶ˆæ¯å†å²
                ),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "Cache-Control"
                }
            )
        else:
            # éæµå¼å“åº”
            llm_start = time.time()
            response = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )
            logger.info(f"[PERF] llm_service.chat_completion took {time.time() - llm_start:.2f}s")

            if isinstance(response, LLMResponse):
                # æ£€æµ‹å¹¶æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨ä¹‹å‰æå–çš„åŸå§‹é—®é¢˜å’ŒæŒ‡å®šçš„æ•°æ®æºï¼‰
                enhanced_content = await _execute_sql_if_needed(
                    response.content,
                    tenant_id,
                    db,
                    original_question,
                    request.data_source_ids
                )

                # æ›´æ–°å“åº”å†…å®¹
                response.content = enhanced_content

                return _convert_response(response)
            else:
                raise HTTPException(
                    status_code=500,
                    detail="Unexpected response type from LLM service"
                )

    except Exception as e:
        logger.error(f"Chat completion failed for tenant {tenant_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chat completion failed: {str(e)}"
        )


@router.get("/providers/status", response_model=ProviderStatusResponse)
async def get_provider_status(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–LLMæä¾›å•†çŠ¶æ€
    """
    try:
        tenant_id = current_user.tenant_id
        status = await llm_service.validate_providers(tenant_id)

        return ProviderStatusResponse(
            zhipu=status.get("zhipu", False),
            openrouter=status.get("openrouter", False)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get provider status: {str(e)}"
        )


@router.get("/models", response_model=AvailableModelsResponse)
async def get_available_models(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    try:
        tenant_id = current_user.tenant_id
        models = await llm_service.get_available_models(tenant_id)

        return AvailableModelsResponse(providers=models)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get available models: {str(e)}"
        )


@router.post("/test")
async def test_llm_service(
    provider: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•LLMæœåŠ¡
    """
    try:
        tenant_id = current_user.tenant_id

        # è½¬æ¢æä¾›å•†
        llm_provider = None
        if provider:
            try:
                llm_provider = LLMProvider(provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {provider}"
                )

        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        test_messages = [
            LLMMessage(
                role="user",
                content="ä½ å¥½ï¼Œè¯·å›å¤ä¸€æ¡ç®€çŸ­çš„æµ‹è¯•æ¶ˆæ¯"
            )
        ]

        # è°ƒç”¨LLMæœåŠ¡
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=llm_provider,
            max_tokens=50
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "LLM service test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"LLM service test failed: {str(e)}"
        }


@router.post("/test/multimodal")
async def test_multimodal(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€åŠŸèƒ½ï¼ˆéœ€è¦OpenRouterï¼‰
    """
    try:
        tenant_id = current_user.tenant_id

        # åˆ›å»ºå¤šæ¨¡æ€æµ‹è¯•æ¶ˆæ¯
        multimodal_content = [
            {
                "type": "text",
                "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            }
        ]

        test_messages = [
            LLMMessage(
                role="user",
                content=multimodal_content
            )
        ]

        # ä¼˜å…ˆä½¿ç”¨OpenRouterè¿›è¡Œå¤šæ¨¡æ€æµ‹è¯•
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=LLMProvider.OPENROUTER,
            max_tokens=200
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "Multimodal test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"Multimodal test failed: {str(e)}"
        }


# ===== æ–°å¢çš„é«˜çº§LLMåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹ =====

@router.get("/test/stream-thinking")
async def test_stream_thinking(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
    """
    try:
        from src.app.services.llm_service import LLMMessage

        messages = [
            LLMMessage(
                role="user",
                content="è¯·è¯¦ç»†åˆ†ææœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨åœºæ™¯"
            )
        ]

        async def stream_generator():
            try:
                # å¿…é¡»å…ˆ await è·å– AsyncGenerator å¯¹è±¡
                response_generator = await llm_service.chat_completion(
                    tenant_id=current_user.id,
                    messages=messages,
                    stream=True,
                    enable_thinking=None  # è‡ªåŠ¨åˆ¤æ–­
                )
                # ç„¶åæ‰èƒ½ä½¿ç”¨ async for è¿­ä»£ç”Ÿæˆå™¨
                async for chunk in response_generator:
                    yield f"data: {json.dumps(chunk.dict(), ensure_ascii=False)}\n\n"

                yield "data: [DONE]\n\n"
            except Exception as e:
                yield f"data: {{'type': 'error', 'content': '{str(e)}'}}\n\n"

        return StreamingResponse(
            stream_generator(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stream thinking test failed: {str(e)}")


@router.get("/test/intelligent-params")
async def test_intelligent_params(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æ™ºèƒ½å‚æ•°è°ƒæ•´åŠŸèƒ½
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„é—®é¢˜
        test_cases = [
            {
                "name": "ç®€å•é—®é¢˜",
                "messages": [LLMMessage(role="user", content="ä½ å¥½")]
            },
            {
                "name": "å¤æ‚é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†ã€å¸¸è§çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒæŠ€å·§ä»¥åŠåœ¨å®é™…é¡¹ç›®ä¸­çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¹¶è®¨è®ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
                )]
            },
            {
                "name": "éœ€è¦æ€è€ƒçš„é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="ä¸ºä»€ä¹ˆTransformeræ¶æ„èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„RNNæ¨¡å‹ï¼Ÿè¯·ä»æ³¨æ„åŠ›æœºåˆ¶ã€å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€é•¿æœŸä¾èµ–å¤„ç†ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œæ·±å…¥åˆ†æã€‚"
                )]
            }
        ]

        results = []

        for case in test_cases:
            complexity = llm_service.analyze_conversation_complexity(case["messages"])

            # ä½¿ç”¨æ™ºèƒ½å‚æ•°è°ƒç”¨
            response = await llm_service.chat_completion(
                tenant_id=current_user.id,
                messages=case["messages"],
                enable_thinking=None,  # è‡ªåŠ¨åˆ¤æ–­
                temperature=complexity.get("recommend_temperature", 0.7),
                max_tokens=complexity.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
            )

            results.append({
                "case_name": case["name"],
                "complexity_analysis": complexity,
                "response_type": type(response).__name__,
                "success": response is not None
            })

        return {
            "success": True,
            "message": "Intelligent parameter adjustment test completed",
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Intelligent params test failed: {str(e)}")


@router.get("/test/multimodal-upload")
async def test_multimodal_upload(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€å†…å®¹å¤„ç†å’ŒMinIOä¸Šä¼ 
    """
    try:
        from src.app.services.llm_service import LLMMessage
        from src.app.services.multimodal_processor import multimodal_processor

        # æ„å»ºåŒ…å«å›¾ç‰‡URLçš„æµ‹è¯•æ¶ˆæ¯
        test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

        messages = [
            LLMMessage(
                role="user",
                content=[
                    {
                        "type": "text",
                        "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": test_image_url
                        }
                    }
                ]
            )
        ]

        # æµ‹è¯•å¤šæ¨¡æ€å¤„ç†
        processed_content = await multimodal_processor.process_content_list(
            messages[0].content,
            current_user.id
        )

        # å°è¯•è°ƒç”¨OpenRouterï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            provider=LLMProvider.OPENROUTER,
            stream=False
        )

        return {
            "success": True,
            "message": "Multimodal upload test completed",
            "original_content_count": len(messages[0].content),
            "processed_content_count": len(processed_content),
            "response_received": response is not None,
            "processed_sample": processed_content[:2] if processed_content else []
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Multimodal upload test failed: {str(e)}")


@router.get("/test/tenant-isolation")
async def test_tenant_isolation(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•ç§Ÿæˆ·éš”ç¦»åŠŸèƒ½
    """
    try:
        from src.app.services.tenant_config_manager import tenant_config_manager, ProviderType

        # æµ‹è¯•ç§Ÿæˆ·é…ç½®è·å–
        test_tenant_id = current_user.id
        providers = []

        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            api_key = await tenant_config_manager.get_tenant_api_key(
                test_tenant_id, provider, use_global_fallback=True
            )
            if api_key:
                providers.append(provider.value)

        # æµ‹è¯•æ¨¡å‹é…ç½®è·å–
        model_configs = {}
        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            config = await tenant_config_manager.get_tenant_model_config(test_tenant_id, provider)
            model_configs[provider.value] = config

        # éªŒè¯ç§Ÿæˆ·é…ç½®
        validation_results = await tenant_config_manager.validate_tenant_config(test_tenant_id)

        return {
            "success": True,
            "message": "Tenant isolation test completed",
            "tenant_id": test_tenant_id,
            "available_providers": providers,
            "model_configs": model_configs,
            "validation_results": validation_results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tenant isolation test failed: {str(e)}")


@router.get("/test/data-sources-context")
async def test_data_sources_context(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•æ•°æ®æºä¸Šä¸‹æ–‡è·å–
    """
    tenant_id = current_user.get("tenant_id", "")
    context = await _get_data_sources_context(tenant_id, db)
    return {
        "tenant_id": tenant_id,
        "context_length": len(context),
        "context": context
    }


@router.get("/test/all-features")
async def test_all_features(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    ç»¼åˆåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æ„å»ºå¤æ‚çš„æµ‹è¯•åœºæ™¯
        messages = [
            LLMMessage(
                role="user",
                content="ä½œä¸ºä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·åˆ†æå’Œè¯„ä¼°å½“å‰å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„å‘å±•ç°çŠ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ¶æ„ã€åº”ç”¨åœºæ™¯ã€ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å‘å±•è¶‹åŠ¿ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„å»ºè®®ã€‚"
            )
        ]

        # è·å–å¯¹è¯å¤æ‚åº¦åˆ†æ
        complexity_analysis = llm_service.analyze_conversation_complexity(messages)

        # è°ƒç”¨èŠå¤©å®Œæˆï¼ˆå¯ç”¨æ™ºèƒ½æ€è€ƒæ¨¡å¼ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            stream=False,
            enable_thinking=None,  # è‡ªåŠ¨å¯ç”¨æ€è€ƒæ¨¡å¼
            temperature=complexity_analysis.get("recommend_temperature", 0.7),
            max_tokens=complexity_analysis.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
        )

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = await llm_service.get_available_models(current_user.id)

        # éªŒè¯æä¾›å•†çŠ¶æ€
        provider_status = await llm_service.validate_providers(current_user.id)

        return {
            "success": True,
            "message": "Comprehensive LLM features test completed",
            "complexity_analysis": complexity_analysis,
            "response_received": response is not None,
            "available_models": available_models,
            "provider_status": provider_status,
            "features_tested": [
                "æ™ºèƒ½æ€è€ƒæ¨¡å¼",
                "å¤æ‚åº¦åˆ†æ",
                "æ™ºèƒ½å‚æ•°è°ƒæ•´",
                "å¤šæä¾›å•†æ”¯æŒ",
                "ç§Ÿæˆ·éš”ç¦»"
            ]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Comprehensive test failed: {str(e)}")