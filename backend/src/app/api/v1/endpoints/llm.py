"""
LLM APIç«¯ç‚¹
æä¾›ç»Ÿä¸€çš„èŠå¤©å®Œæˆã€æµå¼è¾“å‡ºå’Œå¤šæ¨¡æ€æ”¯æŒ
"""

import json
import asyncio
import logging
import io
import time
from typing import Dict, Any, Optional, List, Union
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import pandas as pd

from src.app.services.llm_service import (
    llm_service,
    LLMProvider,
    LLMMessage,
    LLMResponse,
    LLMStreamChunk
)
from src.app.core.auth import get_current_user_with_tenant
from src.app.data.models import Tenant, DataSourceConnection, DataSourceConnectionStatus
from src.app.data.database import get_db
from src.app.services.data_source_service import data_source_service
from src.app.services.minio_client import minio_service
from src.app.services.database_interface import PostgreSQLAdapter
from src.app.services.zhipu_client import zhipu_service
import re
import duckdb

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["LLM"])


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser, assistant, system")
    content: Union[str, List[Dict[str, Any]]] = Field(
        ...,
        description="æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€å†…å®¹"
    )
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹ï¼ˆä»…assistantè§’è‰²ï¼‰")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹"""
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    provider: Optional[str] = Field(
        None,
        description="LLMæä¾›å•†ï¼šzhipu, openrouterï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©"
    )
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§è¾“å‡ºtokens")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•°(0-1)")
    stream: bool = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
    enable_thinking: bool = Field(False, description="æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä»…Zhipuæ”¯æŒï¼‰")
    data_source_ids: Optional[List[str]] = Field(None, description="æŒ‡å®šä½¿ç”¨çš„æ•°æ®æºIDåˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰æ´»è·ƒæ•°æ®æº")


class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”æ¨¡å‹"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨æƒ…å†µ")
    model: Optional[str] = Field(None, description="ä½¿ç”¨çš„æ¨¡å‹")
    provider: Optional[str] = Field(None, description="ä½¿ç”¨çš„æä¾›å•†")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")
    created_at: Optional[str] = Field(None, description="åˆ›å»ºæ—¶é—´")


class ProviderStatusResponse(BaseModel):
    """æä¾›å•†çŠ¶æ€å“åº”æ¨¡å‹"""
    zhipu: bool = Field(..., description="æ™ºè°±AIå¯ç”¨æ€§")
    openrouter: bool = Field(..., description="OpenRouterå¯ç”¨æ€§")


class AvailableModelsResponse(BaseModel):
    """å¯ç”¨æ¨¡å‹å“åº”æ¨¡å‹"""
    providers: Dict[str, List[str]] = Field(..., description="å„æä¾›å•†çš„å¯ç”¨æ¨¡å‹")


def _convert_chat_messages(messages: List[ChatMessage]) -> List[LLMMessage]:
    """è½¬æ¢èŠå¤©æ¶ˆæ¯æ ¼å¼"""
    llm_messages = []
    for msg in messages:
        llm_messages.append(LLMMessage(
            role=msg.role,
            content=msg.content,
            thinking=msg.thinking
        ))
    return llm_messages


def _get_column_type(dtype_str: str) -> str:
    """å°†pandasæ•°æ®ç±»å‹è½¬æ¢ä¸ºå‹å¥½çš„ç±»å‹æè¿°"""
    if 'int' in dtype_str:
        return 'integer'
    elif 'float' in dtype_str:
        return 'float'
    elif 'datetime' in dtype_str:
        return 'datetime'
    elif 'bool' in dtype_str:
        return 'boolean'
    else:
        return 'text'


def _build_table_schema(df: pd.DataFrame, table_name: str) -> Dict[str, Any]:
    """ä»DataFrameæ„å»ºå•ä¸ªè¡¨çš„schemaä¿¡æ¯"""
    columns = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        columns.append({
            "name": str(col),
            "type": _get_column_type(dtype),
            "nullable": df[col].isnull().any()
        })

    # è·å–ç¤ºä¾‹æ•°æ®ï¼ˆå‰5è¡Œï¼‰
    sample_rows = []
    for _, row in df.head(5).iterrows():
        row_data = {}
        for col in df.columns[:10]:  # é™åˆ¶åˆ—æ•°
            value = row[col]
            if pd.isna(value):
                row_data[str(col)] = None
            else:
                row_data[str(col)] = str(value)
        sample_rows.append(row_data)

    return {
        "table_info": {
            "name": table_name,
            "columns": columns,
            "row_count": len(df)
        },
        "sample_data": {
            "columns": [str(c) for c in df.columns[:10]],
            "data": sample_rows
        }
    }


async def _get_file_schema(connection_string: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶æ•°æ®æºè·å–schemaä¿¡æ¯ï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼ˆæ ¼å¼: file://data-sources/{tenant_id}/{file_id}.xlsxï¼‰
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsç­‰ï¼‰
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¡¨ï¼ˆSheetï¼‰çš„åˆ—åã€ç±»å‹å’Œç¤ºä¾‹æ•°æ®
    """
    try:
        # è§£æå­˜å‚¨è·¯å¾„
        if connection_string.startswith("file://"):
            storage_path = connection_string[7:]  # å»æ‰ "file://" å‰ç¼€
        else:
            storage_path = connection_string

        # ä»MinIOä¸‹è½½æ–‡ä»¶
        logger.info(f"ä»MinIOä¸‹è½½æ–‡ä»¶: {storage_path}")
        file_data = minio_service.download_file(
            bucket_name="data-sources",
            object_name=storage_path
        )

        if not file_data:
            logger.warning(f"æ— æ³•ä»MinIOè·å–æ–‡ä»¶: {storage_path}")
            return {}

        tables = []
        sample_data = {}

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheet
            try:
                # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                return {}
            except Exception as e:
                logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                return {}
            
            sheet_names = excel_file.sheet_names
            logger.info(f"Excelæ–‡ä»¶åŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨å
                    table_schema = _build_table_schema(df, sheet_name)
                    tables.append(table_schema["table_info"])
                    sample_data[sheet_name] = table_schema["sample_data"]
                    logger.info(f"Sheet '{sheet_name}': {len(df)}è¡Œ, {len(df.columns)}åˆ—")
                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                try:
                    df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if df is not None:
                table_schema = _build_table_schema(df, data_source_name)
                tables.append(table_schema["table_info"])
                sample_data[data_source_name] = table_schema["sample_data"]

        if not tables:
            logger.warning(f"æ— æ³•ä»æ–‡ä»¶è§£æä»»ä½•è¡¨: {storage_path}")
            return {}

        schema_info = {
            "tables": tables,
            "sample_data": sample_data
        }

        total_rows = sum(t.get("row_count", 0) for t in tables)
        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
        return schema_info

    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶schemaå¤±è´¥: {e}")
        return {}


async def _try_find_file_in_minio(tenant_id: str, data_source_id: str, db_type: str) -> Optional[str]:
    """
    å°è¯•åœ¨MinIOä¸­æŸ¥æ‰¾æ•°æ®æºå¯¹åº”çš„æ–‡ä»¶

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹

    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
        prefix = f"{tenant_id}/{data_source_id}"
        ext = f".{db_type}"

        # åˆ—å‡ºMinIOä¸­çš„æ–‡ä»¶
        objects = minio_service.list_files(
            bucket_name="data-sources",
            prefix=prefix
        )

        for obj in objects:
            obj_name = obj.object_name if hasattr(obj, 'object_name') else str(obj)
            if obj_name.endswith(ext):
                return f"file://{obj_name}"

        logger.warning(f"åœ¨MinIOä¸­æœªæ‰¾åˆ°æ•°æ®æº {data_source_id} çš„æ–‡ä»¶")
        return None

    except Exception as e:
        logger.error(f"åœ¨MinIOä¸­æœç´¢æ–‡ä»¶å¤±è´¥: {e}")
        return None


async def _try_get_file_schema_fallback(tenant_id: str, data_source_id: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ä»MinIOç›´æ¥è·å–æ–‡ä»¶schemaï¼ˆæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
        ext = f".{db_type}"
        possible_paths = [
            f"{tenant_id}/{data_source_id}{ext}",
            f"{tenant_id}/{data_source_name}{ext}",
        ]

        for path in possible_paths:
            try:
                logger.info(f"å°è¯•ä»MinIOè·å–æ–‡ä»¶: {path}")
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=path
                )

                if file_data:
                    tables = []
                    sample_data = {}

                    if db_type in ["xlsx", "xls"]:
                        # è¯»å–æ‰€æœ‰Sheet
                        try:
                            # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                            excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
                        except ImportError as e:
                            logger.error(f"System Error: Missing dependency 'openpyxl'. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        except Exception as e:
                            logger.error(f"Execution Error: Failed to read Excel file. {str(e)}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        
                        sheet_names = excel_file.sheet_names
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆ: ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

                        for sheet_name in sheet_names:
                            try:
                                # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                                df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                                if df.empty:
                                    continue
                                table_schema = _build_table_schema(df, sheet_name)
                                tables.append(table_schema["table_info"])
                                sample_data[sheet_name] = table_schema["sample_data"]
                            except Exception as e:
                                logger.debug(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                                continue

                    elif db_type == "csv":
                        df = None
                        for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                            try:
                                df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                                break
                            except UnicodeDecodeError:
                                continue

                        if df is not None:
                            table_schema = _build_table_schema(df, data_source_name)
                            tables.append(table_schema["table_info"])
                            sample_data[data_source_name] = table_schema["sample_data"]

                    if tables:
                        total_rows = sum(t.get("row_count", 0) for t in tables)
                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆæˆåŠŸè·å–schema: {len(tables)}ä¸ªè¡¨, å…±{total_rows}è¡Œ")
                        return {
                            "tables": tables,
                            "sample_data": sample_data
                        }

            except Exception as e:
                logger.debug(f"å°è¯•è·¯å¾„ {path} å¤±è´¥: {e}")
                continue

        logger.warning(f"å¤‡é€‰æ–¹æ¡ˆæœªèƒ½è·å–æ•°æ®æº {data_source_name} çš„schema")
        return {}

    except Exception as e:
        logger.error(f"å¤‡é€‰æ–¹æ¡ˆè·å–schemaå¤±è´¥: {e}")
        return {}


async def _get_data_sources_context(tenant_id: str, db: Session, data_source_ids: Optional[List[str]] = None) -> str:
    """
    è·å–ç§Ÿæˆ·æ•°æ®æºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬schemaï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰æ´»è·ƒæ•°æ®æº

    Returns:
        æ•°æ®æºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    start_time = time.time()
    try:
        # è·å–ç§Ÿæˆ·çš„æ‰€æœ‰æ´»è·ƒæ•°æ®æº
        t1 = time.time()
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )
        perf_msg = f"[PERF] get_data_sources took {time.time() - t1:.2f}s, found {len(data_sources) if data_sources else 0} sources"
        print(perf_msg)  # ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
        logger.info(perf_msg)

        if not data_sources:
            return ""

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        if data_source_ids:
            data_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            logger.info(f"ç­›é€‰æŒ‡å®šæ•°æ®æº: {data_source_ids}, æ‰¾åˆ° {len(data_sources)} ä¸ªåŒ¹é…çš„æ•°æ®æº")
            if not data_sources:
                return ""

        context_parts = []
        context_parts.append("## å¯ç”¨æ•°æ®æº\n")

        for ds in data_sources:
            try:
                ds_start = time.time()
                schema_info = None
                connection_string = None

                # å°è¯•è·å–è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
                try:
                    t2 = time.time()
                    connection_string = await data_source_service.get_decrypted_connection_string(
                        data_source_id=ds.id,
                        tenant_id=tenant_id,
                        db=db
                    )
                    print(f"[PERF] get_decrypted_connection_string for {ds.name} took {time.time() - t2:.2f}s")
                except Exception as decrypt_error:
                    print(f"[PERF] è§£å¯†æ•°æ®æº {ds.name} è¿æ¥å­—ç¬¦ä¸²å¤±è´¥: {decrypt_error}")
                    # å¯¹äºæ–‡ä»¶ç±»å‹æ•°æ®æºï¼Œå°è¯•ä»MinIOç›´æ¥æœç´¢æ–‡ä»¶
                    if ds.db_type in ["xlsx", "xls", "csv"]:
                        connection_string = await _try_find_file_in_minio(tenant_id, ds.id, ds.db_type)

                # æ ¹æ®æ•°æ®æºç±»å‹è·å–schema
                if ds.db_type == "postgresql" and connection_string:
                    t3 = time.time()
                    adapter = PostgreSQLAdapter(connection_string)
                    try:
                        await adapter.connect()
                        schema_result = await adapter.get_schema_info()
                        # å°†SchemaInfoå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        schema_info = {
                            "database_type": schema_result.database_type.value if schema_result.database_type else "postgresql",
                            "tables": [
                                {
                                    "name": table.name,
                                    "columns": [
                                        {
                                            "name": col.name,
                                            "type": col.data_type,
                                            "nullable": col.is_nullable
                                        }
                                        for col in table.columns
                                    ]
                                }
                                for table in schema_result.tables.values()
                            ] if schema_result.tables else []
                        }
                    finally:
                        await adapter.disconnect()
                    print(f"[PERF] PostgreSQL get_schema for {ds.name} took {time.time() - t3:.2f}s")

                elif ds.db_type in ["xlsx", "xls", "csv"]:
                    if connection_string:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä»MinIOè¯»å–æ–‡ä»¶å¹¶è§£æschema
                        t4 = time.time()
                        schema_info = await _get_file_schema(connection_string, ds.db_type, ds.name)
                        print(f"[PERF] _get_file_schema for {ds.name} took {time.time() - t4:.2f}s")
                    else:
                        # è¿æ¥å­—ç¬¦ä¸²è·å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
                        print(f"[PERF] å°è¯•å¤‡é€‰æ–¹æ¡ˆè·å–æ•°æ®æº {ds.name} çš„schema")
                        schema_info = await _try_get_file_schema_fallback(tenant_id, ds.id, ds.db_type, ds.name)

                print(f"[PERF] Total processing for data source {ds.name} took {time.time() - ds_start:.2f}s")

                if schema_info and schema_info.get("tables"):
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ–‡ä»¶/æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")

                    # æ·»åŠ è¡¨ä¿¡æ¯
                    context_parts.append("\n#### è¡¨ç»“æ„:")
                    for table in schema_info["tables"][:20]:  # é™åˆ¶è¡¨æ•°é‡é¿å…tokenè¿‡å¤š
                        table_name = table.get("name", "unknown")
                        row_count = table.get("row_count", "æœªçŸ¥")
                        context_parts.append(f"\n**è¡¨: {table_name}** (å…±{row_count}è¡Œ)")

                        columns = table.get("columns", [])
                        if columns:
                            col_info = []
                            for col in columns[:30]:  # é™åˆ¶åˆ—æ•°é‡
                                col_name = col.get("name", "unknown")
                                col_type = col.get("type", "unknown")
                                nullable = "å¯ç©º" if col.get("nullable") else "éç©º"
                                col_info.append(f"  - {col_name} ({col_type}, {nullable})")
                            context_parts.append("\n".join(col_info))

                        # æ·»åŠ ä¸»é”®ä¿¡æ¯
                        if table.get("primary_key"):
                            context_parts.append(f"  - ä¸»é”®: {', '.join(table['primary_key'])}")

                        # æ·»åŠ å¤–é”®ä¿¡æ¯
                        if table.get("foreign_keys"):
                            for fk in table["foreign_keys"]:
                                context_parts.append(
                                    f"  - å¤–é”®: {fk['column']} -> {fk['references_table']}.{fk['references_column']}"
                                )

                    # æ·»åŠ è¡¨å…³ç³»ä¿¡æ¯ï¼ˆå¤–é”®å…³è”ï¼‰
                    relationships = schema_info.get("relationships", [])
                    if relationships:
                        context_parts.append("\n#### è¡¨å…³ç³»ï¼ˆå¤–é”®ï¼‰:")
                        context_parts.append("**é‡è¦ï¼š** ä»¥ä¸‹æ˜¯è¡¨ä¹‹é—´çš„å…³è”å…³ç³»ï¼ŒæŸ¥è¯¢æ—¶å¿…é¡»é€šè¿‡è¿™äº›å¤–é”®è¿›è¡ŒJOINï¼š")
                        for rel in relationships:
                            from_table = rel.get("from_table", "unknown")
                            from_column = rel.get("from_column", "unknown")
                            to_table = rel.get("to_table", "unknown")
                            to_column = rel.get("to_column", "unknown")
                            context_parts.append(
                                f"  - {from_table}.{from_column} -> {to_table}.{to_column}"
                            )

                    # æ·»åŠ ç¤ºä¾‹æ•°æ®
                    sample_data = schema_info.get("sample_data", {})
                    if sample_data:
                        context_parts.append("\n#### ç¤ºä¾‹æ•°æ®:")
                        for table_name, samples in list(sample_data.items())[:5]:  # é™åˆ¶è¡¨æ•°é‡
                            if samples.get("data"):
                                context_parts.append(f"\n**{table_name}** (å‰5è¡Œ):")
                                for row in samples["data"][:3]:  # é™åˆ¶è¡Œæ•°
                                    row_str = ", ".join([f"{k}={v}" for k, v in list(row.items())[:5]])
                                    context_parts.append(f"  {row_str}")
                else:
                    # å…¶ä»–æ•°æ®åº“ç±»å‹æš‚æ—¶åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")
                    context_parts.append("- æ³¨: æ­¤æ•°æ®åº“ç±»å‹çš„schemaå‘ç°åŠŸèƒ½å¼€å‘ä¸­")

            except Exception as e:
                logger.warning(f"è·å–æ•°æ®æº {ds.name} çš„schemaå¤±è´¥: {e}")
                context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                context_parts.append(f"- ç±»å‹: {ds.db_type}")
                context_parts.append(f"- æ³¨: æ— æ³•è·å–schemaä¿¡æ¯ ({str(e)[:50]})")

        total_time = time.time() - start_time
        logger.info(f"[PERF] _get_data_sources_context TOTAL took {total_time:.2f}s")
        return "\n".join(context_parts)

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return ""


def _build_system_prompt_with_context(data_sources_context: str) -> str:
    """
    æ„å»ºåŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç®€æ´ç‰ˆæœ¬ï¼‰

    Args:
        data_sources_context: æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯

    Returns:
        ç³»ç»Ÿæç¤ºè¯
    """
    if data_sources_context:
        # æœ‰æ•°æ®æºæ—¶çš„ç®€æ´æç¤ºï¼Œå¼ºè°ƒå¿…é¡»ä½¿ç”¨schemaä¸­çš„å®é™…è¡¨åå’Œåˆ—å
        return f"""ä½ æ˜¯ä¸€ä¸ªSQLæ•°æ®åˆ†æåŠ©æ‰‹ã€‚

## ğŸ”´ğŸ”´ğŸ”´ æœ€é‡è¦çš„è§„åˆ™ï¼šå¿…é¡»ä½¿ç”¨ä¸‹é¢æä¾›çš„å®é™…è¡¨åå’Œåˆ—å ğŸ”´ğŸ”´ğŸ”´

ä½ åªèƒ½ä½¿ç”¨ä»¥ä¸‹Schemaä¸­åˆ—å‡ºçš„è¡¨åå’Œåˆ—åã€‚**ç¦æ­¢**æ ¹æ®ç”¨æˆ·æè¿°çŒœæµ‹æˆ–ç¿»è¯‘è¡¨åã€‚

{data_sources_context}

## æ ¸å¿ƒè§„åˆ™

1. **å¿…é¡»ä½¿ç”¨ä¸Šè¿°Schemaä¸­çš„å®é™…è¡¨åå’Œåˆ—å**ï¼š
   - âŒ é”™è¯¯ï¼šç”¨æˆ·é—®"å®¢æˆ·æ¶ˆè´¹"ï¼Œä½ ç”¨ `SELECT * FROM å®¢æˆ·` ï¼ˆè¿™æ˜¯çŒœæµ‹çš„ä¸­æ–‡è¡¨åï¼ï¼‰
   - âœ… æ­£ç¡®ï¼šç”¨æˆ·é—®"å®¢æˆ·æ¶ˆè´¹"ï¼Œä½ æŸ¥çœ‹Schemaï¼Œå‘ç°å®é™…è¡¨åæ˜¯ `customers`ï¼Œåˆ™ç”¨ `SELECT * FROM customers`
   - âŒ é”™è¯¯ï¼šç”¨æˆ·é—®"è®¢å•é‡‘é¢"ï¼Œä½ ç”¨ `SELECT é‡‘é¢ FROM è®¢å•` ï¼ˆè¿™æ˜¯çŒœæµ‹çš„ä¸­æ–‡åˆ—åï¼ï¼‰
   - âœ… æ­£ç¡®ï¼šç”¨æˆ·é—®"è®¢å•é‡‘é¢"ï¼Œä½ æŸ¥çœ‹Schemaï¼Œå‘ç°å®é™…æ˜¯ `orders` è¡¨çš„ `amount` åˆ—

2. **ç›´æ¥ç”ŸæˆSQL**ï¼šå½“ç”¨æˆ·é—®æ•°æ®ç›¸å…³é—®é¢˜æ—¶ï¼Œç«‹å³ç”ŸæˆSQLæŸ¥è¯¢ã€‚

3. **SQLä»£ç å—æ ¼å¼**ï¼šå°†SQLæ”¾åœ¨ ```sql ä»£ç å—ä¸­ã€‚

4. **ç³»ç»Ÿè‡ªåŠ¨æ‰§è¡Œ**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒSQLå¹¶æ˜¾ç¤ºç»“æœï¼Œ**ä½ ä¸éœ€è¦ä¹Ÿä¸åº”è¯¥è‡ªå·±ç¼–å†™æˆ–çŒœæµ‹æŸ¥è¯¢ç»“æœ**ã€‚

5. **ğŸ”´ æå€¼æŸ¥è¯¢å¿…é¡»ä½¿ç”¨ LIMIT 1**ï¼šå½“ç”¨æˆ·é—®"æœ€å¤§"ã€"æœ€å°"ã€"æœ€é•¿"ã€"æœ€çŸ­"ã€"æœ€é«˜"ã€"æœ€ä½"ã€"ç¬¬ä¸€ä¸ª"ã€"æœ€æ—©"ã€"æœ€æ™š"ç­‰æå€¼é—®é¢˜æ—¶ï¼Œ**å¿…é¡»åªè¿”å›ä¸€æ¡è®°å½•**ï¼š
   - âŒ é”™è¯¯ï¼š`SELECT name, hire_date FROM employees ORDER BY hire_date ASC` ï¼ˆå¯èƒ½è¿”å›å¤šè¡Œï¼ï¼‰
   - âœ… æ­£ç¡®ï¼š`SELECT name, hire_date FROM employees ORDER BY hire_date ASC LIMIT 1` ï¼ˆåªè¿”å›ä¸€è¡Œï¼‰
   - ç”¨æˆ·é—®"è°å·¥ä½œæ—¶é—´æœ€é•¿"â†’ æ„æ€æ˜¯"è°å…¥èŒæœ€æ—©"â†’ ç”¨ `ORDER BY hire_date ASC LIMIT 1`
   - ç”¨æˆ·é—®"è°è–ªèµ„æœ€é«˜"â†’ ç”¨ `ORDER BY salary DESC LIMIT 1`

6. **åªç”Ÿæˆä¸€ä¸ªSQLæŸ¥è¯¢**ï¼šæ¯æ¬¡å›ç­”åªç”Ÿæˆä¸€ä¸ªSQLè¯­å¥ï¼Œä¸è¦ç”Ÿæˆå¤šä¸ªç‹¬ç«‹çš„æŸ¥è¯¢ã€‚

## å›ç­”æµç¨‹

1. é˜…è¯»ç”¨æˆ·é—®é¢˜ï¼Œç†è§£æ„å›¾
2. æŸ¥çœ‹ä¸Šæ–¹çš„Schemaä¿¡æ¯ï¼Œæ‰¾åˆ°å¯¹åº”çš„**å®é™…è¡¨å**å’Œ**å®é™…åˆ—å**
3. ä½¿ç”¨Schemaä¸­çš„å®é™…åç§°ç”ŸæˆSQL
4. **åªæä¾›SQLè¯­å¥**ï¼Œä¸è¦è‡ªå·±ç¼–é€ ç»“æœè¡¨æ ¼

**é‡è¦æé†’**ï¼š
- ä¸è¦ç¿»è¯‘è¡¨åï¼å¦‚æœSchemaä¸­æ˜¯ `customers`ï¼Œå°±ç”¨ `customers`ï¼Œä¸è¦ç”¨ `å®¢æˆ·`
- ä¸è¦ç¿»è¯‘åˆ—åï¼å¦‚æœSchemaä¸­æ˜¯ `total_amount`ï¼Œå°±ç”¨ `total_amount`ï¼Œä¸è¦ç”¨ `æ€»é‡‘é¢`
- ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒSQLå¹¶æ˜¾ç¤ºçœŸå®ç»“æœ
- **ğŸš« ç¦æ­¢è‡ªå·±ç”Ÿæˆæˆ–çŒœæµ‹æŸ¥è¯¢ç»“æœè¡¨æ ¼ï¼** åªéœ€æä¾›SQLè¯­å¥ï¼Œç»“æœç”±ç³»ç»Ÿè‡ªåŠ¨æ‰§è¡Œåå±•ç¤º
- **ğŸ”´ æå€¼é—®é¢˜å¿…é¡»ä½¿ç”¨ LIMIT 1 ç¡®ä¿åªè¿”å›ä¸€æ¡è®°å½•ï¼**"""
    else:
        # æ²¡æœ‰æ•°æ®æºæ—¶çš„æç¤º
        return """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚

å½“å‰ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰è¿æ¥ä»»ä½•æ•°æ®æºã€‚

å¦‚æœç”¨æˆ·è¯¢é—®æ•°æ®ç›¸å…³é—®é¢˜ï¼Œè¯·å‘Šè¯‰ä»–ä»¬éœ€è¦å…ˆåœ¨"æ•°æ®æºç®¡ç†"é¡µé¢æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚

ä¸è¦å‡è®¾æˆ–çŒœæµ‹æ•°æ®åº“ç»“æ„ï¼Œä¸è¦ç”Ÿæˆä»»ä½•SQLæŸ¥è¯¢ã€‚"""


def _parse_sql_error(error_message: str) -> Dict[str, str]:
    """
    è§£æSQLé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯

    Args:
        error_message: å®Œæ•´çš„é”™è¯¯ä¿¡æ¯

    Returns:
        åŒ…å«main_error, hint, suggestionçš„å­—å…¸
    """
    result = {
        'main_error': error_message,
        'hint': None,
        'suggestion': None
    }

    try:
        # æå–ä¸»è¦é”™è¯¯ä¿¡æ¯
        lines = error_message.split('\n')

        # é¦–å…ˆå°è¯•ä»å®Œæ•´é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ—/è¡¨ä¸å­˜åœ¨çš„é”™è¯¯
        column_match = re.search(r'column "([^"]+)" does not exist', error_message, re.IGNORECASE)
        table_match = re.search(r'table "([^"]+)" does not exist', error_message, re.IGNORECASE)
        relation_match = re.search(r'relation "([^"]+)" does not exist', error_message, re.IGNORECASE)

        if column_match:
            wrong_column = column_match.group(1)
            result['main_error'] = f'column "{wrong_column}" does not exist'
        elif table_match:
            wrong_table = table_match.group(1)
            result['main_error'] = f'table "{wrong_table}" does not exist'
        elif relation_match:
            wrong_relation = relation_match.group(1)
            result['main_error'] = f'relation "{wrong_relation}" does not exist'
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”™è¯¯ï¼Œå°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
            for line in lines:
                if 'psycopg2.errors' in line:
                    # æå–æ‹¬å·åçš„å†…å®¹
                    match = re.search(r'\)\s*(.+?)(?:\n|$)', line)
                    if match:
                        result['main_error'] = match.group(1).strip()
                        break
                elif line.strip() and not line.startswith('LINE') and not line.startswith('[SQL') and not line.startswith('HINT'):
                    result['main_error'] = line.strip()
                    break

        # æå–HINTä¿¡æ¯
        hint_match = re.search(r'HINT:\s*(.+?)(?:\n|$)', error_message, re.IGNORECASE)
        if hint_match:
            result['hint'] = hint_match.group(1).strip()

            # æ ¹æ®HINTç”Ÿæˆå»ºè®®
            if 'Perhaps you meant to reference the column' in result['hint']:
                # æå–å»ºè®®çš„åˆ—å
                column_match = re.search(r'column "([^"]+)"', result['hint'])
                if column_match:
                    suggested_column = column_match.group(1)
                    # æå–ç®€å•åˆ—åï¼ˆå»æ‰è¡¨åå‰ç¼€ï¼‰
                    simple_column = suggested_column.split('.')[-1] if '.' in suggested_column else suggested_column
                    result['suggestion'] = f"è¯·ä½¿ç”¨åˆ—å `{simple_column}` è€Œä¸æ˜¯é”™è¯¯çš„åˆ—åã€‚"

        # å¦‚æœæ˜¯åˆ—ä¸å­˜åœ¨é”™è¯¯ä½†æ²¡æœ‰HINTå»ºè®®
        if column_match and not result['suggestion']:
            wrong_column = column_match.group(1)
            result['suggestion'] = f"åˆ— `{wrong_column}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…åˆ—åã€‚"

        # å¦‚æœæ˜¯è¡¨ä¸å­˜åœ¨é”™è¯¯
        if table_match and not result['suggestion']:
            wrong_table = table_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_table}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

        # å¦‚æœæ˜¯relationä¸å­˜åœ¨é”™è¯¯ï¼ˆPostgreSQLçš„è¡¨ä¸å­˜åœ¨é”™è¯¯ï¼‰
        if relation_match and not result['suggestion']:
            wrong_relation = relation_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_relation}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

    except Exception as e:
        logger.warning(f"è§£æSQLé”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

    return result


async def _fix_sql_with_ai(
    original_sql: str,
    error_message: str,
    schema_context: str,
    original_question: str
) -> Optional[str]:
    """
    ä½¿ç”¨AIä¿®å¤å¤±è´¥çš„SQLæŸ¥è¯¢

    Args:
        original_sql: åŸå§‹SQLæŸ¥è¯¢
        error_message: é”™è¯¯ä¿¡æ¯
        schema_context: æ•°æ®åº“schemaä¸Šä¸‹æ–‡
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜

    Returns:
        ä¿®å¤åçš„SQLï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
    """
    try:
        # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
        error_details = _parse_sql_error(error_message)

        # æ„å»ºæ›´ç²¾ç¡®çš„ä¿®å¤æç¤º
        fix_prompt = f"""ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚ç”¨æˆ·çš„æŸ¥è¯¢æ‰§è¡Œå¤±è´¥äº†ï¼Œè¯·å¸®åŠ©ä¿®å¤SQLè¯­å¥ã€‚

# ç”¨æˆ·åŸå§‹é—®é¢˜
{original_question}

# å¤±è´¥çš„SQLæŸ¥è¯¢
```sql
{original_sql}
```

# é”™è¯¯ä¿¡æ¯
{error_details['main_error']}

# PostgreSQLæ•°æ®åº“æç¤º
{error_details.get('hint', 'æ— ')}

# ğŸ”´ğŸ”´ğŸ”´ æ•°æ®åº“Schemaä¿¡æ¯ï¼ˆå¿…é¡»ä½¿ç”¨è¿™é‡Œçš„å®é™…è¡¨åå’Œåˆ—åï¼‰
{schema_context}

# ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®å¤è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

## ç¬¬1æ­¥ï¼šç†è§£é”™è¯¯
- **ä¸»è¦é”™è¯¯**: {error_details['main_error']}
- **æ•°æ®åº“æç¤º**: {error_details.get('hint', 'æ— æç¤º')}

## ç¬¬2æ­¥ï¼šæŸ¥æ‰¾æ­£ç¡®çš„è¡¨å/åˆ—å
**ğŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šSQLä¸­ä½¿ç”¨äº†ä¸å­˜åœ¨çš„è¡¨åæˆ–åˆ—åï¼**

1. **å¦‚æœé”™è¯¯æ˜¯"Table does not exist"**ï¼š
   - è¿™é€šå¸¸æ„å‘³ç€SQLä¸­ä½¿ç”¨äº†é”™è¯¯çš„è¡¨åï¼ˆå¯èƒ½æ˜¯ç”¨æˆ·æƒ³è±¡çš„ä¸­æ–‡åï¼‰
   - å¿…é¡»ä»ä¸Šé¢çš„Schemaä¿¡æ¯ä¸­æ‰¾åˆ°å®é™…å­˜åœ¨çš„è¡¨å
   - ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"å®¢æˆ·"ï¼Œä½†Schemaä¸­å®é™…çš„è¡¨å¯èƒ½å« `customers`
   - ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"è®¢å•"ï¼Œä½†Schemaä¸­å®é™…çš„è¡¨å¯èƒ½å« `orders`

2. **å¦‚æœé”™è¯¯æ˜¯"Column does not exist"**ï¼š
   - æŸ¥çœ‹PostgreSQLçš„HINTæç¤º
   - åœ¨Schemaä¸­æ‰¾åˆ°æ­£ç¡®çš„åˆ—å

3. **å¸¸è§é”™è¯¯æ¨¡å¼ï¼ˆä¸­æ–‡è¡¨åâ†’è‹±æ–‡å®é™…è¡¨åï¼‰ï¼š**
   - âŒ `FROM å®¢æˆ·` â†’ âœ… `FROM customers`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM è®¢å•` â†’ âœ… `FROM orders`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM äº§å“` â†’ âœ… `FROM products`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰
   - âŒ `FROM å‘˜å·¥` â†’ âœ… `FROM employees`ï¼ˆæˆ–Schemaä¸­çš„å®é™…è¡¨åï¼‰

## ç¬¬3æ­¥ï¼šä¿®å¤SQL
1. ä»”ç»†é˜…è¯»ä¸Šé¢çš„Schemaä¿¡æ¯ï¼Œæ‰¾åˆ°å¯¹åº”çš„**å®é™…è¡¨åå’Œåˆ—å**
2. å°†SQLä¸­é”™è¯¯çš„è¡¨å/åˆ—åæ›¿æ¢ä¸ºSchemaä¸­çš„å®é™…åç§°
3. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®
4. åªä½¿ç”¨SELECTæŸ¥è¯¢
5. ğŸ”´ æå€¼æŸ¥è¯¢å¿…é¡»ä½¿ç”¨ LIMIT 1ï¼šå¦‚æœåŸå§‹é—®é¢˜æ¶‰åŠ"æœ€å¤§"ã€"æœ€å°"ã€"æœ€é•¿"ã€"æœ€çŸ­"ç­‰æå€¼ï¼Œç¡®ä¿SQLä½¿ç”¨ ORDER BY + LIMIT 1

## ç¬¬4æ­¥ï¼šè¿”å›ç»“æœ
- **åªè¿”å›ä¿®å¤åçš„SQLè¯­å¥** - ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–markdownæ ‡è®°
- **å¦‚æœSchemaä¸­æ²¡æœ‰ç›¸å…³çš„è¡¨æˆ–åˆ—** - è¿”å›"CANNOT_FIX"
- **ä¸è¦æ·»åŠ ```sqlæ ‡è®°** - ç›´æ¥è¿”å›çº¯SQLè¯­å¥

# ä¿®å¤ç¤ºä¾‹

**é”™è¯¯SQL:**
```sql
SELECT å®¢æˆ·.name, SUM(è®¢å•.total_amount) FROM å®¢æˆ· JOIN è®¢å• ON å®¢æˆ·.id = è®¢å•.customer_id
```

**é”™è¯¯ä¿¡æ¯:**
Table with name å®¢æˆ· does not exist

**Schemaä¿¡æ¯ä¸­æ˜¾ç¤ºå®é™…è¡¨åæ˜¯ï¼šcustomers, orders**

**ä¿®å¤åçš„SQL:**
SELECT customers.name, SUM(orders.total_amount) as total_spent FROM customers JOIN orders ON customers.id = orders.customer_id GROUP BY customers.name

---

ç°åœ¨è¯·ä¿®å¤ä¸Šè¿°å¤±è´¥çš„SQLæŸ¥è¯¢ï¼Œç›´æ¥è¿”å›ä¿®å¤åçš„SQLè¯­å¥ï¼š"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¿®å¤ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®é”™è¯¯ä¿¡æ¯å’Œschemaä¿®å¤SQLæŸ¥è¯¢ã€‚"},
            {"role": "user", "content": fix_prompt}
        ]

        # è°ƒç”¨æ™ºè°±AIä¿®å¤SQLï¼ˆè·³è¿‡å®‰å…¨æ£€æŸ¥ï¼Œå› ä¸ºè¿™æ˜¯å†…éƒ¨è°ƒç”¨ï¼‰
        response = await zhipu_service.chat_completion(
            messages=messages,
            max_tokens=1000,
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            stream=False,
            skip_security_check=True  # å†…éƒ¨SQLä¿®å¤è°ƒç”¨ï¼Œè·³è¿‡å®‰å…¨æ£€æŸ¥
        )

        if response and response.get("content"):
            fixed_sql = response["content"].strip()

            # æ¸…ç†è¿”å›çš„SQL
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            fixed_sql = re.sub(r'```sql\s*', '', fixed_sql)
            fixed_sql = re.sub(r'```\s*', '', fixed_sql)
            fixed_sql = fixed_sql.strip()

            # æ£€æŸ¥æ˜¯å¦æ— æ³•ä¿®å¤
            if "CANNOT_FIX" in fixed_sql.upper():
                logger.warning("AIè¡¨ç¤ºæ— æ³•ä¿®å¤æ­¤SQL")
                return None

            # éªŒè¯æ˜¯å¦æ˜¯SELECTæŸ¥è¯¢
            if not fixed_sql.upper().startswith('SELECT'):
                logger.warning("ä¿®å¤åçš„SQLä¸æ˜¯SELECTæŸ¥è¯¢")
                return None

            logger.info(f"AIæˆåŠŸä¿®å¤SQL: {fixed_sql[:100]}...")
            return fixed_sql

        return None

    except Exception as e:
        logger.error(f"AIä¿®å¤SQLå¤±è´¥: {e}")
        return None


async def _execute_sql_on_file_datasource(
    connection_string: str,
    db_type: str,
    sql_query: str,
    data_source_name: str
) -> Dict[str, Any]:
    """
    åœ¨æ–‡ä»¶ç±»å‹æ•°æ®æºä¸Šæ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨duckdbï¼Œæ”¯æŒExcelå¤šSheetï¼‰

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsï¼‰
        sql_query: SQLæŸ¥è¯¢è¯­å¥
        data_source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºè¡¨åï¼‰

    Returns:
        æŸ¥è¯¢ç»“æœå­—å…¸
    """
    try:
        # è§£æå­˜å‚¨è·¯å¾„
        if connection_string.startswith("file://"):
            storage_path = connection_string[7:]
        else:
            storage_path = connection_string

        # ä»MinIOä¸‹è½½æ–‡ä»¶
        logger.info(f"ä»MinIOä¸‹è½½æ–‡ä»¶ç”¨äºSQLæ‰§è¡Œ: {storage_path}")
        file_data = minio_service.download_file(
            bucket_name="data-sources",
            object_name=storage_path
        )

        if not file_data:
            return {
                "success": False,
                "error": f"æ— æ³•ä»MinIOè·å–æ–‡ä»¶: {storage_path}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        # åˆ›å»ºduckdbè¿æ¥
        conn = duckdb.connect(':memory:')
        registered_tables = []

        if db_type in ["xlsx", "xls"]:
            # è¯»å–æ‰€æœ‰Sheetå¹¶æ³¨å†Œä¸ºä¸åŒçš„è¡¨
            try:
                # æ˜¾å¼æŒ‡å®š engine='openpyxl' ä»¥ç¡®ä¿æ­£ç¡®è¯»å–
                excel_file = pd.ExcelFile(io.BytesIO(file_data), engine='openpyxl')
            except ImportError as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"System Error: Missing dependency 'openpyxl'. Please install it: pip install openpyxl. Original error: {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            except Exception as e:
                conn.close()
                return {
                    "success": False,
                    "error": f"Execution Error: Failed to read Excel file. {str(e)}",
                    "data": [],
                    "columns": [],
                    "row_count": 0
                }
            
            sheet_names = excel_file.sheet_names
            logger.info(f"ExcelåŒ…å« {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            for sheet_name in sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='openpyxl')
                    if df.empty:
                        logger.debug(f"è·³è¿‡ç©ºSheet: {sheet_name}")
                        continue

                    # ä½¿ç”¨Sheetåç§°ä½œä¸ºè¡¨åï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
                    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œå› ä¸ºDuckDBæ”¯æŒä¸­æ–‡è¡¨å
                    clean_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', sheet_name)
                    if not clean_table_name or clean_table_name[0].isdigit():
                        clean_table_name = f"sheet_{clean_table_name}"

                    conn.register(clean_table_name, df)
                    registered_tables.append(clean_table_name)
                    logger.info(f"æ³¨å†Œè¡¨ '{clean_table_name}' (æ¥è‡ªSheet '{sheet_name}'): {len(df)}è¡Œ")

                    # åŒæ—¶ç”¨åŸå§‹Sheetåæ³¨å†Œï¼ˆå¦‚æœä¸åŒï¼‰
                    if sheet_name != clean_table_name:
                        try:
                            conn.register(sheet_name, df)
                            registered_tables.append(sheet_name)
                        except Exception:
                            pass  # å¦‚æœåŸåæ³¨å†Œå¤±è´¥ï¼Œå¿½ç•¥

                except Exception as e:
                    logger.warning(f"è¯»å–Sheet '{sheet_name}' å¤±è´¥: {e}")
                    continue

            # å¦‚æœç¬¬ä¸€ä¸ªSheetå­˜åœ¨ï¼Œä¹Ÿç”¨æ•°æ®æºåç§°æ³¨å†Œï¼ˆå‘åå…¼å®¹ï¼‰
            if sheet_names:
                try:
                    # æ˜¾å¼æŒ‡å®š engine='openpyxl'
                    first_df = pd.read_excel(excel_file, sheet_name=0, engine='openpyxl')
                    if not first_df.empty:
                        ds_table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                        if ds_table_name not in registered_tables:
                            conn.register(ds_table_name, first_df)
                            registered_tables.append(ds_table_name)
                except Exception as e:
                    logger.warning(f"æ³¨å†Œæ•°æ®æºåç§°è¡¨å¤±è´¥: {e}")
                    pass

        elif db_type == "csv":
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ªè¡¨
            df = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                try:
                    df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if df is not None:
                table_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', data_source_name)
                if not table_name or table_name[0].isdigit():
                    table_name = f"data_{table_name}"
                conn.register(table_name, df)
                registered_tables.append(table_name)
                # åŒæ—¶æ³¨å†Œä¸º 'data' ä½œä¸ºå¤‡ç”¨
                conn.register('data', df)

        if not registered_tables:
            conn.close()
            return {
                "success": False,
                "error": f"æ— æ³•ä»æ–‡ä»¶è¯»å–ä»»ä½•æ•°æ®: {storage_path}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        logger.info(f"æˆåŠŸæ³¨å†Œ {len(registered_tables)} ä¸ªè¡¨: {registered_tables}")

        # æ‰§è¡ŒSQLæŸ¥è¯¢
        try:
            result_df = conn.execute(sql_query).fetchdf()
        except Exception as sql_error:
            error_msg = str(sql_error)
            logger.warning(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«å¯ç”¨çš„è¡¨å
            conn.close()
            return {
                "success": False,
                "error": f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}\n\nå¯ç”¨çš„è¡¨: {', '.join(registered_tables)}",
                "data": [],
                "columns": [],
                "row_count": 0
            }

        conn.close()

        # è½¬æ¢ç»“æœä¸ºå­—å…¸åˆ—è¡¨
        columns = list(result_df.columns)
        data = result_df.to_dict('records')

        logger.info(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(data)} è¡Œ")

        return {
            "success": True,
            "data": data,
            "columns": columns,
            "row_count": len(data)
        }

    except Exception as e:
        logger.error(f"æ–‡ä»¶æ•°æ®æºSQLæ‰§è¡Œå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": [],
            "columns": [],
            "row_count": 0
        }


async def _execute_sql_if_needed(
    content: str,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None
) -> str:
    """
    æ£€æµ‹AIå›å¤ä¸­çš„SQLæŸ¥è¯¢å¹¶æ‰§è¡Œï¼Œå°†ç»“æœæ’å…¥å›å¤ä¸­ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰

    Args:
        content: AIç”Ÿæˆçš„å›å¤å†…å®¹
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        data_source_ids: æŒ‡å®šçš„æ•°æ®æºIDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº

    Returns:
        å¢å¼ºåçš„å›å¤å†…å®¹ï¼ˆåŒ…å«æŸ¥è¯¢ç»“æœï¼‰
    """
    try:
        # æ£€æµ‹SQLä»£ç å—
        sql_pattern = r'```sql\s*(.*?)\s*```'
        sql_matches = re.findall(sql_pattern, content, re.DOTALL | re.IGNORECASE)

        if not sql_matches:
            return content

        # å»é‡ï¼šå¦‚æœæœ‰å¤šä¸ªç›¸åŒçš„SQLï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
        seen_sqls = set()
        unique_sql_matches = []
        for sql in sql_matches:
            normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
            if normalized_sql not in seen_sqls:
                seen_sqls.add(normalized_sql)
                unique_sql_matches.append(sql)
            else:
                logger.warning(f"æ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

        sql_matches = unique_sql_matches
        logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

        # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )

        if not data_sources:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
            return content + "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚"

        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
        if data_source_ids:
            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
            if matching_sources:
                data_source = matching_sources[0]
                logger.info(f"ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
            else:
                data_source = data_sources[0]
                logger.warning(f"æœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
        else:
            data_source = data_sources[0]

        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
        connection_string = await data_source_service.get_decrypted_connection_string(
            data_source_id=data_source.id,
            tenant_id=tenant_id,
            db=db
        )

        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢
        enhanced_content = content
        for sql_query in sql_matches:
            current_sql = sql_query.strip()
            retry_count = 0
            max_retries = 2
            last_error = None
            execution_success = False

            while retry_count <= max_retries and not execution_success:
                try:
                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢
                    if not current_sql.upper().startswith('SELECT'):
                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                        break

                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                        logger.info(f"ä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                        result = await _execute_sql_on_file_datasource(
                            connection_string=connection_string,
                            db_type=data_source.db_type,
                            sql_query=current_sql,
                            data_source_name=data_source.name
                        )
                        if not result.get("success", False) and result.get("error"):
                            raise Exception(result["error"])
                    else:
                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                        adapter = PostgreSQLAdapter(connection_string)
                        try:
                            await adapter.connect()
                            query_result = await adapter.execute_query(current_sql)
                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            result = {
                                "data": query_result.data,
                                "columns": query_result.columns,
                                "row_count": query_result.row_count
                            }
                        finally:
                            await adapter.disconnect()

                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                    row_count = len(result.get("data", []))

                    if result.get("data") and row_count > 0:
                        # è·å–åˆ—åï¼šä¼˜å…ˆä½¿ç”¨columnså­—æ®µï¼Œå¦åˆ™ä»æ•°æ®ä¸­æå–
                        columns = result.get("columns") or list(result["data"][0].keys())
                        # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                        result_text = "\n\n| " + " | ".join(columns) + " |\n"
                        result_text += "|" + "|".join(["---" for _ in columns]) + "|\n"

                        for row in result["data"][:10]:
                            row_values = [str(row.get(col, "")) for col in columns]
                            result_text += "| " + " | ".join(row_values) + " |\n"

                        # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                        if row_count > 10:
                            result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                    else:
                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ›¿æ¢ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                    if retry_count > 0:
                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                        # å®Œå…¨æ›¿æ¢åŸå§‹SQLå—ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                        sql_block = f"```sql\n{sql_query}\n```"
                        fixed_sql_block = f"**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            fixed_sql_block + result_text
                        )
                    else:
                        # æ²¡æœ‰é‡è¯•ï¼Œç›´æ¥å°†ç»“æœæ’å…¥åˆ°SQLä»£ç å—åé¢
                        sql_block = f"```sql\n{sql_query}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            sql_block + result_text
                        )

                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result.get('data', []))} è¡Œ")
                    execution_success = True

                except Exception as e:
                    last_error = str(e)
                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                    if retry_count < max_retries:
                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                        fixed_sql = await _fix_sql_with_ai(
                            original_sql=current_sql,
                            error_message=last_error,
                            schema_context=schema_context,
                            original_question=original_question
                        )

                        if fixed_sql:
                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                            current_sql = fixed_sql
                            retry_count += 1
                        else:
                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                            break
                    else:
                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                        break

            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if not execution_success and last_error:
                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                error_details = _parse_sql_error(last_error)

                # æ„å»ºé”™è¯¯ä¿¡æ¯
                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                if error_details.get('hint'):
                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                if retry_count > 0:
                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                # æ·»åŠ å»ºè®®
                if error_details.get('suggestion'):
                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                else:
                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                # æ›¿æ¢åŸå§‹SQLå—
                sql_block = f"```sql\n{sql_query}\n```"
                if retry_count > 0:
                    # å¦‚æœç»è¿‡é‡è¯•ï¼Œå®Œå…¨æ›¿æ¢ä¸ºé”™è¯¯ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºåŸå§‹SQLï¼‰
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        f"**âš ï¸ åŸå§‹SQLæœ‰è¯¯ï¼Œå°è¯•ä¿®å¤åä»ç„¶å¤±è´¥ï¼š**\n{error_text}"
                    )
                else:
                    # æ²¡æœ‰é‡è¯•ï¼Œåœ¨åŸå§‹SQLåæ·»åŠ é”™è¯¯ä¿¡æ¯
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        sql_block + error_text
                    )

        return enhanced_content

    except Exception as e:
        logger.error(f"SQLæ‰§è¡Œå¤„ç†å¤±è´¥: {e}")
        return content


def _convert_response(response: LLMResponse) -> ChatCompletionResponse:
    """è½¬æ¢å“åº”æ ¼å¼"""
    return ChatCompletionResponse(
        content=response.content,
        thinking=response.thinking,
        usage=response.usage,
        model=response.model,
        provider=response.provider,
        finish_reason=response.finish_reason,
        created_at=response.created_at
    )


async def _stream_response_generator(
    stream_generator,
    tenant_id: str,
    db: Session,
    original_question: str = "",
    data_source_ids: Optional[List[str]] = None
):
    """
    æµå¼å“åº”ç”Ÿæˆå™¨
    æ”¯æŒSQLæŸ¥è¯¢è‡ªåŠ¨æ‰§è¡Œï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
    """
    try:
        # æ”¶é›†å®Œæ•´çš„å“åº”å†…å®¹ç”¨äºSQLæ£€æµ‹
        full_content = ""
        thinking_content = ""

        async for chunk in stream_generator:
            # å‘é€åŸå§‹chunk
            chunk_data = {
                "type": chunk.type,
                "content": chunk.content,
                "provider": chunk.provider,
                "finished": chunk.finished,
                "tenant_id": tenant_id
            }
            yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

            # æ”¶é›†å†…å®¹
            if chunk.type == "content":
                full_content += chunk.content
            elif chunk.type == "thinking":
                thinking_content += chunk.content

            # å¦‚æœæµç»“æŸï¼Œæ£€æµ‹å¹¶æ‰§è¡ŒSQL
            if chunk.finished and chunk.type == "content":
                logger.info(f"æµå¼å“åº”å®Œæˆï¼Œæ£€æµ‹SQLæŸ¥è¯¢ã€‚å†…å®¹é•¿åº¦: {len(full_content)}")

                # æ£€æµ‹SQLä»£ç å—
                sql_pattern = r'```sql\s*(.*?)\s*```'
                sql_matches = re.findall(sql_pattern, full_content, re.DOTALL | re.IGNORECASE)

                if sql_matches:
                    # å»é‡ï¼šå¦‚æœæœ‰å¤šä¸ªç›¸åŒçš„SQLï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
                    seen_sqls = set()
                    unique_sql_matches = []
                    for sql in sql_matches:
                        normalized_sql = sql.strip().upper()  # æ ‡å‡†åŒ–æ¯”è¾ƒ
                        if normalized_sql not in seen_sqls:
                            seen_sqls.add(normalized_sql)
                            unique_sql_matches.append(sql)
                        else:
                            logger.warning(f"æµå¼å“åº”ï¼šæ£€æµ‹åˆ°é‡å¤SQLï¼Œå·²è·³è¿‡: {sql[:50]}...")

                    sql_matches = unique_sql_matches
                    logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªå”¯ä¸€SQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

                    # è·å–ç§Ÿæˆ·çš„æ´»è·ƒæ•°æ®æº
                    data_sources = await data_source_service.get_data_sources(
                        tenant_id=tenant_id,
                        db=db,
                        active_only=True
                    )

                    if data_sources:
                        # å¦‚æœæŒ‡å®šäº†æ•°æ®æºIDï¼Œä½¿ç”¨æŒ‡å®šçš„ç¬¬ä¸€ä¸ªï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                        if data_source_ids:
                            matching_sources = [ds for ds in data_sources if ds.id in data_source_ids]
                            if matching_sources:
                                data_source = matching_sources[0]
                                logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨æŒ‡å®šçš„æ•°æ®æº: {data_source.name} ({data_source.id})")
                            else:
                                data_source = data_sources[0]
                                logger.warning(f"æµå¼å“åº”ï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: {data_source.name}")
                        else:
                            data_source = data_sources[0]

                        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
                        connection_string = await data_source_service.get_decrypted_connection_string(
                            data_source_id=data_source.id,
                            tenant_id=tenant_id,
                            db=db
                        )

                        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
                        schema_context = await _get_data_sources_context(tenant_id, db, data_source_ids)

                        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
                        for sql_query in sql_matches:
                            current_sql = sql_query.strip()
                            retry_count = 0
                            max_retries = 2
                            last_error = None
                            execution_success = False

                            while retry_count <= max_retries and not execution_success:
                                try:
                                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢
                                    if not current_sql.upper().startswith('SELECT'):
                                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                                        break

                                    # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
                                    if data_source.db_type in ["xlsx", "xls", "csv"]:
                                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä½¿ç”¨duckdbæ‰§è¡Œ
                                        logger.info(f"æµå¼å“åº”ï¼šä½¿ç”¨duckdbæ‰§è¡Œæ–‡ä»¶æ•°æ®æºæŸ¥è¯¢: {data_source.db_type}")
                                        result = await _execute_sql_on_file_datasource(
                                            connection_string=connection_string,
                                            db_type=data_source.db_type,
                                            sql_query=current_sql,
                                            data_source_name=data_source.name
                                        )
                                        if not result.get("success", False) and result.get("error"):
                                            raise Exception(result["error"])
                                    else:
                                        # æ•°æ®åº“ç±»å‹æ•°æ®æºï¼šä½¿ç”¨PostgreSQLAdapter
                                        adapter = PostgreSQLAdapter(connection_string)
                                        try:
                                            await adapter.connect()
                                            query_result = await adapter.execute_query(current_sql)
                                            # å°†QueryResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                                            result = {
                                                "data": query_result.data,
                                                "columns": query_result.columns,
                                                "row_count": query_result.row_count
                                            }
                                        finally:
                                            await adapter.disconnect()

                                    # æ ¼å¼åŒ–ç»“æœ - ç®€æ´ç‰ˆ
                                    row_count = result.get('row_count', 0)

                                    if result.get('data'):
                                        data = result['data'][:10]
                                        if data:
                                            headers = list(data[0].keys())
                                            # æ„å»ºç®€æ´çš„Markdownè¡¨æ ¼
                                            result_text = "\n\n| " + " | ".join(headers) + " |\n"
                                            result_text += "|" + "|".join(["---"] * len(headers)) + "|\n"

                                            for row in data:
                                                values = [str(row.get(h, '')) for h in headers]
                                                result_text += "| " + " | ".join(values) + " |\n"

                                            # åªæœ‰å½“è¿”å›è¡Œæ•°è¶…è¿‡10è¡Œæ—¶æ‰æ˜¾ç¤ºæç¤º
                                            if row_count > 10:
                                                result_text += f"\n*ï¼ˆå…±{row_count}è¡Œï¼Œä»…æ˜¾ç¤ºå‰10è¡Œï¼‰*\n"
                                        else:
                                            result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"
                                    else:
                                        result_text = "\n\n*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n"

                                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œå‘é€ä¿®å¤è¯´æ˜
                                    if retry_count > 0:
                                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                                        # å‘é€ä¿®å¤åçš„SQL
                                        fix_info = f"\n\n**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```\n"
                                        fix_chunk = {
                                            "type": "content",
                                            "content": fix_info,
                                            "provider": chunk.provider,
                                            "finished": False,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(fix_chunk, ensure_ascii=False)}\n\n"

                                    # å‘é€æŸ¥è¯¢ç»“æœä½œä¸ºæ–°çš„content chunk
                                    result_chunk = {
                                        "type": "content",
                                        "content": result_text,
                                        "provider": chunk.provider,
                                        "finished": False,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(result_chunk, ensure_ascii=False)}\n\n"

                                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {result.get('row_count', 0)} è¡Œ")
                                    execution_success = True

                                except Exception as e:
                                    last_error = str(e)
                                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                                    if retry_count < max_retries:
                                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                                        fixed_sql = await _fix_sql_with_ai(
                                            original_sql=current_sql,
                                            error_message=last_error,
                                            schema_context=schema_context,
                                            original_question=original_question
                                        )

                                        if fixed_sql:
                                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                                            current_sql = fixed_sql
                                            retry_count += 1
                                        else:
                                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                                            break
                                    else:
                                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                                        break

                            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                            if not execution_success and last_error:
                                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                                error_details = _parse_sql_error(last_error)

                                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                                if error_details.get('hint'):
                                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                                if retry_count > 0:
                                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                                # æ·»åŠ å»ºè®®
                                if error_details.get('suggestion'):
                                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                                else:
                                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                                error_chunk = {
                                    "type": "content",
                                    "content": error_text,
                                    "provider": chunk.provider,
                                    "finished": False,
                                    "tenant_id": tenant_id
                                }
                                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                    else:
                        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
                        warning_text = "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚\n"

                        warning_chunk = {
                            "type": "content",
                            "content": warning_text,
                            "provider": chunk.provider,
                            "finished": False,
                            "tenant_id": tenant_id
                        }
                        yield f"data: {json.dumps(warning_chunk, ensure_ascii=False)}\n\n"

        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå™¨é”™è¯¯: {e}")
        error_data = {
            "type": "error",
            "content": f"Stream error: {str(e)}",
            "provider": "unknown",
            "finished": True,
            "tenant_id": tenant_id
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


@router.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completion(
    request: ChatCompletionRequest,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    èŠå¤©å®Œæˆæ¥å£
    æ”¯æŒå¤šæä¾›å•†ã€å¤šæ¨¡æ€å’Œæµå¼è¾“å‡º
    è‡ªåŠ¨è·å–ç”¨æˆ·æ•°æ®æºä¿¡æ¯å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    """
    try:
        # è·å–tenant_idï¼Œæ”¯æŒå¼€å‘ç¯å¢ƒä¸‹çš„é»˜è®¤ç§Ÿæˆ·
        tenant_id = getattr(current_user, 'tenant_id', None) or current_user.get('tenant_id', 'default_tenant')
        logger.info(f"Chat completion request for tenant: {tenant_id}")

        # è½¬æ¢æä¾›å•†
        provider = None
        if request.provider:
            try:
                provider = LLMProvider(request.provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {request.provider}"
                )

        # è·å–æ•°æ®æºä¸Šä¸‹æ–‡ï¼Œå¦‚æœæŒ‡å®šäº†æ•°æ®æºIDåˆ™åªè·å–æŒ‡å®šçš„æ•°æ®æº
        print(f"[DEBUG] Starting _get_data_sources_context for tenant: {tenant_id}, data_source_ids: {request.data_source_ids}")
        import time as _time
        _ctx_start = _time.time()
        data_sources_context = await _get_data_sources_context(tenant_id, db, request.data_source_ids)
        print(f"[DEBUG] _get_data_sources_context took {_time.time() - _ctx_start:.2f}s")
        if data_sources_context:
            logger.info(f"Data sources context retrieved for tenant {tenant_id}, length: {len(data_sources_context)}")
            # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æ•°æ®æºä¸Šä¸‹æ–‡çš„å‰1000ä¸ªå­—ç¬¦
            logger.debug(f"Data sources context content (first 1000 chars): {data_sources_context[:1000]}")
        else:
            logger.info(f"No data sources found for tenant {tenant_id}")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = _convert_chat_messages(request.messages)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ åŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„systemæ¶ˆæ¯
        has_system_message = any(msg.role == "system" for msg in messages)
        if not has_system_message:
            system_prompt = _build_system_prompt_with_context(data_sources_context)
            system_message = LLMMessage(role="system", content=system_prompt)
            messages.insert(0, system_message)
            logger.info("Added system message with data sources context")
        elif data_sources_context:
            # å¦‚æœå·²æœ‰systemæ¶ˆæ¯ï¼Œæ›¿æ¢ä¸ºå®Œæ•´çš„æ•°æ®åˆ†æç³»ç»Ÿæç¤ºï¼ˆåŒ…å«SQLç”ŸæˆæŒ‡ä»¤ï¼‰
            # è¿™æ ·ç¡®ä¿AIçŸ¥é“å¦‚ä½•æ­£ç¡®ç”ŸæˆSQLæŸ¥è¯¢
            full_system_prompt = _build_system_prompt_with_context(data_sources_context)
            for i, msg in enumerate(messages):
                if msg.role == "system":
                    messages[i] = LLMMessage(role="system", content=full_system_prompt, thinking=msg.thinking)
                    logger.info("Replaced existing system message with full data sources context and SQL instructions")
                    break

        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºåŸå§‹é—®é¢˜
        original_question = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                original_question = msg.content
                break

        # è°ƒç”¨LLMæœåŠ¡
        if request.stream:
            # æµå¼å“åº”
            response_generator = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )

            return StreamingResponse(
                _stream_response_generator(response_generator, tenant_id, db, original_question, request.data_source_ids),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "Cache-Control"
                }
            )
        else:
            # éæµå¼å“åº”
            llm_start = time.time()
            response = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )
            logger.info(f"[PERF] llm_service.chat_completion took {time.time() - llm_start:.2f}s")

            if isinstance(response, LLMResponse):
                # æ£€æµ‹å¹¶æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨ä¹‹å‰æå–çš„åŸå§‹é—®é¢˜å’ŒæŒ‡å®šçš„æ•°æ®æºï¼‰
                enhanced_content = await _execute_sql_if_needed(
                    response.content,
                    tenant_id,
                    db,
                    original_question,
                    request.data_source_ids
                )

                # æ›´æ–°å“åº”å†…å®¹
                response.content = enhanced_content

                return _convert_response(response)
            else:
                raise HTTPException(
                    status_code=500,
                    detail="Unexpected response type from LLM service"
                )

    except Exception as e:
        logger.error(f"Chat completion failed for tenant {tenant_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chat completion failed: {str(e)}"
        )


@router.get("/providers/status", response_model=ProviderStatusResponse)
async def get_provider_status(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–LLMæä¾›å•†çŠ¶æ€
    """
    try:
        tenant_id = current_user.tenant_id
        status = await llm_service.validate_providers(tenant_id)

        return ProviderStatusResponse(
            zhipu=status.get("zhipu", False),
            openrouter=status.get("openrouter", False)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get provider status: {str(e)}"
        )


@router.get("/models", response_model=AvailableModelsResponse)
async def get_available_models(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    try:
        tenant_id = current_user.tenant_id
        models = await llm_service.get_available_models(tenant_id)

        return AvailableModelsResponse(providers=models)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get available models: {str(e)}"
        )


@router.post("/test")
async def test_llm_service(
    provider: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•LLMæœåŠ¡
    """
    try:
        tenant_id = current_user.tenant_id

        # è½¬æ¢æä¾›å•†
        llm_provider = None
        if provider:
            try:
                llm_provider = LLMProvider(provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {provider}"
                )

        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        test_messages = [
            LLMMessage(
                role="user",
                content="ä½ å¥½ï¼Œè¯·å›å¤ä¸€æ¡ç®€çŸ­çš„æµ‹è¯•æ¶ˆæ¯"
            )
        ]

        # è°ƒç”¨LLMæœåŠ¡
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=llm_provider,
            max_tokens=50
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "LLM service test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"LLM service test failed: {str(e)}"
        }


@router.post("/test/multimodal")
async def test_multimodal(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€åŠŸèƒ½ï¼ˆéœ€è¦OpenRouterï¼‰
    """
    try:
        tenant_id = current_user.tenant_id

        # åˆ›å»ºå¤šæ¨¡æ€æµ‹è¯•æ¶ˆæ¯
        multimodal_content = [
            {
                "type": "text",
                "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            }
        ]

        test_messages = [
            LLMMessage(
                role="user",
                content=multimodal_content
            )
        ]

        # ä¼˜å…ˆä½¿ç”¨OpenRouterè¿›è¡Œå¤šæ¨¡æ€æµ‹è¯•
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=LLMProvider.OPENROUTER,
            max_tokens=200
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "Multimodal test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"Multimodal test failed: {str(e)}"
        }


# ===== æ–°å¢çš„é«˜çº§LLMåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹ =====

@router.get("/test/stream-thinking")
async def test_stream_thinking(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
    """
    try:
        from src.app.services.llm_service import LLMMessage

        messages = [
            LLMMessage(
                role="user",
                content="è¯·è¯¦ç»†åˆ†ææœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨åœºæ™¯"
            )
        ]

        async def stream_generator():
            try:
                async for chunk in llm_service.chat_completion(
                    tenant_id=current_user.id,
                    messages=messages,
                    stream=True,
                    enable_thinking=None  # è‡ªåŠ¨åˆ¤æ–­
                ):
                    yield f"data: {json.dumps(chunk.dict(), ensure_ascii=False)}\n\n"

                yield "data: [DONE]\n\n"
            except Exception as e:
                yield f"data: {{'type': 'error', 'content': '{str(e)}'}}\n\n"

        return StreamingResponse(
            stream_generator(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stream thinking test failed: {str(e)}")


@router.get("/test/intelligent-params")
async def test_intelligent_params(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æ™ºèƒ½å‚æ•°è°ƒæ•´åŠŸèƒ½
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„é—®é¢˜
        test_cases = [
            {
                "name": "ç®€å•é—®é¢˜",
                "messages": [LLMMessage(role="user", content="ä½ å¥½")]
            },
            {
                "name": "å¤æ‚é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†ã€å¸¸è§çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒæŠ€å·§ä»¥åŠåœ¨å®é™…é¡¹ç›®ä¸­çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¹¶è®¨è®ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
                )]
            },
            {
                "name": "éœ€è¦æ€è€ƒçš„é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="ä¸ºä»€ä¹ˆTransformeræ¶æ„èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„RNNæ¨¡å‹ï¼Ÿè¯·ä»æ³¨æ„åŠ›æœºåˆ¶ã€å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€é•¿æœŸä¾èµ–å¤„ç†ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œæ·±å…¥åˆ†æã€‚"
                )]
            }
        ]

        results = []

        for case in test_cases:
            complexity = llm_service.analyze_conversation_complexity(case["messages"])

            # ä½¿ç”¨æ™ºèƒ½å‚æ•°è°ƒç”¨
            response = await llm_service.chat_completion(
                tenant_id=current_user.id,
                messages=case["messages"],
                enable_thinking=None,  # è‡ªåŠ¨åˆ¤æ–­
                temperature=complexity.get("recommend_temperature", 0.7),
                max_tokens=complexity.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
            )

            results.append({
                "case_name": case["name"],
                "complexity_analysis": complexity,
                "response_type": type(response).__name__,
                "success": response is not None
            })

        return {
            "success": True,
            "message": "Intelligent parameter adjustment test completed",
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Intelligent params test failed: {str(e)}")


@router.get("/test/multimodal-upload")
async def test_multimodal_upload(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€å†…å®¹å¤„ç†å’ŒMinIOä¸Šä¼ 
    """
    try:
        from src.app.services.llm_service import LLMMessage
        from src.app.services.multimodal_processor import multimodal_processor

        # æ„å»ºåŒ…å«å›¾ç‰‡URLçš„æµ‹è¯•æ¶ˆæ¯
        test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

        messages = [
            LLMMessage(
                role="user",
                content=[
                    {
                        "type": "text",
                        "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": test_image_url
                        }
                    }
                ]
            )
        ]

        # æµ‹è¯•å¤šæ¨¡æ€å¤„ç†
        processed_content = await multimodal_processor.process_content_list(
            messages[0].content,
            current_user.id
        )

        # å°è¯•è°ƒç”¨OpenRouterï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            provider=LLMProvider.OPENROUTER,
            stream=False
        )

        return {
            "success": True,
            "message": "Multimodal upload test completed",
            "original_content_count": len(messages[0].content),
            "processed_content_count": len(processed_content),
            "response_received": response is not None,
            "processed_sample": processed_content[:2] if processed_content else []
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Multimodal upload test failed: {str(e)}")


@router.get("/test/tenant-isolation")
async def test_tenant_isolation(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•ç§Ÿæˆ·éš”ç¦»åŠŸèƒ½
    """
    try:
        from src.app.services.tenant_config_manager import tenant_config_manager, ProviderType

        # æµ‹è¯•ç§Ÿæˆ·é…ç½®è·å–
        test_tenant_id = current_user.id
        providers = []

        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            api_key = await tenant_config_manager.get_tenant_api_key(
                test_tenant_id, provider, use_global_fallback=True
            )
            if api_key:
                providers.append(provider.value)

        # æµ‹è¯•æ¨¡å‹é…ç½®è·å–
        model_configs = {}
        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            config = await tenant_config_manager.get_tenant_model_config(test_tenant_id, provider)
            model_configs[provider.value] = config

        # éªŒè¯ç§Ÿæˆ·é…ç½®
        validation_results = await tenant_config_manager.validate_tenant_config(test_tenant_id)

        return {
            "success": True,
            "message": "Tenant isolation test completed",
            "tenant_id": test_tenant_id,
            "available_providers": providers,
            "model_configs": model_configs,
            "validation_results": validation_results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tenant isolation test failed: {str(e)}")


@router.get("/test/data-sources-context")
async def test_data_sources_context(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•æ•°æ®æºä¸Šä¸‹æ–‡è·å–
    """
    tenant_id = current_user.get("tenant_id", "")
    context = await _get_data_sources_context(tenant_id, db)
    return {
        "tenant_id": tenant_id,
        "context_length": len(context),
        "context": context
    }


@router.get("/test/all-features")
async def test_all_features(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    ç»¼åˆåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æ„å»ºå¤æ‚çš„æµ‹è¯•åœºæ™¯
        messages = [
            LLMMessage(
                role="user",
                content="ä½œä¸ºä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·åˆ†æå’Œè¯„ä¼°å½“å‰å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„å‘å±•ç°çŠ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ¶æ„ã€åº”ç”¨åœºæ™¯ã€ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å‘å±•è¶‹åŠ¿ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„å»ºè®®ã€‚"
            )
        ]

        # è·å–å¯¹è¯å¤æ‚åº¦åˆ†æ
        complexity_analysis = llm_service.analyze_conversation_complexity(messages)

        # è°ƒç”¨èŠå¤©å®Œæˆï¼ˆå¯ç”¨æ™ºèƒ½æ€è€ƒæ¨¡å¼ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            stream=False,
            enable_thinking=None,  # è‡ªåŠ¨å¯ç”¨æ€è€ƒæ¨¡å¼
            temperature=complexity_analysis.get("recommend_temperature", 0.7),
            max_tokens=complexity_analysis.get("recommend_max_tokens", getattr(settings, "llm_max_output_tokens", 8192))
        )

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = await llm_service.get_available_models(current_user.id)

        # éªŒè¯æä¾›å•†çŠ¶æ€
        provider_status = await llm_service.validate_providers(current_user.id)

        return {
            "success": True,
            "message": "Comprehensive LLM features test completed",
            "complexity_analysis": complexity_analysis,
            "response_received": response is not None,
            "available_models": available_models,
            "provider_status": provider_status,
            "features_tested": [
                "æ™ºèƒ½æ€è€ƒæ¨¡å¼",
                "å¤æ‚åº¦åˆ†æ",
                "æ™ºèƒ½å‚æ•°è°ƒæ•´",
                "å¤šæä¾›å•†æ”¯æŒ",
                "ç§Ÿæˆ·éš”ç¦»"
            ]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Comprehensive test failed: {str(e)}")