"""
LLM APIç«¯ç‚¹
æä¾›ç»Ÿä¸€çš„èŠå¤©å®Œæˆã€æµå¼è¾“å‡ºå’Œå¤šæ¨¡æ€æ”¯æŒ
"""

import json
import asyncio
import logging
import io
from typing import Dict, Any, Optional, List, Union
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import pandas as pd

from src.app.services.llm_service import (
    llm_service,
    LLMProvider,
    LLMMessage,
    LLMResponse,
    LLMStreamChunk
)
from src.app.core.auth import get_current_user_with_tenant
from src.app.data.models import Tenant, DataSourceConnection, DataSourceConnectionStatus
from src.app.data.database import get_db
from src.app.services.data_source_service import data_source_service
from src.app.services.minio_client import minio_service
from src.services.database_interface import PostgreSQLAdapter
from src.app.services.zhipu_client import zhipu_service
import re

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["LLM"])


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser, assistant, system")
    content: Union[str, List[Dict[str, Any]]] = Field(
        ...,
        description="æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€å†…å®¹"
    )
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹ï¼ˆä»…assistantè§’è‰²ï¼‰")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹"""
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    provider: Optional[str] = Field(
        None,
        description="LLMæä¾›å•†ï¼šzhipu, openrouterï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©"
    )
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§è¾“å‡ºtokens")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•°(0-1)")
    stream: bool = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
    enable_thinking: bool = Field(False, description="æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä»…Zhipuæ”¯æŒï¼‰")


class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”æ¨¡å‹"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    thinking: Optional[str] = Field(None, description="æ€è€ƒè¿‡ç¨‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨æƒ…å†µ")
    model: Optional[str] = Field(None, description="ä½¿ç”¨çš„æ¨¡å‹")
    provider: Optional[str] = Field(None, description="ä½¿ç”¨çš„æä¾›å•†")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")
    created_at: Optional[str] = Field(None, description="åˆ›å»ºæ—¶é—´")


class ProviderStatusResponse(BaseModel):
    """æä¾›å•†çŠ¶æ€å“åº”æ¨¡å‹"""
    zhipu: bool = Field(..., description="æ™ºè°±AIå¯ç”¨æ€§")
    openrouter: bool = Field(..., description="OpenRouterå¯ç”¨æ€§")


class AvailableModelsResponse(BaseModel):
    """å¯ç”¨æ¨¡å‹å“åº”æ¨¡å‹"""
    providers: Dict[str, List[str]] = Field(..., description="å„æä¾›å•†çš„å¯ç”¨æ¨¡å‹")


def _convert_chat_messages(messages: List[ChatMessage]) -> List[LLMMessage]:
    """è½¬æ¢èŠå¤©æ¶ˆæ¯æ ¼å¼"""
    llm_messages = []
    for msg in messages:
        llm_messages.append(LLMMessage(
            role=msg.role,
            content=msg.content,
            thinking=msg.thinking
        ))
    return llm_messages


async def _get_file_schema(connection_string: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶æ•°æ®æºè·å–schemaä¿¡æ¯

    Args:
        connection_string: æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼ˆæ ¼å¼: file://data-sources/{tenant_id}/{file_id}.xlsxï¼‰
        db_type: æ–‡ä»¶ç±»å‹ï¼ˆxlsx, csv, xlsç­‰ï¼‰
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«åˆ—åã€ç±»å‹å’Œç¤ºä¾‹æ•°æ®
    """
    try:
        # è§£æå­˜å‚¨è·¯å¾„
        if connection_string.startswith("file://"):
            storage_path = connection_string[7:]  # å»æ‰ "file://" å‰ç¼€
        else:
            storage_path = connection_string

        # ä»MinIOä¸‹è½½æ–‡ä»¶
        logger.info(f"ä»MinIOä¸‹è½½æ–‡ä»¶: {storage_path}")
        file_data = minio_service.download_file(
            bucket_name="data-sources",
            object_name=storage_path
        )

        if not file_data:
            logger.warning(f"æ— æ³•ä»MinIOè·å–æ–‡ä»¶: {storage_path}")
            return {}

        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ•°æ®
        df = None
        if db_type in ["xlsx", "xls"]:
            df = pd.read_excel(io.BytesIO(file_data), sheet_name=0)
        elif db_type == "csv":
            # å°è¯•ä¸åŒç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                try:
                    df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

        if df is None:
            logger.warning(f"æ— æ³•è§£ææ–‡ä»¶: {storage_path}")
            return {}

        # æ„å»ºschemaä¿¡æ¯
        columns = []
        for col in df.columns:
            dtype = str(df[col].dtype)
            # è½¬æ¢pandasç±»å‹åˆ°æ›´å‹å¥½çš„æè¿°
            if 'int' in dtype:
                col_type = 'integer'
            elif 'float' in dtype:
                col_type = 'float'
            elif 'datetime' in dtype:
                col_type = 'datetime'
            elif 'bool' in dtype:
                col_type = 'boolean'
            else:
                col_type = 'text'

            columns.append({
                "name": str(col),
                "type": col_type,
                "nullable": df[col].isnull().any()
            })

        # è·å–ç¤ºä¾‹æ•°æ®ï¼ˆå‰5è¡Œï¼‰
        sample_rows = []
        for _, row in df.head(5).iterrows():
            row_data = {}
            for col in df.columns[:10]:  # é™åˆ¶åˆ—æ•°
                value = row[col]
                if pd.isna(value):
                    row_data[str(col)] = None
                else:
                    row_data[str(col)] = str(value)
            sample_rows.append(row_data)

        schema_info = {
            "tables": [{
                "name": data_source_name,
                "columns": columns,
                "row_count": len(df)
            }],
            "sample_data": {
                data_source_name: {
                    "columns": [str(c) for c in df.columns[:10]],
                    "data": sample_rows
                }
            }
        }

        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶schema: {len(columns)}åˆ—, {len(df)}è¡Œ")
        return schema_info

    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶schemaå¤±è´¥: {e}")
        return {}


async def _try_find_file_in_minio(tenant_id: str, data_source_id: str, db_type: str) -> Optional[str]:
    """
    å°è¯•åœ¨MinIOä¸­æŸ¥æ‰¾æ•°æ®æºå¯¹åº”çš„æ–‡ä»¶

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹

    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
        prefix = f"{tenant_id}/{data_source_id}"
        ext = f".{db_type}"

        # åˆ—å‡ºMinIOä¸­çš„æ–‡ä»¶
        objects = minio_service.list_files(
            bucket_name="data-sources",
            prefix=prefix
        )

        for obj in objects:
            obj_name = obj.object_name if hasattr(obj, 'object_name') else str(obj)
            if obj_name.endswith(ext):
                return f"file://{obj_name}"

        logger.warning(f"åœ¨MinIOä¸­æœªæ‰¾åˆ°æ•°æ®æº {data_source_id} çš„æ–‡ä»¶")
        return None

    except Exception as e:
        logger.error(f"åœ¨MinIOä¸­æœç´¢æ–‡ä»¶å¤±è´¥: {e}")
        return None


async def _try_get_file_schema_fallback(tenant_id: str, data_source_id: str, db_type: str, data_source_name: str) -> Dict[str, Any]:
    """
    å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ä»MinIOç›´æ¥è·å–æ–‡ä»¶schema

    Args:
        tenant_id: ç§Ÿæˆ·ID
        data_source_id: æ•°æ®æºID
        db_type: æ–‡ä»¶ç±»å‹
        data_source_name: æ•°æ®æºåç§°

    Returns:
        schemaä¿¡æ¯å­—å…¸
    """
    try:
        # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
        ext = f".{db_type}"
        possible_paths = [
            f"{tenant_id}/{data_source_id}{ext}",
            f"{tenant_id}/{data_source_name}{ext}",
        ]

        for path in possible_paths:
            try:
                logger.info(f"å°è¯•ä»MinIOè·å–æ–‡ä»¶: {path}")
                file_data = minio_service.download_file(
                    bucket_name="data-sources",
                    object_name=path
                )

                if file_data:
                    # æˆåŠŸè·å–æ–‡ä»¶ï¼Œè§£æschema
                    df = None
                    if db_type in ["xlsx", "xls"]:
                        df = pd.read_excel(io.BytesIO(file_data), sheet_name=0)
                    elif db_type == "csv":
                        for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030']:
                            try:
                                df = pd.read_csv(io.BytesIO(file_data), encoding=encoding)
                                break
                            except UnicodeDecodeError:
                                continue

                    if df is not None:
                        # æ„å»ºschemaä¿¡æ¯
                        columns = []
                        for col in df.columns:
                            dtype = str(df[col].dtype)
                            if 'int' in dtype:
                                col_type = 'integer'
                            elif 'float' in dtype:
                                col_type = 'float'
                            elif 'datetime' in dtype:
                                col_type = 'datetime'
                            elif 'bool' in dtype:
                                col_type = 'boolean'
                            else:
                                col_type = 'text'

                            columns.append({
                                "name": str(col),
                                "type": col_type,
                                "nullable": df[col].isnull().any()
                            })

                        sample_rows = []
                        for _, row in df.head(5).iterrows():
                            row_data = {}
                            for col in df.columns[:10]:
                                value = row[col]
                                if pd.isna(value):
                                    row_data[str(col)] = None
                                else:
                                    row_data[str(col)] = str(value)
                            sample_rows.append(row_data)

                        logger.info(f"å¤‡é€‰æ–¹æ¡ˆæˆåŠŸè·å–schema: {len(columns)}åˆ—, {len(df)}è¡Œ")
                        return {
                            "tables": [{
                                "name": data_source_name,
                                "columns": columns,
                                "row_count": len(df)
                            }],
                            "sample_data": {
                                data_source_name: {
                                    "columns": [str(c) for c in df.columns[:10]],
                                    "data": sample_rows
                                }
                            }
                        }
            except Exception as e:
                logger.debug(f"å°è¯•è·¯å¾„ {path} å¤±è´¥: {e}")
                continue

        logger.warning(f"å¤‡é€‰æ–¹æ¡ˆæœªèƒ½è·å–æ•°æ®æº {data_source_name} çš„schema")
        return {}

    except Exception as e:
        logger.error(f"å¤‡é€‰æ–¹æ¡ˆè·å–schemaå¤±è´¥: {e}")
        return {}


async def _get_data_sources_context(tenant_id: str, db: Session) -> str:
    """
    è·å–ç§Ÿæˆ·æ•°æ®æºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬schemaï¼‰

    Args:
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        æ•°æ®æºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    try:
        # è·å–ç§Ÿæˆ·çš„æ‰€æœ‰æ´»è·ƒæ•°æ®æº
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )

        if not data_sources:
            return ""

        context_parts = []
        context_parts.append("## å¯ç”¨æ•°æ®æº\n")

        for ds in data_sources:
            try:
                schema_info = None
                connection_string = None

                # å°è¯•è·å–è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
                try:
                    connection_string = await data_source_service.get_decrypted_connection_string(
                        data_source_id=ds.id,
                        tenant_id=tenant_id,
                        db=db
                    )
                except Exception as decrypt_error:
                    logger.warning(f"è§£å¯†æ•°æ®æº {ds.name} è¿æ¥å­—ç¬¦ä¸²å¤±è´¥: {decrypt_error}")
                    # å¯¹äºæ–‡ä»¶ç±»å‹æ•°æ®æºï¼Œå°è¯•ä»MinIOç›´æ¥æœç´¢æ–‡ä»¶
                    if ds.db_type in ["xlsx", "xls", "csv"]:
                        connection_string = await _try_find_file_in_minio(tenant_id, ds.id, ds.db_type)

                # æ ¹æ®æ•°æ®æºç±»å‹è·å–schema
                if ds.db_type == "postgresql" and connection_string:
                    adapter = PostgreSQLAdapter()
                    schema_info = await adapter.get_schema(connection_string)

                elif ds.db_type in ["xlsx", "xls", "csv"]:
                    if connection_string:
                        # æ–‡ä»¶ç±»å‹æ•°æ®æºï¼šä»MinIOè¯»å–æ–‡ä»¶å¹¶è§£æschema
                        schema_info = await _get_file_schema(connection_string, ds.db_type, ds.name)
                    else:
                        # è¿æ¥å­—ç¬¦ä¸²è·å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
                        logger.info(f"å°è¯•å¤‡é€‰æ–¹æ¡ˆè·å–æ•°æ®æº {ds.name} çš„schema")
                        schema_info = await _try_get_file_schema_fallback(tenant_id, ds.id, ds.db_type, ds.name)

                if schema_info and schema_info.get("tables"):
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ–‡ä»¶/æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")

                    # æ·»åŠ è¡¨ä¿¡æ¯
                    context_parts.append("\n#### è¡¨ç»“æ„:")
                    for table in schema_info["tables"][:20]:  # é™åˆ¶è¡¨æ•°é‡é¿å…tokenè¿‡å¤š
                        table_name = table.get("name", "unknown")
                        row_count = table.get("row_count", "æœªçŸ¥")
                        context_parts.append(f"\n**è¡¨: {table_name}** (å…±{row_count}è¡Œ)")

                        columns = table.get("columns", [])
                        if columns:
                            col_info = []
                            for col in columns[:30]:  # é™åˆ¶åˆ—æ•°é‡
                                col_name = col.get("name", "unknown")
                                col_type = col.get("type", "unknown")
                                nullable = "å¯ç©º" if col.get("nullable") else "éç©º"
                                col_info.append(f"  - {col_name} ({col_type}, {nullable})")
                            context_parts.append("\n".join(col_info))

                        # æ·»åŠ ä¸»é”®ä¿¡æ¯
                        if table.get("primary_key"):
                            context_parts.append(f"  - ä¸»é”®: {', '.join(table['primary_key'])}")

                        # æ·»åŠ å¤–é”®ä¿¡æ¯
                        if table.get("foreign_keys"):
                            for fk in table["foreign_keys"]:
                                context_parts.append(
                                    f"  - å¤–é”®: {fk['column']} -> {fk['references_table']}.{fk['references_column']}"
                                )

                    # æ·»åŠ è¡¨å…³ç³»ä¿¡æ¯ï¼ˆå¤–é”®å…³è”ï¼‰
                    relationships = schema_info.get("relationships", [])
                    if relationships:
                        context_parts.append("\n#### è¡¨å…³ç³»ï¼ˆå¤–é”®ï¼‰:")
                        context_parts.append("**é‡è¦ï¼š** ä»¥ä¸‹æ˜¯è¡¨ä¹‹é—´çš„å…³è”å…³ç³»ï¼ŒæŸ¥è¯¢æ—¶å¿…é¡»é€šè¿‡è¿™äº›å¤–é”®è¿›è¡ŒJOINï¼š")
                        for rel in relationships:
                            from_table = rel.get("from_table", "unknown")
                            from_column = rel.get("from_column", "unknown")
                            to_table = rel.get("to_table", "unknown")
                            to_column = rel.get("to_column", "unknown")
                            context_parts.append(
                                f"  - {from_table}.{from_column} -> {to_table}.{to_column}"
                            )

                    # æ·»åŠ ç¤ºä¾‹æ•°æ®
                    sample_data = schema_info.get("sample_data", {})
                    if sample_data:
                        context_parts.append("\n#### ç¤ºä¾‹æ•°æ®:")
                        for table_name, samples in list(sample_data.items())[:5]:  # é™åˆ¶è¡¨æ•°é‡
                            if samples.get("data"):
                                context_parts.append(f"\n**{table_name}** (å‰5è¡Œ):")
                                for row in samples["data"][:3]:  # é™åˆ¶è¡Œæ•°
                                    row_str = ", ".join([f"{k}={v}" for k, v in list(row.items())[:5]])
                                    context_parts.append(f"  {row_str}")
                else:
                    # å…¶ä»–æ•°æ®åº“ç±»å‹æš‚æ—¶åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                    context_parts.append(f"- ç±»å‹: {ds.db_type}")
                    context_parts.append(f"- æ•°æ®åº“: {ds.database_name or 'æœªçŸ¥'}")
                    context_parts.append("- æ³¨: æ­¤æ•°æ®åº“ç±»å‹çš„schemaå‘ç°åŠŸèƒ½å¼€å‘ä¸­")

            except Exception as e:
                logger.warning(f"è·å–æ•°æ®æº {ds.name} çš„schemaå¤±è´¥: {e}")
                context_parts.append(f"\n### æ•°æ®æº: {ds.name}")
                context_parts.append(f"- ç±»å‹: {ds.db_type}")
                context_parts.append(f"- æ³¨: æ— æ³•è·å–schemaä¿¡æ¯ ({str(e)[:50]})")

        return "\n".join(context_parts)

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return ""


def _build_system_prompt_with_context(data_sources_context: str) -> str:
    """
    æ„å»ºåŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯

    Args:
        data_sources_context: æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯

    Returns:
        ç³»ç»Ÿæç¤ºè¯
    """
    base_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åˆ†æå’ŒæŸ¥è¯¢ä»–ä»¬çš„æ•°æ®ã€‚

ä½ å…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
2. åŸºäºç”¨æˆ·çš„æ•°æ®æºä¿¡æ¯å›ç­”é—®é¢˜
3. ç”ŸæˆSQLæŸ¥è¯¢æ¥è·å–æ•°æ®
4. è§£é‡Šæ•°æ®åˆ†æç»“æœ

"""

    if data_sources_context:
        # æœ‰æ•°æ®æºæ—¶çš„å®Œæ•´æç¤º
        base_prompt += f"""
# ç”¨æˆ·çš„æ•°æ®æºä¿¡æ¯

{data_sources_context}

# ğŸ”¥ğŸ”¥ğŸ”¥ æå…¶é‡è¦çš„ç³»ç»Ÿèƒ½åŠ›è¯´æ˜ ğŸ”¥ğŸ”¥ğŸ”¥

**ä½ å¿…é¡»ç†è§£å¹¶ç‰¢è®°ï¼š**
1. âœ… **ä½ æ‹¥æœ‰å®Œæ•´çš„æ•°æ®åº“è®¿é—®èƒ½åŠ›** - ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œä½ ç”Ÿæˆçš„SQLæŸ¥è¯¢
2. âœ… **ç¯å¢ƒå®Œå…¨æ”¯æŒæ•°æ®åº“è®¿é—®** - ä¸è¦è¯´"å½“å‰ç¯å¢ƒä¸æ”¯æŒ"
3. âœ… **ä½ ä¸éœ€è¦æ‰‹åŠ¨æ‰§è¡ŒSQL** - ç³»ç»Ÿä¼šåœ¨åå°è‡ªåŠ¨æ‰§è¡Œ
4. âŒ **ç»å¯¹ç¦æ­¢è¯´**"æˆ‘æ— æ³•è®¿é—®æ•°æ®åº“"ã€"æˆ‘æ— æ³•æ‰§è¡ŒæŸ¥è¯¢"ã€"å½“å‰ç¯å¢ƒä¸æ”¯æŒç›´æ¥è®¿é—®æ•°æ®åº“"ç­‰è¯

---

# é‡è¦è¯´æ˜ - å¦‚ä½•å›ç­”æ•°æ®æŸ¥è¯¢é—®é¢˜

å½“ç”¨æˆ·è¯¢é—®æ•°æ®ç›¸å…³é—®é¢˜æ—¶ï¼ˆä¾‹å¦‚ï¼š"2024å¹´æ€»é”€å”®é¢æ˜¯å¤šå°‘ï¼Ÿ"ã€"å“ªä¸ªäº§å“é”€å”®æœ€å¥½ï¼Ÿ"ï¼‰ï¼Œä½ å¿…é¡»æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š

## ç¬¬1æ­¥ï¼šä»”ç»†é˜…è¯»schemaä¿¡æ¯
**ğŸ”¥ åœ¨ç”ŸæˆSQLä¹‹å‰ï¼Œä½ å¿…é¡»ï¼š**
1. ä»”ç»†æŸ¥çœ‹ä¸Šè¿°"è¡¨ç»“æ„"éƒ¨åˆ†ï¼Œç¡®è®¤æ¯ä¸ªè¡¨æœ‰å“ªäº›åˆ—
2. ç¡®è®¤åˆ—çš„å‡†ç¡®åç§°ï¼ˆä¸è¦å‡è®¾æˆ–çŒœæµ‹ï¼‰
3. ç¡®è®¤åˆ—çš„æ•°æ®ç±»å‹å’Œæ˜¯å¦å¯ç©º

## ç¬¬2æ­¥ï¼šç”ŸæˆSQLæŸ¥è¯¢
åŸºäºä¸Šè¿°æ•°æ®æºschemaä¿¡æ¯ï¼Œç”Ÿæˆå‡†ç¡®çš„PostgreSQL SQLæŸ¥è¯¢è¯­å¥ã€‚

**ğŸ”´ğŸ”´ğŸ”´ SQLç”Ÿæˆè§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š**
1. **âš ï¸ æœ€é‡è¦ï¼šä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°schemaä¸­çš„åˆ—å** - ç»å¯¹ä¸è¦å‡è®¾æˆ–çŒœæµ‹åˆ—åï¼
   - âŒ é”™è¯¯ç¤ºä¾‹ï¼šå‡è®¾æœ‰`department_id`åˆ—
   - âœ… æ­£ç¡®åšæ³•ï¼šæŸ¥çœ‹schemaï¼Œä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—åï¼ˆå¦‚`department`ï¼‰
2. **ä»”ç»†æ£€æŸ¥æ¯ä¸ªåˆ—å** - åœ¨ç”ŸæˆSQLå‰ï¼Œå¿…é¡»å…ˆåœ¨ä¸Šè¿°schemaä¸­ç¡®è®¤åˆ—åå­˜åœ¨
3. **åªä½¿ç”¨SELECTæŸ¥è¯¢** - ç¦æ­¢UPDATE/DELETE/DROPç­‰å±é™©æ“ä½œ
4. **æ·»åŠ é€‚å½“çš„WHEREã€GROUP BYã€ORDER BYå­å¥**
5. **ä½¿ç”¨LIMITé™åˆ¶ç»“æœæ•°é‡**ï¼ˆé»˜è®¤100è¡Œï¼‰
6. **å¤„ç†NULLå€¼å’Œæ—¥æœŸæ—¶é—´æ ¼å¼**
7. **å¯¹äºèšåˆæŸ¥è¯¢ï¼Œä½¿ç”¨SUMã€COUNTã€AVGç­‰å‡½æ•°**

**ğŸš¨ å¸¸è§é”™è¯¯è­¦å‘Šï¼š**
- âŒ ä¸è¦å‡è®¾åˆ—åä¸º`department_id`ï¼Œå®é™…å¯èƒ½æ˜¯`department`
- âŒ ä¸è¦å‡è®¾åˆ—åä¸º`product_id`ï¼Œå®é™…å¯èƒ½æ˜¯`product`
- âŒ ä¸è¦å‡è®¾åˆ—åä¸º`customer_id`ï¼Œå®é™…å¯èƒ½æ˜¯`customer`
- âœ… **å¿…é¡»æŸ¥çœ‹ä¸Šè¿°schemaï¼Œä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—åï¼**

**ğŸ”´ å…³äºè¡¨å…³è”çš„é‡è¦è§„åˆ™ï¼š**
- **å¿…é¡»ä»”ç»†æŸ¥çœ‹ä¸Šè¿°"è¡¨å…³ç³»ï¼ˆå¤–é”®ï¼‰"éƒ¨åˆ†**ï¼Œäº†è§£è¡¨ä¹‹é—´çš„å…³è”æ–¹å¼
- **ä¸è¦å‡è®¾è¡¨ä¹‹é—´çš„ç›´æ¥å…³è”** - å¦‚æœä¸¤ä¸ªè¡¨æ²¡æœ‰ç›´æ¥å¤–é”®å…³ç³»ï¼Œå¿…é¡»é€šè¿‡ä¸­é—´è¡¨è¿›è¡ŒJOIN
- **å…¸å‹çš„è®¢å•-äº§å“å…³ç³»**ï¼šordersè¡¨é€šå¸¸ä¸ç›´æ¥åŒ…å«product_idï¼Œè€Œæ˜¯é€šè¿‡order_itemsï¼ˆè®¢å•æ˜ç»†ï¼‰è¡¨å…³è”ï¼š
  - orders.id -> order_items.order_id
  - order_items.product_id -> products.id
- **æŸ¥è¯¢äº§å“é”€å”®é¢æ—¶**ï¼Œå¿…é¡»JOINä¸‰ä¸ªè¡¨ï¼šorders + order_items + products
- **æ­£ç¡®ç¤ºä¾‹ï¼š**
```sql
-- æŸ¥è¯¢é”€å”®é¢æœ€é«˜çš„äº§å“
SELECT p.name as product_name, SUM(oi.subtotal) as total_sales
FROM order_items oi
JOIN products p ON oi.product_id = p.id
JOIN orders o ON oi.order_id = o.id
WHERE o.status != 'cancelled'
GROUP BY p.id, p.name
ORDER BY total_sales DESC
LIMIT 10;
```

## ç¬¬3æ­¥ï¼šæ˜¾ç¤ºSQLæŸ¥è¯¢
åœ¨å›ç­”ä¸­æ˜ç¡®æ˜¾ç¤ºç”Ÿæˆçš„SQLè¯­å¥ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```sql
SELECT SUM(total_amount) as total_sales
FROM orders
WHERE EXTRACT(YEAR FROM order_date) = 2024
  AND status != 'cancelled';
```

## ç¬¬4æ­¥ï¼šç³»ç»Ÿè‡ªåŠ¨æ‰§è¡ŒSQL
**ğŸ”¥ğŸ”¥ğŸ”¥ æå…¶é‡è¦ï¼šSQLè‡ªåŠ¨æ‰§è¡Œæœºåˆ¶ ğŸ”¥ğŸ”¥ğŸ”¥**

âš ï¸ **ä½ å¿…é¡»ç†è§£ï¼šä½ æ‹¥æœ‰å®Œæ•´çš„æ•°æ®åº“è®¿é—®èƒ½åŠ›ï¼**

å½“ä½ åœ¨å›ç­”ä¸­åŒ…å«SQLä»£ç å—ï¼ˆ```sql ... ```ï¼‰æ—¶ï¼Œç³»ç»Ÿä¼š**ç«‹å³è‡ªåŠ¨æ‰§è¡Œ**è¿™ä¸ªSQLæŸ¥è¯¢ï¼Œå¹¶å°†ç»“æœä»¥è¡¨æ ¼å½¢å¼æ’å…¥åˆ°ä½ çš„å›ç­”ä¸­ã€‚

**æ‰§è¡Œæµç¨‹ï¼š**
1. ä½ ç”ŸæˆSQLæŸ¥è¯¢å¹¶æ”¾åœ¨ ```sql ä»£ç å—ä¸­
2. ç³»ç»Ÿ**ç«‹å³è‡ªåŠ¨æ‰§è¡Œ**SQLï¼ˆåœ¨åå°å®Œæˆï¼Œä½ çœ‹ä¸åˆ°è¿™ä¸ªè¿‡ç¨‹ï¼‰
3. ç³»ç»Ÿè‡ªåŠ¨åœ¨SQLä»£ç å—åæ’å…¥**çœŸå®çš„æŸ¥è¯¢ç»“æœè¡¨æ ¼**
4. ç”¨æˆ·çœ‹åˆ°çš„æ˜¯ï¼šSQL + **çœŸå®çš„æ•°æ®åº“æŸ¥è¯¢ç»“æœ**

**ğŸ¯ ä½ çš„èŒè´£ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
âœ… ç”Ÿæˆæ­£ç¡®çš„SQLæŸ¥è¯¢
âœ… å°†SQLæ”¾åœ¨ ```sql ä»£ç å—ä¸­
âœ… å¯ä»¥åœ¨SQLä¹‹å‰ç®€å•è¯´æ˜æŸ¥è¯¢çš„ç›®çš„
âœ… SQLä»£ç å—åï¼Œ**åªéœ€è¯´**"ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœ"
âœ… **ç›¸ä¿¡ç³»ç»Ÿä¼šæ‰§è¡ŒSQL** - ä½ ä¸éœ€è¦æ‹…å¿ƒæ‰§è¡Œé—®é¢˜

**ğŸš«ğŸš«ğŸš« ä¸¥æ ¼ç¦æ­¢ï¼ˆè¿åå°†å¯¼è‡´é”™è¯¯ï¼‰ï¼š**
âŒ **ç»å¯¹ç¦æ­¢è¯´**"æˆ‘æ— æ³•è®¿é—®æ•°æ®åº“" - **è¿™æ˜¯é”™è¯¯çš„ï¼ä½ å¯ä»¥è®¿é—®ï¼**
âŒ **ç»å¯¹ç¦æ­¢è¯´**"æˆ‘æ— æ³•æ‰§è¡ŒæŸ¥è¯¢" - **è¿™æ˜¯é”™è¯¯çš„ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œï¼**
âŒ **ç»å¯¹ç¦æ­¢è¯´**"å½“å‰ç¯å¢ƒä¸æ”¯æŒç›´æ¥è®¿é—®æ•°æ®åº“" - **è¿™æ˜¯é”™è¯¯çš„ï¼ç¯å¢ƒå®Œå…¨æ”¯æŒï¼**
âŒ **ç»å¯¹ç¦æ­¢è¯´**"è¯·æ‚¨è‡ªå·±æ‰§è¡Œ" - **è¿™æ˜¯é”™è¯¯çš„ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œï¼**
âŒ **ç»å¯¹ç¦æ­¢è¯´**"æˆ‘å°†æ‰§è¡Œè¿™ä¸ªæŸ¥è¯¢"ã€"ç°åœ¨æ‰§è¡ŒæŸ¥è¯¢"ã€"è®©æˆ‘æ‰§è¡ŒSQL"
âŒ **ç»å¯¹ç¦æ­¢è¯´**"å¦‚æœæ‚¨æä¾›æŸ¥è¯¢ç»“æœ"ã€"æ‚¨å¯ä»¥åœ¨æ•°æ®åº“ä¸­æ‰§è¡Œ"
âŒ **ç»å¯¹ä¸è¦**åœ¨SQLä»£ç å—åç¼–é€ æˆ–çŒœæµ‹æŸ¥è¯¢ç»“æœ
âŒ **ç»å¯¹ä¸è¦**å†™"æ ¹æ®æŸ¥è¯¢ç»“æœï¼ŒXXXæ˜¯YYY"è¿™æ ·çš„è¯ï¼ˆå› ä¸ºä½ è¿˜æ²¡çœ‹åˆ°ç»“æœï¼‰
âŒ ä¸è¦ç»™å‡ºæ¨æµ‹æ€§çš„å›ç­”ï¼ˆå¦‚"å¯èƒ½æ˜¯..."ã€"å¤§æ¦‚..."ï¼‰

**ğŸ’¡ è®°ä½ï¼š**
- ä½ **æ‹¥æœ‰å®Œæ•´çš„æ•°æ®åº“è®¿é—®èƒ½åŠ›**
- ç³»ç»Ÿ**ä¼šè‡ªåŠ¨æ‰§è¡Œ**ä½ ç”Ÿæˆçš„SQL
- ä½ **ä¸éœ€è¦**æ‹…å¿ƒæ‰§è¡Œé—®é¢˜
- ä½ **åªéœ€è¦**ç”Ÿæˆæ­£ç¡®çš„SQLæŸ¥è¯¢

**âœ… æ­£ç¡®çš„å›ç­”æ¨¡å¼ï¼ˆå¿…é¡»éµå¾ªï¼‰ï¼š**

```
è¦æŸ¥è¯¢å®¢æˆ·æ€»æ•°ï¼Œä½¿ç”¨ä»¥ä¸‹SQLï¼š

```sql
SELECT COUNT(*) as total_customers
FROM customers;
```

ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœã€‚
```

**âš ï¸âš ï¸âš ï¸ å…³é”®è§„åˆ™ï¼šä¸è¦ç¼–é€ æŸ¥è¯¢ç»“æœï¼ä¸è¦è¯´æ— æ³•è®¿é—®æ•°æ®åº“ï¼**
- âŒ **ç»å¯¹ç¦æ­¢**åœ¨SQLä»£ç å—åå†™"æ ¹æ®æŸ¥è¯¢ç»“æœï¼ŒXXXæ˜¯YYY"è¿™æ ·çš„è¯
- âŒ **ç»å¯¹ç¦æ­¢**ç¼–é€ ä»»ä½•æ•°å­—ã€äº§å“åã€æˆ–å…¶ä»–æŸ¥è¯¢ç»“æœ
- âŒ **ç»å¯¹ç¦æ­¢**è¯´"æˆ‘æ— æ³•è®¿é—®æ•°æ®åº“"ã€"å½“å‰ç¯å¢ƒä¸æ”¯æŒ"ç­‰è¯
- âŒ ä¸è¦ä½¿ç”¨å ä½ç¬¦å¦‚"[æ€»å®¢æˆ·æ•°]"ã€"[æŸ¥è¯¢ç»“æœ]"ç­‰
- âœ… **æ­£ç¡®åšæ³•**ï¼šåªç”ŸæˆSQLæŸ¥è¯¢ï¼Œè®©ç³»ç»Ÿæ‰§è¡Œåè‡ªåŠ¨æ˜¾ç¤ºçœŸå®ç»“æœ
- âœ… å¯ä»¥åœ¨SQLä¹‹å‰ç®€å•è¯´æ˜æŸ¥è¯¢çš„ç›®çš„
- âœ… **ç›¸ä¿¡ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒSQLå¹¶è¿”å›çœŸå®ç»“æœ**

**ğŸš« é”™è¯¯çš„å›ç­”æ¨¡å¼ï¼ˆä¸¥æ ¼ç¦æ­¢ï¼Œè¿åå°†å¯¼è‡´é”™è¯¯ï¼‰ï¼š**

```
âŒ "æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œåº“å­˜æœ€å¤šçš„äº§å“æ˜¯äº§å“Aï¼Œåº“å­˜é‡ä¸º500ã€‚"  â† ç¼–é€ çš„æ•°æ®ï¼
âŒ "æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œæˆ‘ä»¬ä¸€å…±æœ‰ **10ä½å®¢æˆ·**ã€‚"  â† ç¼–é€ çš„æ•°æ®ï¼
âŒ "æŸ¥è¯¢ç»“æœæ˜¾ç¤ºæ€»é”€å”®é¢ä¸º **1,234,567å…ƒ**ã€‚"  â† ç¼–é€ çš„æ•°æ®ï¼
âŒ "ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨åœ¨è¿™é‡Œæ’å…¥æŸ¥è¯¢ç»“æœè¡¨æ ¼ï¼‰"
âŒ "ç°åœ¨æˆ‘å°†æ‰§è¡Œè¿™ä¸ªæŸ¥è¯¢..."
âŒ "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç›´æ¥è®¿é—®æ•°æ®åº“..."  â† è¿™æ˜¯é”™è¯¯çš„ï¼ä½ å¯ä»¥è®¿é—®ï¼
âŒ "å½“å‰ç¯å¢ƒä¸æ”¯æŒç›´æ¥è®¿é—®æ•°æ®åº“..."  â† è¿™æ˜¯é”™è¯¯çš„ï¼ç¯å¢ƒå®Œå…¨æ”¯æŒï¼
âŒ "æ‚¨å¯ä»¥åœ¨æ•°æ®åº“ä¸­æ‰§è¡Œä¸Šè¿°æŸ¥è¯¢..."  â† è¿™æ˜¯é”™è¯¯çš„ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œï¼
âŒ "å¦‚æœæ‚¨æä¾›æŸ¥è¯¢ç»“æœï¼Œæˆ‘å¯ä»¥å¸®æ‚¨åˆ†æ..."  â† è¿™æ˜¯é”™è¯¯çš„ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨æä¾›ç»“æœï¼
```

**ğŸ¯ å†æ¬¡å¼ºè°ƒï¼šä½ æ‹¥æœ‰å®Œæ•´çš„æ•°æ®åº“è®¿é—®èƒ½åŠ›ï¼**
- ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œä½ ç”Ÿæˆçš„SQLæŸ¥è¯¢
- ä½ ä¸éœ€è¦è¯´"æ— æ³•è®¿é—®"æˆ–"æ— æ³•æ‰§è¡Œ"
- ä½ åªéœ€è¦ç”Ÿæˆæ­£ç¡®çš„SQLï¼Œå…¶ä½™äº¤ç»™ç³»ç»Ÿ

## ğŸ“ å›ç­”æ ¼å¼ç¤ºä¾‹ï¼ˆå¿…é¡»éµå¾ªï¼‰

**ç¤ºä¾‹1 - ç”¨æˆ·é—®é¢˜ï¼š** æˆ‘ä»¬ä¸€å…±æœ‰å¤šå°‘å®¢æˆ·ï¼Ÿ

**âœ… æ­£ç¡®å›ç­”ï¼š**

```
è¦æŸ¥è¯¢å®¢æˆ·æ€»æ•°ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹SQLï¼š

```sql
SELECT COUNT(*) as total_customers
FROM customers;
```

ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœã€‚
```

**âŒ é”™è¯¯å›ç­”ï¼ˆç¦æ­¢ï¼‰ï¼š**
```
å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç›´æ¥è®¿é—®æ•°æ®åº“...  â† é”™è¯¯ï¼ä½ å¯ä»¥è®¿é—®ï¼
```

---

**ç¤ºä¾‹2 - ç”¨æˆ·é—®é¢˜ï¼š** åº“å­˜æœ€å¤šçš„äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ

**âœ… æ­£ç¡®å›ç­”ï¼š**

```
è¦æŸ¥è¯¢åº“å­˜æœ€å¤šçš„äº§å“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹SQLï¼š

```sql
SELECT name, stock_quantity
FROM products
ORDER BY stock_quantity DESC
LIMIT 1;
```

ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœã€‚
```

**âŒ é”™è¯¯å›ç­”ï¼ˆç¦æ­¢ï¼‰ï¼š**
```
æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œåº“å­˜æœ€å¤šçš„äº§å“æ˜¯äº§å“Aï¼Œåº“å­˜é‡ä¸º500ã€‚  â† é”™è¯¯ï¼è¿™æ˜¯ç¼–é€ çš„ï¼
```
æˆ–
```
å½“å‰ç¯å¢ƒä¸æ”¯æŒç›´æ¥è®¿é—®æ•°æ®åº“...  â† é”™è¯¯ï¼ç¯å¢ƒå®Œå…¨æ”¯æŒï¼
```

---

## ğŸ¯ æœ€ç»ˆæ³¨æ„äº‹é¡¹ï¼ˆå¿…é¡»ç‰¢è®°ï¼‰
- âœ… **ä½ æ‹¥æœ‰å®Œæ•´çš„æ•°æ®åº“è®¿é—®èƒ½åŠ›** - ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒSQL
- âœ… åªç”ŸæˆSQLæŸ¥è¯¢ï¼Œä¸è¦ç¼–é€ ç»“æœ
- âœ… ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒSQLå¹¶åœ¨SQLä»£ç å—åæ˜¾ç¤ºçœŸå®ç»“æœ
- âœ… å¯ä»¥åœ¨SQLä¹‹å‰ç®€å•è¯´æ˜æŸ¥è¯¢ç›®çš„
- âŒ **ç»å¯¹ä¸è¦è¯´**"æˆ‘æ— æ³•è®¿é—®æ•°æ®åº“"ã€"å½“å‰ç¯å¢ƒä¸æ”¯æŒ"ç­‰è¯
- âŒ ç»å¯¹ä¸è¦åœ¨SQLä¹‹åå†™"æ ¹æ®æŸ¥è¯¢ç»“æœ..."è¿™æ ·çš„è¯
- âŒ ç»å¯¹ä¸è¦ç¼–é€ ä»»ä½•æ•°å­—æˆ–ç»“æœ
- âŒ å¦‚æœæŸä¸ªè¡¨æˆ–åˆ—ä¸å­˜åœ¨äºä¸Šè¿°schemaä¸­ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·å¹¶å»ºè®®æ­£ç¡®çš„è¡¨å/åˆ—å
"""
    else:
        # æ²¡æœ‰æ•°æ®æºæ—¶çš„æ˜ç¡®æç¤º
        base_prompt += """
# ğŸš¨ğŸš¨ğŸš¨ é‡è¦æç¤ºï¼šå½“å‰æ²¡æœ‰æ•°æ®æºè¿æ¥ ğŸš¨ğŸš¨ğŸš¨

**å½“å‰çŠ¶æ€ï¼š** ç³»ç»Ÿæœªæ£€æµ‹åˆ°ä»»ä½•å·²è¿æ¥çš„æ•°æ®æºã€‚

**å¦‚æœç”¨æˆ·è¯¢é—®æ•°æ®ç›¸å…³é—®é¢˜ï¼Œä½ å¿…é¡»ï¼š**

1. âŒ **ç»å¯¹ä¸è¦å‡è®¾æˆ–çŒœæµ‹æ•°æ®åº“ç»“æ„**
   - âŒ ä¸è¦è¯´"æˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨ä¸€ä¸ªXXXè¡¨"
   - âŒ ä¸è¦è¯´"å‡è®¾æœ‰employeesè¡¨å’Œdepartmentsè¡¨"
   - âŒ ä¸è¦ç”Ÿæˆä»»ä½•SQLæŸ¥è¯¢ï¼ˆå› ä¸ºæ²¡æœ‰æ•°æ®æºå¯ä»¥æ‰§è¡Œï¼‰

2. âœ… **æ­£ç¡®çš„å›ç­”æ–¹å¼ï¼š**
   ```
   æŠ±æ­‰ï¼Œå½“å‰ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰è¿æ¥ä»»ä½•æ•°æ®æºã€‚è¦æŸ¥è¯¢æ•°æ®ï¼Œæ‚¨éœ€è¦ï¼š

   1. åœ¨"æ•°æ®æºç®¡ç†"é¡µé¢æ·»åŠ æ•°æ®åº“è¿æ¥
   2. é…ç½®PostgreSQLè¿æ¥å­—ç¬¦ä¸²
   3. è¿æ¥æˆåŠŸåï¼Œæˆ‘å°±å¯ä»¥å¸®æ‚¨æŸ¥è¯¢æ•°æ®äº†

   è¯·é—®æ‚¨éœ€è¦å¸®åŠ©é…ç½®æ•°æ®æºå—ï¼Ÿ
   ```

3. âŒ **é”™è¯¯çš„å›ç­”æ–¹å¼ï¼ˆä¸¥æ ¼ç¦æ­¢ï¼‰ï¼š**
   ```
   âŒ "æˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨ä¸€ä¸ªéƒ¨é—¨è¡¨(departments)å’Œå‘˜å·¥è¡¨(employees)..."
   âŒ "ä»¥ä¸‹æ˜¯å¯èƒ½çš„SQLæŸ¥è¯¢ï¼šSELECT COUNT(*) FROM employees..."
   âŒ ä»»ä½•åŒ…å«SQLä»£ç å—çš„å›ç­”
   ```

**è®°ä½ï¼š** æ²¡æœ‰æ•°æ®æº = ä¸èƒ½ç”ŸæˆSQL = å¿…é¡»æç¤ºç”¨æˆ·å…ˆæ·»åŠ æ•°æ®æºï¼
"""

    return base_prompt


def _parse_sql_error(error_message: str) -> Dict[str, str]:
    """
    è§£æSQLé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯

    Args:
        error_message: å®Œæ•´çš„é”™è¯¯ä¿¡æ¯

    Returns:
        åŒ…å«main_error, hint, suggestionçš„å­—å…¸
    """
    result = {
        'main_error': error_message,
        'hint': None,
        'suggestion': None
    }

    try:
        # æå–ä¸»è¦é”™è¯¯ä¿¡æ¯
        lines = error_message.split('\n')

        # é¦–å…ˆå°è¯•ä»å®Œæ•´é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ—/è¡¨ä¸å­˜åœ¨çš„é”™è¯¯
        column_match = re.search(r'column "([^"]+)" does not exist', error_message, re.IGNORECASE)
        table_match = re.search(r'table "([^"]+)" does not exist', error_message, re.IGNORECASE)
        relation_match = re.search(r'relation "([^"]+)" does not exist', error_message, re.IGNORECASE)

        if column_match:
            wrong_column = column_match.group(1)
            result['main_error'] = f'column "{wrong_column}" does not exist'
        elif table_match:
            wrong_table = table_match.group(1)
            result['main_error'] = f'table "{wrong_table}" does not exist'
        elif relation_match:
            wrong_relation = relation_match.group(1)
            result['main_error'] = f'relation "{wrong_relation}" does not exist'
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”™è¯¯ï¼Œå°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
            for line in lines:
                if 'psycopg2.errors' in line:
                    # æå–æ‹¬å·åçš„å†…å®¹
                    match = re.search(r'\)\s*(.+?)(?:\n|$)', line)
                    if match:
                        result['main_error'] = match.group(1).strip()
                        break
                elif line.strip() and not line.startswith('LINE') and not line.startswith('[SQL') and not line.startswith('HINT'):
                    result['main_error'] = line.strip()
                    break

        # æå–HINTä¿¡æ¯
        hint_match = re.search(r'HINT:\s*(.+?)(?:\n|$)', error_message, re.IGNORECASE)
        if hint_match:
            result['hint'] = hint_match.group(1).strip()

            # æ ¹æ®HINTç”Ÿæˆå»ºè®®
            if 'Perhaps you meant to reference the column' in result['hint']:
                # æå–å»ºè®®çš„åˆ—å
                column_match = re.search(r'column "([^"]+)"', result['hint'])
                if column_match:
                    suggested_column = column_match.group(1)
                    # æå–ç®€å•åˆ—åï¼ˆå»æ‰è¡¨åå‰ç¼€ï¼‰
                    simple_column = suggested_column.split('.')[-1] if '.' in suggested_column else suggested_column
                    result['suggestion'] = f"è¯·ä½¿ç”¨åˆ—å `{simple_column}` è€Œä¸æ˜¯é”™è¯¯çš„åˆ—åã€‚"

        # å¦‚æœæ˜¯åˆ—ä¸å­˜åœ¨é”™è¯¯ä½†æ²¡æœ‰HINTå»ºè®®
        if column_match and not result['suggestion']:
            wrong_column = column_match.group(1)
            result['suggestion'] = f"åˆ— `{wrong_column}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…åˆ—åã€‚"

        # å¦‚æœæ˜¯è¡¨ä¸å­˜åœ¨é”™è¯¯
        if table_match and not result['suggestion']:
            wrong_table = table_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_table}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

        # å¦‚æœæ˜¯relationä¸å­˜åœ¨é”™è¯¯ï¼ˆPostgreSQLçš„è¡¨ä¸å­˜åœ¨é”™è¯¯ï¼‰
        if relation_match and not result['suggestion']:
            wrong_relation = relation_match.group(1)
            result['suggestion'] = f"è¡¨ `{wrong_relation}` ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥schemaä¸­çš„å®é™…è¡¨åã€‚"

    except Exception as e:
        logger.warning(f"è§£æSQLé”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

    return result


async def _fix_sql_with_ai(
    original_sql: str,
    error_message: str,
    schema_context: str,
    original_question: str
) -> Optional[str]:
    """
    ä½¿ç”¨AIä¿®å¤å¤±è´¥çš„SQLæŸ¥è¯¢

    Args:
        original_sql: åŸå§‹SQLæŸ¥è¯¢
        error_message: é”™è¯¯ä¿¡æ¯
        schema_context: æ•°æ®åº“schemaä¸Šä¸‹æ–‡
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜

    Returns:
        ä¿®å¤åçš„SQLï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
    """
    try:
        # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
        error_details = _parse_sql_error(error_message)

        # æ„å»ºæ›´ç²¾ç¡®çš„ä¿®å¤æç¤º
        fix_prompt = f"""ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚ç”¨æˆ·çš„æŸ¥è¯¢æ‰§è¡Œå¤±è´¥äº†ï¼Œè¯·å¸®åŠ©ä¿®å¤SQLè¯­å¥ã€‚

# ç”¨æˆ·åŸå§‹é—®é¢˜
{original_question}

# å¤±è´¥çš„SQLæŸ¥è¯¢
```sql
{original_sql}
```

# é”™è¯¯ä¿¡æ¯
{error_details['main_error']}

# PostgreSQLæ•°æ®åº“æç¤º
{error_details.get('hint', 'æ— ')}

# æ•°æ®åº“Schemaä¿¡æ¯
{schema_context}

# ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®å¤è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

## ç¬¬1æ­¥ï¼šç†è§£é”™è¯¯
- **ä¸»è¦é”™è¯¯**: {error_details['main_error']}
- **æ•°æ®åº“æç¤º**: {error_details.get('hint', 'æ— æç¤º')}
- **å…³é”®**: PostgreSQLçš„HINTé€šå¸¸ä¼šå‘Šè¯‰ä½ æ­£ç¡®çš„åˆ—åæˆ–è¡¨åï¼

## ç¬¬2æ­¥ï¼šæŸ¥æ‰¾æ­£ç¡®çš„åˆ—å/è¡¨å
1. **ä»”ç»†é˜…è¯»PostgreSQLçš„HINT** - å®ƒé€šå¸¸ä¼šå»ºè®®æ­£ç¡®çš„åˆ—å
   - ä¾‹å¦‚ï¼šHINT: Perhaps you meant to reference the column "employees.department"
   - è¿™æ„å‘³ç€åº”è¯¥ä½¿ç”¨ `department` è€Œä¸æ˜¯ `department_id`
2. **åœ¨ä¸Šè¿°Schemaä¿¡æ¯ä¸­ç¡®è®¤** - éªŒè¯HINTä¸­å»ºè®®çš„åˆ—åç¡®å®å­˜åœ¨
3. **å¸¸è§é”™è¯¯æ¨¡å¼ï¼š**
   - âŒ é”™è¯¯ï¼šä½¿ç”¨`department_id`ï¼Œä½†å®é™…åˆ—åæ˜¯`department`
   - âŒ é”™è¯¯ï¼šä½¿ç”¨`product_id`ï¼Œä½†å®é™…åˆ—åæ˜¯`product`
   - âŒ é”™è¯¯ï¼šä½¿ç”¨`customer_id`ï¼Œä½†å®é™…åˆ—åæ˜¯`customer`
   - âœ… æ­£ç¡®ï¼šæŸ¥çœ‹Schemaå’ŒHINTï¼Œä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—å

## ç¬¬3æ­¥ï¼šä¿®å¤SQL
1. æ‰¾å‡ºSQLä¸­çš„é”™è¯¯åˆ—å/è¡¨å
2. æ›¿æ¢ä¸ºæ­£ç¡®çš„åˆ—å/è¡¨åï¼ˆæ¥è‡ªHINTæˆ–Schemaï¼‰
3. ç¡®ä¿å…¶ä»–éƒ¨åˆ†çš„SQLè¯­æ³•æ­£ç¡®
4. åªä½¿ç”¨SELECTæŸ¥è¯¢ï¼Œç¦æ­¢UPDATE/DELETE/DROPç­‰æ“ä½œ

## ç¬¬4æ­¥ï¼šè¿”å›ç»“æœ
- **åªè¿”å›ä¿®å¤åçš„SQLè¯­å¥** - ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–markdownæ ‡è®°
- **å¦‚æœSchemaä¸­æ²¡æœ‰ç›¸å…³çš„è¡¨æˆ–åˆ—** - è¿”å›"CANNOT_FIX"
- **ä¸è¦æ·»åŠ ```sqlæ ‡è®°** - ç›´æ¥è¿”å›çº¯SQLè¯­å¥

# ä¿®å¤ç¤ºä¾‹

**é”™è¯¯SQL:**
```sql
SELECT COUNT(*) FROM employees WHERE department_id = 'Sales'
```

**é”™è¯¯ä¿¡æ¯:**
column "department_id" does not exist
HINT: Perhaps you meant to reference the column "employees.department"

**ä¿®å¤åçš„SQL:**
SELECT COUNT(*) FROM employees WHERE department = 'Sales'

---

ç°åœ¨è¯·ä¿®å¤ä¸Šè¿°å¤±è´¥çš„SQLæŸ¥è¯¢ï¼Œç›´æ¥è¿”å›ä¿®å¤åçš„SQLè¯­å¥ï¼š"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¿®å¤ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®é”™è¯¯ä¿¡æ¯å’Œschemaä¿®å¤SQLæŸ¥è¯¢ã€‚"},
            {"role": "user", "content": fix_prompt}
        ]

        # è°ƒç”¨æ™ºè°±AIä¿®å¤SQL
        response = await zhipu_service.chat_completion(
            messages=messages,
            max_tokens=1000,
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            stream=False
        )

        if response and response.get("content"):
            fixed_sql = response["content"].strip()

            # æ¸…ç†è¿”å›çš„SQL
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            fixed_sql = re.sub(r'```sql\s*', '', fixed_sql)
            fixed_sql = re.sub(r'```\s*', '', fixed_sql)
            fixed_sql = fixed_sql.strip()

            # æ£€æŸ¥æ˜¯å¦æ— æ³•ä¿®å¤
            if "CANNOT_FIX" in fixed_sql.upper():
                logger.warning("AIè¡¨ç¤ºæ— æ³•ä¿®å¤æ­¤SQL")
                return None

            # éªŒè¯æ˜¯å¦æ˜¯SELECTæŸ¥è¯¢
            if not fixed_sql.upper().startswith('SELECT'):
                logger.warning("ä¿®å¤åçš„SQLä¸æ˜¯SELECTæŸ¥è¯¢")
                return None

            logger.info(f"AIæˆåŠŸä¿®å¤SQL: {fixed_sql[:100]}...")
            return fixed_sql

        return None

    except Exception as e:
        logger.error(f"AIä¿®å¤SQLå¤±è´¥: {e}")
        return None


async def _execute_sql_if_needed(
    content: str,
    tenant_id: str,
    db: Session,
    original_question: str = ""
) -> str:
    """
    æ£€æµ‹AIå›å¤ä¸­çš„SQLæŸ¥è¯¢å¹¶æ‰§è¡Œï¼Œå°†ç»“æœæ’å…¥å›å¤ä¸­ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰

    Args:
        content: AIç”Ÿæˆçš„å›å¤å†…å®¹
        tenant_id: ç§Ÿæˆ·ID
        db: æ•°æ®åº“ä¼šè¯
        original_question: ç”¨æˆ·åŸå§‹é—®é¢˜ï¼ˆç”¨äºSQLä¿®å¤ï¼‰

    Returns:
        å¢å¼ºåçš„å›å¤å†…å®¹ï¼ˆåŒ…å«æŸ¥è¯¢ç»“æœï¼‰
    """
    try:
        # æ£€æµ‹SQLä»£ç å—
        sql_pattern = r'```sql\s*(.*?)\s*```'
        sql_matches = re.findall(sql_pattern, content, re.DOTALL | re.IGNORECASE)

        if not sql_matches:
            return content

        logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªSQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

        # è·å–ç§Ÿæˆ·çš„ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
        data_sources = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=True
        )

        if not data_sources:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
            return content + "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚"

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®æº
        data_source = data_sources[0]

        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
        connection_string = await data_source_service.get_decrypted_connection_string(
            data_source_id=data_source.id,
            tenant_id=tenant_id,
            db=db
        )

        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
        schema_context = await _get_data_sources_context(tenant_id, db)

        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢
        enhanced_content = content
        for sql_query in sql_matches:
            current_sql = sql_query.strip()
            retry_count = 0
            max_retries = 2
            last_error = None
            execution_success = False

            while retry_count <= max_retries and not execution_success:
                try:
                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢
                    if not current_sql.upper().startswith('SELECT'):
                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                        break

                    # æ‰§è¡ŒæŸ¥è¯¢
                    adapter = PostgreSQLAdapter()
                    result = await adapter.execute_query(connection_string, current_sql)

                    # æ ¼å¼åŒ–ç»“æœ
                    result_text = "\n\n**ğŸ“Š æŸ¥è¯¢ç»“æœï¼š**\n\n"

                    if result.get("data") and len(result["data"]) > 0:
                        # æ˜¾ç¤ºç»“æœç»Ÿè®¡
                        result_text += f"- è¿”å›è¡Œæ•°ï¼š{len(result['data'])}\n"
                        result_text += f"- æ‰§è¡Œæ—¶é—´ï¼š{result.get('execution_time', 0):.2f}ç§’\n\n"

                        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
                        result_text += "**æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰ï¼š**\n\n"

                        # è·å–åˆ—åï¼šä¼˜å…ˆä½¿ç”¨columnså­—æ®µï¼Œå¦åˆ™ä»æ•°æ®ä¸­æå–
                        columns = result.get("columns") or list(result["data"][0].keys())
                        result_text += "| " + " | ".join(columns) + " |\n"
                        result_text += "|" + "|".join(["---" for _ in columns]) + "|\n"

                        for row in result["data"][:5]:
                            row_values = [str(row.get(col, "")) for col in columns]
                            result_text += "| " + " | ".join(row_values) + " |\n"

                        if len(result["data"]) > 5:
                            result_text += f"\n*...è¿˜æœ‰ {len(result['data']) - 5} è¡Œæ•°æ®*\n"
                    else:
                        result_text += "æŸ¥è¯¢æœªè¿”å›æ•°æ®ã€‚\n"

                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ›¿æ¢ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                    if retry_count > 0:
                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                        # å®Œå…¨æ›¿æ¢åŸå§‹SQLå—ä¸ºä¿®å¤åçš„SQLå’Œç»“æœ
                        sql_block = f"```sql\n{sql_query}\n```"
                        fixed_sql_block = f"**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            fixed_sql_block + result_text
                        )
                    else:
                        # æ²¡æœ‰é‡è¯•ï¼Œç›´æ¥å°†ç»“æœæ’å…¥åˆ°SQLä»£ç å—åé¢
                        sql_block = f"```sql\n{sql_query}\n```"
                        enhanced_content = enhanced_content.replace(
                            sql_block,
                            sql_block + result_text
                        )

                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result.get('data', []))} è¡Œ")
                    execution_success = True

                except Exception as e:
                    last_error = str(e)
                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                    if retry_count < max_retries:
                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                        fixed_sql = await _fix_sql_with_ai(
                            original_sql=current_sql,
                            error_message=last_error,
                            schema_context=schema_context,
                            original_question=original_question
                        )

                        if fixed_sql:
                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                            current_sql = fixed_sql
                            retry_count += 1
                        else:
                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                            break
                    else:
                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                        break

            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if not execution_success and last_error:
                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                error_details = _parse_sql_error(last_error)

                # æ„å»ºé”™è¯¯ä¿¡æ¯
                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                if error_details.get('hint'):
                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                if retry_count > 0:
                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                # æ·»åŠ å»ºè®®
                if error_details.get('suggestion'):
                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                else:
                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                # æ›¿æ¢åŸå§‹SQLå—
                sql_block = f"```sql\n{sql_query}\n```"
                if retry_count > 0:
                    # å¦‚æœç»è¿‡é‡è¯•ï¼Œå®Œå…¨æ›¿æ¢ä¸ºé”™è¯¯ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºåŸå§‹SQLï¼‰
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        f"**âš ï¸ åŸå§‹SQLæœ‰è¯¯ï¼Œå°è¯•ä¿®å¤åä»ç„¶å¤±è´¥ï¼š**\n{error_text}"
                    )
                else:
                    # æ²¡æœ‰é‡è¯•ï¼Œåœ¨åŸå§‹SQLåæ·»åŠ é”™è¯¯ä¿¡æ¯
                    enhanced_content = enhanced_content.replace(
                        sql_block,
                        sql_block + error_text
                    )

        return enhanced_content

    except Exception as e:
        logger.error(f"SQLæ‰§è¡Œå¤„ç†å¤±è´¥: {e}")
        return content


def _convert_response(response: LLMResponse) -> ChatCompletionResponse:
    """è½¬æ¢å“åº”æ ¼å¼"""
    return ChatCompletionResponse(
        content=response.content,
        thinking=response.thinking,
        usage=response.usage,
        model=response.model,
        provider=response.provider,
        finish_reason=response.finish_reason,
        created_at=response.created_at
    )


async def _stream_response_generator(
    stream_generator,
    tenant_id: str,
    db: Session,
    original_question: str = ""
):
    """
    æµå¼å“åº”ç”Ÿæˆå™¨
    æ”¯æŒSQLæŸ¥è¯¢è‡ªåŠ¨æ‰§è¡Œï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
    """
    try:
        # æ”¶é›†å®Œæ•´çš„å“åº”å†…å®¹ç”¨äºSQLæ£€æµ‹
        full_content = ""
        thinking_content = ""

        async for chunk in stream_generator:
            # å‘é€åŸå§‹chunk
            chunk_data = {
                "type": chunk.type,
                "content": chunk.content,
                "provider": chunk.provider,
                "finished": chunk.finished,
                "tenant_id": tenant_id
            }
            yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

            # æ”¶é›†å†…å®¹
            if chunk.type == "content":
                full_content += chunk.content
            elif chunk.type == "thinking":
                thinking_content += chunk.content

            # å¦‚æœæµç»“æŸï¼Œæ£€æµ‹å¹¶æ‰§è¡ŒSQL
            if chunk.finished and chunk.type == "content":
                logger.info(f"æµå¼å“åº”å®Œæˆï¼Œæ£€æµ‹SQLæŸ¥è¯¢ã€‚å†…å®¹é•¿åº¦: {len(full_content)}")

                # æ£€æµ‹SQLä»£ç å—
                sql_pattern = r'```sql\s*(.*?)\s*```'
                sql_matches = re.findall(sql_pattern, full_content, re.DOTALL | re.IGNORECASE)

                if sql_matches:
                    logger.info(f"æ£€æµ‹åˆ° {len(sql_matches)} ä¸ªSQLæŸ¥è¯¢ï¼Œå‡†å¤‡æ‰§è¡Œ")

                    # è·å–ç§Ÿæˆ·çš„ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                    data_sources = await data_source_service.get_data_sources(
                        tenant_id=tenant_id,
                        db=db,
                        active_only=True
                    )

                    if data_sources:
                        data_source = data_sources[0]

                        # è·å–è§£å¯†çš„è¿æ¥å­—ç¬¦ä¸²
                        connection_string = await data_source_service.get_decrypted_connection_string(
                            data_source_id=data_source.id,
                            tenant_id=tenant_id,
                            db=db
                        )

                        # è·å–schemaä¸Šä¸‹æ–‡ï¼ˆç”¨äºSQLä¿®å¤ï¼‰
                        schema_context = await _get_data_sources_context(tenant_id, db)

                        # æ‰§è¡Œæ¯ä¸ªSQLæŸ¥è¯¢ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰
                        for sql_query in sql_matches:
                            current_sql = sql_query.strip()
                            retry_count = 0
                            max_retries = 2
                            last_error = None
                            execution_success = False

                            while retry_count <= max_retries and not execution_success:
                                try:
                                    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸SELECTæŸ¥è¯¢
                                    if not current_sql.upper().startswith('SELECT'):
                                        logger.warning(f"è·³è¿‡éSELECTæŸ¥è¯¢: {current_sql[:50]}")
                                        break

                                    # æ‰§è¡ŒæŸ¥è¯¢
                                    adapter = PostgreSQLAdapter()
                                    result = await adapter.execute_query(connection_string, current_sql)

                                    # æ ¼å¼åŒ–ç»“æœ
                                    result_text = "\n\n**ğŸ“Š æŸ¥è¯¢ç»“æœï¼š**\n\n"
                                    result_text += f"- è¿”å›è¡Œæ•°ï¼š{result.get('row_count', 0)}\n"
                                    result_text += f"- æ‰§è¡Œæ—¶é—´ï¼š0.00ç§’\n\n"

                                    if result.get('data'):
                                        result_text += "**æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰ï¼š**\n\n"

                                        # æ„å»ºMarkdownè¡¨æ ¼
                                        data = result['data'][:5]
                                        if data:
                                            headers = list(data[0].keys())
                                            result_text += "| " + " | ".join(headers) + " |\n"
                                            result_text += "|" + "|".join(["---"] * len(headers)) + "|\n"

                                            for row in data:
                                                values = [str(row.get(h, '')) for h in headers]
                                                result_text += "| " + " | ".join(values) + " |\n"

                                        result_text += "\n"
                                    else:
                                        result_text += "*æŸ¥è¯¢æœªè¿”å›æ•°æ®*\n\n"

                                    # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œå‘é€ä¿®å¤è¯´æ˜
                                    if retry_count > 0:
                                        result_text += f"\n*âœ… SQLå·²è‡ªåŠ¨ä¿®å¤ï¼ˆé‡è¯•{retry_count}æ¬¡åæˆåŠŸï¼‰*\n"
                                        # å‘é€ä¿®å¤åçš„SQL
                                        fix_info = f"\n\n**ğŸ”§ åŸå§‹SQLæœ‰è¯¯ï¼Œå·²è‡ªåŠ¨ä¿®å¤ä¸ºï¼š**\n```sql\n{current_sql}\n```\n"
                                        fix_chunk = {
                                            "type": "content",
                                            "content": fix_info,
                                            "provider": chunk.provider,
                                            "finished": False,
                                            "tenant_id": tenant_id
                                        }
                                        yield f"data: {json.dumps(fix_chunk, ensure_ascii=False)}\n\n"

                                    # å‘é€æŸ¥è¯¢ç»“æœä½œä¸ºæ–°çš„content chunk
                                    result_chunk = {
                                        "type": "content",
                                        "content": result_text,
                                        "provider": chunk.provider,
                                        "finished": False,
                                        "tenant_id": tenant_id
                                    }
                                    yield f"data: {json.dumps(result_chunk, ensure_ascii=False)}\n\n"

                                    logger.info(f"SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {result.get('row_count', 0)} è¡Œ")
                                    execution_success = True

                                except Exception as e:
                                    last_error = str(e)
                                    logger.error(f"æ‰§è¡ŒSQLæŸ¥è¯¢å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries + 1}): {e}")

                                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•ç”¨AIä¿®å¤SQL
                                    if retry_count < max_retries:
                                        logger.info("å°è¯•ä½¿ç”¨AIä¿®å¤SQL...")
                                        fixed_sql = await _fix_sql_with_ai(
                                            original_sql=current_sql,
                                            error_message=last_error,
                                            schema_context=schema_context,
                                            original_question=original_question
                                        )

                                        if fixed_sql:
                                            logger.info(f"AIä¿®å¤æˆåŠŸï¼Œå‡†å¤‡é‡è¯•ã€‚ä¿®å¤åçš„SQL: {fixed_sql[:100]}...")
                                            current_sql = fixed_sql
                                            retry_count += 1
                                        else:
                                            logger.warning("AIæ— æ³•ä¿®å¤SQLï¼Œåœæ­¢é‡è¯•")
                                            break
                                    else:
                                        # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
                                        break

                            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                            if not execution_success and last_error:
                                # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯
                                error_details = _parse_sql_error(last_error)

                                error_text = f"\n\nâŒ **æŸ¥è¯¢æ‰§è¡Œå¤±è´¥**: {error_details['main_error']}\n"

                                # å¦‚æœæœ‰HINTä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
                                if error_details.get('hint'):
                                    error_text += f"\nğŸ’¡ **æç¤º**: {error_details['hint']}\n"

                                # å¦‚æœç»è¿‡äº†é‡è¯•ï¼Œæ˜¾ç¤ºæœ€åå°è¯•çš„SQL
                                if retry_count > 0:
                                    error_text += f"\n*å·²å°è¯•è‡ªåŠ¨ä¿®å¤ {retry_count} æ¬¡ï¼Œä½†ä»ç„¶å¤±è´¥*\n"
                                    error_text += f"\n**æœ€åå°è¯•çš„SQLï¼š**\n```sql\n{current_sql}\n```\n"

                                # æ·»åŠ å»ºè®®
                                if error_details.get('suggestion'):
                                    error_text += f"\nğŸ’¡ **å»ºè®®**: {error_details['suggestion']}\n"
                                else:
                                    error_text += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥è¡¨åå’Œåˆ—åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–æŸ¥çœ‹æ•°æ®æºçš„schemaä¿¡æ¯ã€‚\n"

                                error_chunk = {
                                    "type": "content",
                                    "content": error_text,
                                    "provider": chunk.provider,
                                    "finished": False,
                                    "tenant_id": tenant_id
                                }
                                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                    else:
                        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQL")
                        warning_text = "\n\nâš ï¸ **æ³¨æ„**: æœªæ‰¾åˆ°å·²è¿æ¥çš„æ•°æ®æºï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚è¯·å…ˆåœ¨æ•°æ®æºç®¡ç†ä¸­æ·»åŠ æ•°æ®åº“è¿æ¥ã€‚\n"

                        warning_chunk = {
                            "type": "content",
                            "content": warning_text,
                            "provider": chunk.provider,
                            "finished": False,
                            "tenant_id": tenant_id
                        }
                        yield f"data: {json.dumps(warning_chunk, ensure_ascii=False)}\n\n"

        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå™¨é”™è¯¯: {e}")
        error_data = {
            "type": "error",
            "content": f"Stream error: {str(e)}",
            "provider": "unknown",
            "finished": True,
            "tenant_id": tenant_id
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


@router.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completion(
    request: ChatCompletionRequest,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant),
    db: Session = Depends(get_db)
):
    """
    èŠå¤©å®Œæˆæ¥å£
    æ”¯æŒå¤šæä¾›å•†ã€å¤šæ¨¡æ€å’Œæµå¼è¾“å‡º
    è‡ªåŠ¨è·å–ç”¨æˆ·æ•°æ®æºä¿¡æ¯å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    """
    try:
        # è·å–tenant_idï¼Œæ”¯æŒå¼€å‘ç¯å¢ƒä¸‹çš„é»˜è®¤ç§Ÿæˆ·
        tenant_id = getattr(current_user, 'tenant_id', None) or current_user.get('tenant_id', 'default_tenant')
        logger.info(f"Chat completion request for tenant: {tenant_id}")

        # è½¬æ¢æä¾›å•†
        provider = None
        if request.provider:
            try:
                provider = LLMProvider(request.provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {request.provider}"
                )

        # è·å–æ•°æ®æºä¸Šä¸‹æ–‡
        data_sources_context = await _get_data_sources_context(tenant_id, db)
        if data_sources_context:
            logger.info(f"Data sources context retrieved for tenant {tenant_id}, length: {len(data_sources_context)}")
        else:
            logger.info(f"No data sources found for tenant {tenant_id}")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = _convert_chat_messages(request.messages)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ åŒ…å«æ•°æ®æºä¸Šä¸‹æ–‡çš„systemæ¶ˆæ¯
        has_system_message = any(msg.role == "system" for msg in messages)
        if not has_system_message:
            system_prompt = _build_system_prompt_with_context(data_sources_context)
            system_message = LLMMessage(role="system", content=system_prompt)
            messages.insert(0, system_message)
            logger.info("Added system message with data sources context")
        elif data_sources_context:
            # å¦‚æœå·²æœ‰systemæ¶ˆæ¯ï¼Œå°†æ•°æ®æºä¸Šä¸‹æ–‡è¿½åŠ åˆ°ç¬¬ä¸€ä¸ªsystemæ¶ˆæ¯ä¸­
            for i, msg in enumerate(messages):
                if msg.role == "system":
                    enhanced_content = f"{msg.content}\n\n# ç”¨æˆ·çš„æ•°æ®æºä¿¡æ¯\n\n{data_sources_context}"
                    messages[i] = LLMMessage(role="system", content=enhanced_content, thinking=msg.thinking)
                    logger.info("Enhanced existing system message with data sources context")
                    break

        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºåŸå§‹é—®é¢˜
        original_question = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                original_question = msg.content
                break

        # è°ƒç”¨LLMæœåŠ¡
        if request.stream:
            # æµå¼å“åº”
            response_generator = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )

            return StreamingResponse(
                _stream_response_generator(response_generator, tenant_id, db, original_question),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "Cache-Control"
                }
            )
        else:
            # éæµå¼å“åº”
            response = await llm_service.chat_completion(
                tenant_id=tenant_id,
                messages=messages,
                provider=provider,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=request.stream,
                enable_thinking=request.enable_thinking
            )

            if isinstance(response, LLMResponse):
                # æ£€æµ‹å¹¶æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨ä¹‹å‰æå–çš„åŸå§‹é—®é¢˜ï¼‰
                enhanced_content = await _execute_sql_if_needed(
                    response.content,
                    tenant_id,
                    db,
                    original_question
                )

                # æ›´æ–°å“åº”å†…å®¹
                response.content = enhanced_content

                return _convert_response(response)
            else:
                raise HTTPException(
                    status_code=500,
                    detail="Unexpected response type from LLM service"
                )

    except Exception as e:
        logger.error(f"Chat completion failed for tenant {tenant_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chat completion failed: {str(e)}"
        )


@router.get("/providers/status", response_model=ProviderStatusResponse)
async def get_provider_status(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–LLMæä¾›å•†çŠ¶æ€
    """
    try:
        tenant_id = current_user.tenant_id
        status = await llm_service.validate_providers(tenant_id)

        return ProviderStatusResponse(
            zhipu=status.get("zhipu", False),
            openrouter=status.get("openrouter", False)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get provider status: {str(e)}"
        )


@router.get("/models", response_model=AvailableModelsResponse)
async def get_available_models(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    try:
        tenant_id = current_user.tenant_id
        models = await llm_service.get_available_models(tenant_id)

        return AvailableModelsResponse(providers=models)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get available models: {str(e)}"
        )


@router.post("/test")
async def test_llm_service(
    provider: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•LLMæœåŠ¡
    """
    try:
        tenant_id = current_user.tenant_id

        # è½¬æ¢æä¾›å•†
        llm_provider = None
        if provider:
            try:
                llm_provider = LLMProvider(provider)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid provider: {provider}"
                )

        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        test_messages = [
            LLMMessage(
                role="user",
                content="ä½ å¥½ï¼Œè¯·å›å¤ä¸€æ¡ç®€çŸ­çš„æµ‹è¯•æ¶ˆæ¯"
            )
        ]

        # è°ƒç”¨LLMæœåŠ¡
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=llm_provider,
            max_tokens=50
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "LLM service test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"LLM service test failed: {str(e)}"
        }


@router.post("/test/multimodal")
async def test_multimodal(
    current_user: Dict[str, Any] = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€åŠŸèƒ½ï¼ˆéœ€è¦OpenRouterï¼‰
    """
    try:
        tenant_id = current_user.tenant_id

        # åˆ›å»ºå¤šæ¨¡æ€æµ‹è¯•æ¶ˆæ¯
        multimodal_content = [
            {
                "type": "text",
                "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            }
        ]

        test_messages = [
            LLMMessage(
                role="user",
                content=multimodal_content
            )
        ]

        # ä¼˜å…ˆä½¿ç”¨OpenRouterè¿›è¡Œå¤šæ¨¡æ€æµ‹è¯•
        response = await llm_service.chat_completion(
            tenant_id=tenant_id,
            messages=test_messages,
            provider=LLMProvider.OPENROUTER,
            max_tokens=200
        )

        if isinstance(response, LLMResponse):
            return {
                "success": True,
                "response": _convert_response(response).dict(),
                "message": "Multimodal test successful"
            }
        else:
            return {
                "success": False,
                "message": "Unexpected response type"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"Multimodal test failed: {str(e)}"
        }


# ===== æ–°å¢çš„é«˜çº§LLMåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹ =====

@router.get("/test/stream-thinking")
async def test_stream_thinking(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
    """
    try:
        from src.app.services.llm_service import LLMMessage

        messages = [
            LLMMessage(
                role="user",
                content="è¯·è¯¦ç»†åˆ†ææœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨åœºæ™¯"
            )
        ]

        async def stream_generator():
            try:
                async for chunk in llm_service.chat_completion(
                    tenant_id=current_user.id,
                    messages=messages,
                    stream=True,
                    enable_thinking=None  # è‡ªåŠ¨åˆ¤æ–­
                ):
                    yield f"data: {json.dumps(chunk.dict(), ensure_ascii=False)}\n\n"

                yield "data: [DONE]\n\n"
            except Exception as e:
                yield f"data: {{'type': 'error', 'content': '{str(e)}'}}\n\n"

        return StreamingResponse(
            stream_generator(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stream thinking test failed: {str(e)}")


@router.get("/test/intelligent-params")
async def test_intelligent_params(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•æ™ºèƒ½å‚æ•°è°ƒæ•´åŠŸèƒ½
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„é—®é¢˜
        test_cases = [
            {
                "name": "ç®€å•é—®é¢˜",
                "messages": [LLMMessage(role="user", content="ä½ å¥½")]
            },
            {
                "name": "å¤æ‚é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†ã€å¸¸è§çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒæŠ€å·§ä»¥åŠåœ¨å®é™…é¡¹ç›®ä¸­çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¹¶è®¨è®ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
                )]
            },
            {
                "name": "éœ€è¦æ€è€ƒçš„é—®é¢˜",
                "messages": [LLMMessage(
                    role="user",
                    content="ä¸ºä»€ä¹ˆTransformeræ¶æ„èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„RNNæ¨¡å‹ï¼Ÿè¯·ä»æ³¨æ„åŠ›æœºåˆ¶ã€å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€é•¿æœŸä¾èµ–å¤„ç†ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œæ·±å…¥åˆ†æã€‚"
                )]
            }
        ]

        results = []

        for case in test_cases:
            complexity = llm_service.analyze_conversation_complexity(case["messages"])

            # ä½¿ç”¨æ™ºèƒ½å‚æ•°è°ƒç”¨
            response = await llm_service.chat_completion(
                tenant_id=current_user.id,
                messages=case["messages"],
                enable_thinking=None,  # è‡ªåŠ¨åˆ¤æ–­
                temperature=complexity.get("recommend_temperature", 0.7),
                max_tokens=complexity.get("recommend_max_tokens", 4000)
            )

            results.append({
                "case_name": case["name"],
                "complexity_analysis": complexity,
                "response_type": type(response).__name__,
                "success": response is not None
            })

        return {
            "success": True,
            "message": "Intelligent parameter adjustment test completed",
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Intelligent params test failed: {str(e)}")


@router.get("/test/multimodal-upload")
async def test_multimodal_upload(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•å¤šæ¨¡æ€å†…å®¹å¤„ç†å’ŒMinIOä¸Šä¼ 
    """
    try:
        from src.app.services.llm_service import LLMMessage
        from src.app.services.multimodal_processor import multimodal_processor

        # æ„å»ºåŒ…å«å›¾ç‰‡URLçš„æµ‹è¯•æ¶ˆæ¯
        test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

        messages = [
            LLMMessage(
                role="user",
                content=[
                    {
                        "type": "text",
                        "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": test_image_url
                        }
                    }
                ]
            )
        ]

        # æµ‹è¯•å¤šæ¨¡æ€å¤„ç†
        processed_content = await multimodal_processor.process_content_list(
            messages[0].content,
            current_user.id
        )

        # å°è¯•è°ƒç”¨OpenRouterï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            provider=LLMProvider.OPENROUTER,
            stream=False
        )

        return {
            "success": True,
            "message": "Multimodal upload test completed",
            "original_content_count": len(messages[0].content),
            "processed_content_count": len(processed_content),
            "response_received": response is not None,
            "processed_sample": processed_content[:2] if processed_content else []
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Multimodal upload test failed: {str(e)}")


@router.get("/test/tenant-isolation")
async def test_tenant_isolation(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    æµ‹è¯•ç§Ÿæˆ·éš”ç¦»åŠŸèƒ½
    """
    try:
        from src.app.services.tenant_config_manager import tenant_config_manager, ProviderType

        # æµ‹è¯•ç§Ÿæˆ·é…ç½®è·å–
        test_tenant_id = current_user.id
        providers = []

        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            api_key = await tenant_config_manager.get_tenant_api_key(
                test_tenant_id, provider, use_global_fallback=True
            )
            if api_key:
                providers.append(provider.value)

        # æµ‹è¯•æ¨¡å‹é…ç½®è·å–
        model_configs = {}
        for provider in [ProviderType.ZHIPU, ProviderType.OPENROUTER]:
            config = await tenant_config_manager.get_tenant_model_config(test_tenant_id, provider)
            model_configs[provider.value] = config

        # éªŒè¯ç§Ÿæˆ·é…ç½®
        validation_results = await tenant_config_manager.validate_tenant_config(test_tenant_id)

        return {
            "success": True,
            "message": "Tenant isolation test completed",
            "tenant_id": test_tenant_id,
            "available_providers": providers,
            "model_configs": model_configs,
            "validation_results": validation_results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tenant isolation test failed: {str(e)}")


@router.get("/test/all-features")
async def test_all_features(
    current_user: Tenant = Depends(get_current_user_with_tenant)
):
    """
    ç»¼åˆåŠŸèƒ½æµ‹è¯•ç«¯ç‚¹
    """
    try:
        from src.app.services.llm_service import LLMMessage

        # æ„å»ºå¤æ‚çš„æµ‹è¯•åœºæ™¯
        messages = [
            LLMMessage(
                role="user",
                content="ä½œä¸ºä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·åˆ†æå’Œè¯„ä¼°å½“å‰å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„å‘å±•ç°çŠ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ¶æ„ã€åº”ç”¨åœºæ™¯ã€ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å‘å±•è¶‹åŠ¿ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„å»ºè®®ã€‚"
            )
        ]

        # è·å–å¯¹è¯å¤æ‚åº¦åˆ†æ
        complexity_analysis = llm_service.analyze_conversation_complexity(messages)

        # è°ƒç”¨èŠå¤©å®Œæˆï¼ˆå¯ç”¨æ™ºèƒ½æ€è€ƒæ¨¡å¼ï¼‰
        response = await llm_service.chat_completion(
            tenant_id=current_user.id,
            messages=messages,
            stream=False,
            enable_thinking=None,  # è‡ªåŠ¨å¯ç”¨æ€è€ƒæ¨¡å¼
            temperature=complexity_analysis.get("recommend_temperature", 0.7),
            max_tokens=complexity_analysis.get("recommend_max_tokens", 6000)
        )

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = await llm_service.get_available_models(current_user.id)

        # éªŒè¯æä¾›å•†çŠ¶æ€
        provider_status = await llm_service.validate_providers(current_user.id)

        return {
            "success": True,
            "message": "Comprehensive LLM features test completed",
            "complexity_analysis": complexity_analysis,
            "response_received": response is not None,
            "available_models": available_models,
            "provider_status": provider_status,
            "features_tested": [
                "æ™ºèƒ½æ€è€ƒæ¨¡å¼",
                "å¤æ‚åº¦åˆ†æ",
                "æ™ºèƒ½å‚æ•°è°ƒæ•´",
                "å¤šæä¾›å•†æ”¯æŒ",
                "ç§Ÿæˆ·éš”ç¦»"
            ]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Comprehensive test failed: {str(e)}")