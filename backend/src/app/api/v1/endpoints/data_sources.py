"""
# [DATA_SOURCES] æ•°æ®æºè¿žæŽ¥ç®¡ç†APIç«¯ç‚¹

## [HEADER]
**æ–‡ä»¶å**: data_sources.py
**èŒè´£**: å®žçŽ°æ•°æ®æºè¿žæŽ¥çš„å®Œæ•´CRUDæ“ä½œã€æ–‡ä»¶ä¸Šä¼ ã€è¿žæŽ¥æµ‹è¯•ã€æ‰¹é‡ç®¡ç†å’Œæœç´¢åŠŸèƒ½ï¼Œæ”¯æŒPostgreSQL/MySQL/SQLiteå’ŒCSV/Excelæ–‡ä»¶ç±»åž‹ï¼Œç¡®ä¿ç§Ÿæˆ·éš”ç¦»å’Œæ•°æ®å®‰å…¨
**ä½œè€…**: Data Agent Team
**ç‰ˆæœ¬**: 1.0.0
**å˜æ›´è®°å½•**:
- v1.0.0 (2026-01-01): åˆå§‹ç‰ˆæœ¬ - å®žçŽ°Story 2.3è¦æ±‚çš„æ•°æ®æºç®¡ç†API

## [INPUT]
- **tenant_id: str** - ç§Ÿæˆ·IDï¼ˆä»ŽæŸ¥è¯¢å‚æ•°èŽ·å–ï¼Œå®žé™…åº”ä»ŽJWTæå–ï¼‰
- **user_id: str** - ç”¨æˆ·IDï¼ˆç”¨äºŽæƒé™éªŒè¯å’Œæ“ä½œå®¡è®¡ï¼‰
- **connection_id: str** - æ•°æ®æºè¿žæŽ¥IDï¼ˆè·¯å¾„å‚æ•°ï¼‰
- **request_data: DataSourceCreateRequest** - åˆ›å»ºæ•°æ®æºè¯·æ±‚æ•°æ®ï¼ˆPydanticæ¨¡åž‹ï¼‰
  - name: æ•°æ®æºåç§°
  - connection_string: æ•°æ®åº“è¿žæŽ¥å­—ç¬¦ä¸²
  - db_type: æ•°æ®åº“ç±»åž‹
  - create_db_if_not_exists: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“
- **update_data: DataSourceUpdateRequest** - æ›´æ–°æ•°æ®æºè¯·æ±‚æ•°æ®ï¼ˆPydanticæ¨¡åž‹ï¼‰
- **file: UploadFile** - ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶ï¼ˆCSV/Excel/SQLiteï¼‰
- **db: Session** - æ•°æ®åº“ä¼šè¯ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥èŽ·å–ï¼‰

## [OUTPUT]
- **data_source_list: list** - æ•°æ®æºè¿žæŽ¥åˆ—è¡¨
  - id: è¿žæŽ¥ID
  - name: æ•°æ®æºåç§°
  - db_type: æ•°æ®åº“ç±»åž‹
  - status: è¿žæŽ¥çŠ¶æ€
  - host: ä¸»æœºåœ°å€
  - port: ç«¯å£å·
  - database_name: æ•°æ®åº“åç§°
  - last_tested_at: æœ€åŽæµ‹è¯•æ—¶é—´
  - test_result: æµ‹è¯•ç»“æžœè¯¦æƒ…
- **overview_stats: dict** - æ•°æ®æºæ¦‚è§ˆç»Ÿè®¡
  - databases: æ•°æ®åº“ç»Ÿè®¡
  - documents: æ–‡æ¡£ç»Ÿè®¡
  - storage: å­˜å‚¨ä½¿ç”¨æƒ…å†µ
  - recent_activity: æœ€è¿‘æ´»åŠ¨è®°å½•
- **test_result: dict** - è¿žæŽ¥æµ‹è¯•ç»“æžœ
  - success: æµ‹è¯•æ˜¯å¦æˆåŠŸ
  - message: æµ‹è¯•æ¶ˆæ¯
  - error_code: é”™è¯¯ä»£ç 
  - response_time_ms: å“åº”æ—¶é—´
- **error_response: HTTPException** - é”™è¯¯å“åº”ï¼ˆ400, 403, 404, 500ï¼‰

## [LINK]
**ä¸Šæ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- [../../data/database.py](../../data/database.py) - get_db(), Session
- [../../data/models.py](../../data/models.py) - DataSourceConnection, Tenant, DataSourceConnectionStatus, TenantStatus
- [../../services/data_source_service.py](../../services/data_source_service.py) - data_source_service, æ•°æ®æºCRUDæ“ä½œ
- [../../services/connection_test_service.py](../../services/connection_test_service.py) - connection_test_service, è¿žæŽ¥æµ‹è¯•
- [../../services/minio_client.py](../../services/minio_client.py) - minio_service, æ–‡ä»¶å­˜å‚¨

**ä¸‹æ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- æ— ï¼ˆAPIç«¯ç‚¹æ˜¯å¶å­æ¨¡å—ï¼‰

**è°ƒç”¨æ–¹**:
- å‰ç«¯æ•°æ®æºç®¡ç†é¡µé¢ - è°ƒç”¨æ•°æ®æºCRUD API
- å‰ç«¯æ•°æ®è¿žæŽ¥è¡¨å• - åˆ›å»ºå’Œæµ‹è¯•æ•°æ®æºè¿žæŽ¥
- å‰ç«¯ä»ªè¡¨æ¿ - æ˜¾ç¤ºæ•°æ®æºæ¦‚è§ˆç»Ÿè®¡
- æ–‡ä»¶ä¸Šä¼ ç»„ä»¶ - ä¸Šä¼ CSV/Excelæ•°æ®æ–‡ä»¶

## [POS]
**è·¯å¾„**: backend/src/app/api/v1/endpoints/data_sources.py
**æ¨¡å—å±‚çº§**: Level 3 - APIç«¯ç‚¹å±‚
**ä¾èµ–æ·±åº¦**: ç›´æŽ¥ä¾èµ– data/*, services/*ï¼›è¢«å‰ç«¯æ•°æ®ç®¡ç†æ¨¡å—è°ƒç”¨
"""

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form, Request
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
from datetime import datetime
import logging
import uuid
import os
import tempfile
import json

from src.app.data.database import get_db
from src.app.data.models import DataSourceConnection, Tenant, DataSourceConnectionStatus, TenantStatus
from src.app.services.data_source_service import data_source_service
from src.app.services.connection_test_service import connection_test_service
from src.app.services.minio_client import minio_service

logger = logging.getLogger(__name__)
router = APIRouter()


# Pydanticæ¨¡åž‹å®šä¹‰
class DataSourceCreateRequest(BaseModel):
    """åˆ›å»ºæ•°æ®æºè¯·æ±‚æ¨¡åž‹"""
    name: str = Field(..., min_length=1, max_length=255, description="æ•°æ®æºåç§°")
    connection_string: str = Field(..., min_length=1, description="æ•°æ®åº“è¿žæŽ¥å­—ç¬¦ä¸²")
    db_type: str = Field(default="postgresql", description="æ•°æ®åº“ç±»åž‹")
    create_db_if_not_exists: bool = Field(default=False, description="å¦‚æžœæ•°æ®åº“ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º")


class DataSourceUpdateRequest(BaseModel):
    """æ›´æ–°æ•°æ®æºè¯·æ±‚æ¨¡åž‹"""
    name: Optional[str] = Field(None, min_length=1, max_length=255, description="æ•°æ®æºåç§°")
    connection_string: Optional[str] = Field(None, min_length=1, description="æ•°æ®åº“è¿žæŽ¥å­—ç¬¦ä¸²")
    db_type: Optional[str] = Field(None, description="æ•°æ®åº“ç±»åž‹")
    is_active: Optional[bool] = Field(None, description="æ˜¯å¦æ¿€æ´»")


class ConnectionTestRequest(BaseModel):
    """è¿žæŽ¥æµ‹è¯•è¯·æ±‚æ¨¡åž‹"""
    connection_string: str = Field(..., min_length=1, description="æ•°æ®åº“è¿žæŽ¥å­—ç¬¦ä¸²")
    db_type: str = Field(default="postgresql", description="æ•°æ®åº“ç±»åž‹")


class DataSourceResponse(BaseModel):
    """æ•°æ®æºå“åº”æ¨¡åž‹"""
    id: str
    tenant_id: str
    name: str
    db_type: str
    status: str
    host: Optional[str]
    port: Optional[int]
    database_name: Optional[str]
    last_tested_at: Optional[datetime]
    test_result: Optional[Dict[str, Any]]
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# APIç«¯ç‚¹å®žçŽ°
@router.get("/", summary="èŽ·å–æ•°æ®æºè¿žæŽ¥åˆ—è¡¨", response_model=List[DataSourceResponse])
async def get_data_sources(
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db_type: Optional[str] = None,
    skip: int = 0,
    limit: int = 100,
    active_only: bool = True,
    db: Session = Depends(get_db)
):
    """
    èŽ·å–æ•°æ®æºè¿žæŽ¥åˆ—è¡¨
    æ”¯æŒç§Ÿæˆ·éš”ç¦»å’Œç­›é€‰åŠŸèƒ½
    """
    # TODO: ä»ŽJWT tokenæˆ–è®¤è¯ä¸­é—´ä»¶èŽ·å–tenant_id
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    try:
        connections = await data_source_service.get_data_sources(
            tenant_id=tenant_id,
            db=db,
            active_only=active_only,
            skip=skip,
            limit=limit
        )

        # è¿‡æ»¤æ•°æ®åº“ç±»åž‹
        if db_type:
            connections = [conn for conn in connections if conn.db_type == db_type]

        return [DataSourceResponse.from_orm(conn) for conn in connections]

    except Exception as e:
        logger.error(f"Failed to fetch data sources: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch data sources: {str(e)}"
        )


# ============================================================================
# å›ºå®šè·¯å¾„ç«¯ç‚¹ï¼ˆå¿…é¡»åœ¨ /{connection_id} ä¹‹å‰å®šä¹‰ï¼Œå¦åˆ™ä¼šè¢«åŠ¨æ€è·¯ç”±åŒ¹é…ï¼‰
# ============================================================================

# æ”¯æŒçš„æ–‡ä»¶ç±»åž‹é…ç½®
SUPPORTED_FILE_TYPES = {
    'csv': {
        'mime_types': ['text/csv', 'application/vnd.ms-excel'],
        'max_size_mb': 100,
        'extensions': ['.csv']
    },
    'xlsx': {
        'mime_types': ['application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'],
        'max_size_mb': 100,
        'extensions': ['.xlsx']
    },
    'xls': {
        'mime_types': ['application/vnd.ms-excel'],
        'max_size_mb': 100,
        'extensions': ['.xls']
    },
    'sqlite': {
        'mime_types': ['application/x-sqlite3', 'application/vnd.sqlite3', 'application/octet-stream'],
        'max_size_mb': 500,
        'extensions': ['.db', '.sqlite', '.sqlite3']
    }
}


def _validate_file_type(filename: str, content_type: str, file_size: int) -> Dict[str, Any]:
    """éªŒè¯æ–‡ä»¶ç±»åž‹å’Œå¤§å°"""
    ext = os.path.splitext(filename)[1].lower()

    for file_type, config in SUPPORTED_FILE_TYPES.items():
        if ext in config['extensions']:
            max_size_bytes = config['max_size_mb'] * 1024 * 1024
            if file_size > max_size_bytes:
                return {
                    "valid": False,
                    "error": "FILE_TOO_LARGE",
                    "message": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤§å…è®¸ {config['max_size_mb']}MB"
                }
            return {
                "valid": True,
                "file_type": file_type,
                "extension": ext
            }

    return {
        "valid": False,
        "error": "UNSUPPORTED_FILE_TYPE",
        "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»åž‹: {ext}ã€‚æ”¯æŒçš„ç±»åž‹: .csv, .xlsx, .xls, .db, .sqlite, .sqlite3"
    }


@router.post("/upload", summary="ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ›å»ºæ•°æ®æº", status_code=status.HTTP_201_CREATED)
async def upload_data_source(
    file: UploadFile = File(..., description="æ•°æ®æ–‡ä»¶ (CSV, Excel, SQLite)"),
    name: str = Form(..., description="æ•°æ®æºåç§°"),
    db_type: Optional[str] = Form(None, description="æ•°æ®ç±»åž‹"),
    tenant_id: Optional[str] = None,
    request: Request = None,
    db: Session = Depends(get_db)
):
    """
    ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ›å»ºæ•°æ®æº
    æ”¯æŒ CSVã€Excel (.xls/.xlsx) å’Œ SQLite æ•°æ®åº“ (.db/.sqlite/.sqlite3) æ–‡ä»¶
    """
    # ä»ŽæŸ¥è¯¢å‚æ•°èŽ·å–tenant_id
    if not tenant_id and request:
        tenant_id = request.query_params.get("tenant_id")

    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    # éªŒè¯æ–‡ä»¶å
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º"
        )

    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        file_size = len(file_content)

        # éªŒè¯æ–‡ä»¶ç±»åž‹å’Œå¤§å°
        validation_result = _validate_file_type(
            file.filename,
            file.content_type or "application/octet-stream",
            file_size
        )

        if not validation_result["valid"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=validation_result["message"]
            )

        detected_file_type = validation_result["file_type"]

        # ç”Ÿæˆå”¯ä¸€çš„å­˜å‚¨è·¯å¾„
        file_id = str(uuid.uuid4())
        file_ext = validation_result["extension"]
        storage_path = f"data-sources/{tenant_id}/{file_id}{file_ext}"

        # ðŸš¨ ä¿®å¤ç­–ç•¥ï¼šå¼ºåˆ¶è½åœ°é€»è¾‘ - ç¡®ä¿æ–‡ä»¶ä¸€å®šä¼šä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
        import io
        import os
        
        # å®šä¹‰æœ¬åœ°å­˜å‚¨ç›®å½•ï¼ˆä½¿ç”¨Dockerå·æŒ‚è½½çš„ç›®å½•ï¼‰
        local_upload_dir = "/app/uploads/data-sources"
        tenant_upload_dir = os.path.join(local_upload_dir, tenant_id)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(tenant_upload_dir, exist_ok=True)
        
        # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå®¹å™¨å†…ç»å¯¹è·¯å¾„ï¼‰
        local_file_path = os.path.join(tenant_upload_dir, f"{file_id}{file_ext}")
        
        # ðŸ”¥ å…³é”®ä¿®å¤ï¼šæ— è®ºMinIOæ˜¯å¦æˆåŠŸï¼Œéƒ½å…ˆä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
        try:
            with open(local_file_path, "wb") as f:
                f.write(file_content)
            logger.info(f"âœ… æ–‡ä»¶å·²å¼ºåˆ¶ä¿å­˜åˆ°æœ¬åœ°: {local_file_path}")
        except Exception as save_error:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°å¤±è´¥: {save_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(save_error)}"
            )
        
        # å°è¯•ä¸Šä¼ åˆ° MinIOï¼ˆå¯é€‰ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰
        minio_upload_success = False
        try:
            upload_success = minio_service.upload_file(
                bucket_name="data-sources",
                object_name=storage_path,
                file_data=io.BytesIO(file_content),
                file_size=file_size,
                content_type=file.content_type or "application/octet-stream"
            )
            if upload_success:
                minio_upload_success = True
                logger.info(f"âœ… æ–‡ä»¶å·²åŒæ—¶ä¸Šä¼ åˆ°MinIO: {storage_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ MinIOä¸Šä¼ å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        # åˆ›å»ºæ•°æ®æºè®°å½•
        # ðŸ”§ ä¿®å¤ï¼šç»Ÿä¸€è·¯å¾„æ ¼å¼
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆå®¹å™¨å†…ç»å¯¹è·¯å¾„ï¼‰ï¼Œç¡®ä¿Agentä¸€å®šèƒ½æ‰¾åˆ°æ–‡ä»¶
        # æ ¼å¼ï¼š/app/uploads/data-sources/{tenant_id}/{file_id}{ext}
        connection_string = local_file_path  # ç›´æŽ¥ä½¿ç”¨å®¹å™¨å†…ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿Agentèƒ½æ‰¾åˆ°æ–‡ä»¶

        new_connection = DataSourceConnection(
            tenant_id=tenant_id,
            name=name,
            db_type=db_type or detected_file_type,
            connection_string=connection_string,
            status=DataSourceConnectionStatus.ACTIVE,
            host=None,
            port=None,
            database_name=file.filename
        )

        db.add(new_connection)
        db.commit()
        db.refresh(new_connection)

        logger.info(f"Created file-based data source '{name}' for tenant '{tenant_id}'")

        return {
            "id": str(new_connection.id),
            "tenant_id": new_connection.tenant_id,
            "name": new_connection.name,
            "db_type": new_connection.db_type,
            "status": new_connection.status.value if hasattr(new_connection.status, 'value') else str(new_connection.status),
            "host": new_connection.host,
            "port": new_connection.port,
            "database_name": new_connection.database_name,
            "last_tested_at": new_connection.last_tested_at,
            "test_result": new_connection.test_result,
            "created_at": new_connection.created_at,
            "updated_at": new_connection.updated_at,
            "file_info": {
                "original_name": file.filename,
                "file_type": detected_file_type,
                "file_size": file_size,
                "storage_path": storage_path
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to upload data source: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ä¸Šä¼ æ•°æ®æºå¤±è´¥: {str(e)}"
        )


@router.get("/overview", summary="èŽ·å–æ•°æ®æºæ¦‚è§ˆç»Ÿè®¡")
async def get_data_sources_overview_route(
    tenant_id: str = None,
    user_id: str = None,
    db: Session = Depends(get_db)
):
    """
    èŽ·å–æ•°æ®æºå’Œæ–‡æ¡£çš„æ¦‚è§ˆç»Ÿè®¡ä¿¡æ¯
    ç”¨äºŽä»ªè¡¨æ¿å±•ç¤ºï¼ŒåŒ…å«å®‰å…¨çš„å­˜å‚¨é…é¢è®¡ç®—å’Œæ•°æ®éš”ç¦»
    """
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="user_id is required"
        )

    # å¼ºåŒ–çš„ç§Ÿæˆ·éªŒè¯å’Œæƒé™æ£€æŸ¥
    await _validate_overview_access(tenant_id, user_id, db)

    try:
        # å®‰å…¨åœ°èŽ·å–æ•°æ®åº“è¿žæŽ¥ç»Ÿè®¡
        db_stats = await _get_secure_database_stats(tenant_id, db)
        doc_stats = await _get_secure_document_stats(tenant_id, db)
        storage_stats = await _get_secure_storage_stats(tenant_id, db)
        recent_activity = await _get_secure_recent_activity(tenant_id, db)

        logger.info(f"Overview accessed by user {user_id} for tenant {tenant_id}")

        return {
            "databases": db_stats,
            "documents": doc_stats,
            "storage": storage_stats,
            "recent_activity": recent_activity
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch data sources overview for tenant {tenant_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to fetch overview statistics"
        )


@router.get("/search", summary="æœç´¢æ•°æ®æº")
async def search_data_sources_route(
    q: str = "",
    type: str = "all",
    status_filter: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
    tenant_id: str = None,
    user_id: str = None,
    db: Session = Depends(get_db)
):
    """æœç´¢æ•°æ®æºå’Œæ–‡æ¡£"""
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="user_id is required"
        )

    await _validate_search_parameters(q, type, status_filter, date_from, date_to, page, limit)
    await _validate_tenant_search_access(tenant_id, user_id, db)

    try:
        results = []
        total_count = 0

        if type in ["all", "database"]:
            query = db.query(DataSourceConnection).filter(
                DataSourceConnection.tenant_id == tenant_id,
                DataSourceConnection.status != DataSourceConnectionStatus.INACTIVE
            )

            if q:
                safe_query = _sanitize_search_query(q)
                query = query.filter(
                    DataSourceConnection.name.contains(safe_query) |
                    DataSourceConnection.db_type.contains(safe_query)
                )

            if status_filter:
                valid_statuses = _validate_and_parse_statuses(status_filter)
                if valid_statuses:
                    query = query.filter(DataSourceConnection.status.in_(valid_statuses))

            if date_from or date_to:
                date_range = _validate_and_parse_date_range(date_from, date_to)
                if date_range['from']:
                    query = query.filter(DataSourceConnection.created_at >= date_range['from'])
                if date_range['to']:
                    query = query.filter(DataSourceConnection.created_at <= date_range['to'])

            db_total = query.with_entities(DataSourceConnection.id).count()
            total_count += db_total

            safe_limit = min(limit, 100)
            safe_page = max(1, min(page, 1000))

            db_results = query.offset((safe_page - 1) * safe_limit).limit(safe_limit).all()

            for conn in db_results:
                if conn.tenant_id != tenant_id:
                    continue
                results.append({
                    "id": conn.id,
                    "type": "database",
                    "name": conn.name,
                    "status": conn.status.value if hasattr(conn.status, 'value') else str(conn.status),
                    "created_at": conn.created_at.isoformat(),
                    "updated_at": conn.updated_at.isoformat(),
                    "db_type": conn.db_type,
                    "host": conn.host,
                    "port": conn.port
                })

        total_pages = (total_count + safe_limit - 1) // safe_limit

        return {
            "results": results,
            "total": total_count,
            "page": safe_page,
            "limit": safe_limit,
            "total_pages": total_pages
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to search data sources for tenant {tenant_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Search operation failed"
        )


class BulkDeleteRequestModel(BaseModel):
    """æ‰¹é‡åˆ é™¤è¯·æ±‚æ¨¡åž‹"""
    item_ids: List[str] = Field(..., min_length=1, max_length=50)
    item_type: str = Field(..., pattern=r"^(database|document)$")
    confirmation_token: Optional[str] = None


@router.post("/bulk-delete", summary="æ‰¹é‡åˆ é™¤æ•°æ®æº")
async def bulk_delete_data_sources_route(
    request: BulkDeleteRequestModel,
    tenant_id: str = None,
    user_id: str = None,
    db: Session = Depends(get_db)
):
    """æ‰¹é‡åˆ é™¤æ•°æ®æºæˆ–æ–‡æ¡£"""
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="user_id is required"
        )

    if len(request.item_ids) > 50:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Cannot delete more than 50 items at once"
        )

    tenant = db.query(Tenant).filter(
        Tenant.id == tenant_id,
        Tenant.status == TenantStatus.ACTIVE
    ).first()

    if not tenant:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid or inactive tenant"
        )

    success_count = 0
    error_count = 0
    errors = []
    deleted_items = []

    try:
        if request.item_type == "database":
            await _validate_bulk_database_delete_permission(tenant_id, user_id, request.item_ids, db)

            for item_id in request.item_ids:
                try:
                    connection = db.query(DataSourceConnection).filter(
                        DataSourceConnection.id == item_id,
                        DataSourceConnection.tenant_id == tenant_id,
                        DataSourceConnection.status != DataSourceConnectionStatus.INACTIVE
                    ).first()

                    if not connection:
                        error_count += 1
                        errors.append({"item_id": item_id, "error": "Data source not found"})
                        continue

                    if not await _can_delete_data_source(connection, db):
                        error_count += 1
                        errors.append({"item_id": item_id, "error": "Data source cannot be deleted"})
                        continue

                    connection.status = DataSourceConnectionStatus.INACTIVE
                    connection.updated_at = datetime.now()
                    deleted_items.append({"item_id": item_id, "name": connection.name, "type": "database"})
                    success_count += 1

                except Exception as e:
                    error_count += 1
                    errors.append({"item_id": item_id, "error": str(e)})

        elif request.item_type == "document":
            for item_id in request.item_ids:
                error_count += 1
                errors.append({"item_id": item_id, "error": "Document deletion not implemented"})

        db.commit()
        return {
            "success_count": success_count,
            "error_count": error_count,
            "errors": errors,
            "deleted_items": deleted_items
        }

    except HTTPException:
        db.rollback()
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Bulk delete failed: {str(e)}"
        )


@router.get("/types/supported", summary="èŽ·å–æ”¯æŒçš„æ•°æ®æºç±»åž‹")
async def get_supported_data_source_types_route():
    """èŽ·å–æ”¯æŒçš„æ•°æ®æºç±»åž‹åˆ—è¡¨"""
    try:
        return connection_test_service.get_supported_database_types()
    except Exception as e:
        logger.error(f"Failed to get supported database types: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get supported database types: {str(e)}"
        )


# ============================================================================
# å›ºå®šè·¯å¾„ POST ç«¯ç‚¹ï¼ˆå¿…é¡»åœ¨åŠ¨æ€è·¯å¾„ç«¯ç‚¹ä¹‹å‰å®šä¹‰ï¼‰
# ============================================================================

async def _create_database_if_not_exists(connection_string: str, db_type: str) -> tuple[bool, str]:
    """
    å¦‚æžœæ•°æ®åº“ä¸å­˜åœ¨åˆ™åˆ›å»ºå®ƒ
    è¿”å›ž (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯)
    """
    if db_type != "postgresql":
        return False, "ä»…æ”¯æŒ PostgreSQL æ•°æ®åº“è‡ªåŠ¨åˆ›å»º"

    import re
    from sqlalchemy import create_engine, text

    # è§£æžè¿žæŽ¥å­—ç¬¦ä¸²èŽ·å–æ•°æ®åº“å
    # æ ¼å¼: postgresql://user:password@host:port/database_name
    pattern = r"postgresql://([^:]+):([^@]+)@([^:]+):(\d+)/([^/?]+)"
    match = re.match(pattern, connection_string)

    if not match:
        return False, "æ— æ³•è§£æžè¿žæŽ¥å­—ç¬¦ä¸²"

    user, password, host, port, database_name = match.groups()

    # ðŸ”§ DockerçŽ¯å¢ƒä¿®å¤: å°† localhost æ›¿æ¢ä¸º host.docker.internal
    # åœ¨ Docker å®¹å™¨å†…ï¼Œlocalhost æŒ‡å‘å®¹å™¨è‡ªå·±ï¼Œéœ€è¦ä½¿ç”¨ host.docker.internal è®¿é—®å®¿ä¸»æœº
    if host in ("localhost", "127.0.0.1"):
        host = "host.docker.internal"
        logger.info(f"DockerçŽ¯å¢ƒ: æ£€æµ‹åˆ°localhostï¼Œè‡ªåŠ¨æ›¿æ¢ä¸º host.docker.internal")

    # è¿žæŽ¥åˆ° postgres é»˜è®¤æ•°æ®åº“æ¥åˆ›å»ºæ–°æ•°æ®åº“
    master_connection_string = f"postgresql://{user}:{password}@{host}:{port}/postgres"
    
    try:
        engine = create_engine(master_connection_string)
        with engine.connect() as conn:
            conn.execution_options(isolation_level="AUTOCOMMIT")
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            result = conn.execute(
                text("SELECT 1 FROM pg_database WHERE datname = :dbname"),
                {"dbname": database_name}
            )
            
            if result.fetchone():
                logger.info(f"æ•°æ®åº“ '{database_name}' å·²å­˜åœ¨")
                return True, f"æ•°æ®åº“ '{database_name}' å·²å­˜åœ¨"
            
            # åˆ›å»ºæ•°æ®åº“
            # æ³¨æ„ï¼šæ•°æ®åº“åä¸èƒ½ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼Œéœ€è¦æ‰‹åŠ¨è½¬ä¹‰
            safe_db_name = database_name.replace('"', '""')
            conn.execute(text(f'CREATE DATABASE "{safe_db_name}"'))
            logger.info(f"æˆåŠŸåˆ›å»ºæ•°æ®åº“ '{database_name}'")
            return True, f"æˆåŠŸåˆ›å»ºæ•°æ®åº“ '{database_name}'"
            
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“å¤±è´¥: {e}")
        return False, f"åˆ›å»ºæ•°æ®åº“å¤±è´¥: {str(e)}"


@router.post("/", summary="åˆ›å»ºæ•°æ®æºè¿žæŽ¥", status_code=status.HTTP_201_CREATED, response_model=DataSourceResponse)
async def create_data_source(
    request: DataSourceCreateRequest,
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºæ–°çš„æ•°æ®æºè¿žæŽ¥
    è‡ªåŠ¨åŠ å¯†è¿žæŽ¥å­—ç¬¦ä¸²å¹¶è§£æžè¿žæŽ¥ä¿¡æ¯
    å¦‚æžœ create_db_if_not_exists=Trueï¼Œä¼šè‡ªåŠ¨åˆ›å»ºä¸å­˜åœ¨çš„æ•°æ®åº“

    ðŸ†• Excel æ–‡ä»¶è‡ªåŠ¨è½¬æ¢ä¸º SQLite:
    - å½“ db_type ä¸º 'xlsx' æˆ– 'xls' æ—¶ï¼Œè‡ªåŠ¨å°† Excel æ–‡ä»¶è½¬æ¢ä¸º SQLite æ•°æ®åº“
    - è½¬æ¢åŽçš„æ•°æ®åº“æ”¯æŒå®Œæ•´çš„ SQL è¯­æ³•ï¼ˆJOIN, GROUP BY, èšåˆå‡½æ•°ç­‰ï¼‰
    - SQLite æ–‡ä»¶å­˜å‚¨åœ¨ backend/data/sqlite_databases/ ç›®å½•ä¸‹
    """
    logger.info(f"æ”¶åˆ°åˆ›å»ºæ•°æ®æºè¯·æ±‚: tenant_id={tenant_id}")
    logger.info(f"  - name: '{request.name}'")
    logger.info(f"  - db_type: '{request.db_type}'")
    logger.info(f"  - connection_string: '{request.connection_string}'")
    logger.info(f"  - create_db_if_not_exists: {request.create_db_if_not_exists}")
    logger.info(f"  - requestå¯¹è±¡: {request.model_dump()}")

    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    # æœ€ç»ˆä½¿ç”¨çš„è¿žæŽ¥å­—ç¬¦ä¸²å’Œç±»åž‹ï¼ˆå¯èƒ½ä¼šè¢«ä¿®æ”¹ï¼‰
    final_connection_string = request.connection_string
    final_db_type = request.db_type
    conversion_metadata = None

    try:
        # ðŸ†• Excel æ–‡ä»¶è‡ªåŠ¨è½¬æ¢ä¸º SQLite
        if request.db_type in ["xlsx", "xls"]:
            logger.info(f"ðŸ“Š æ£€æµ‹åˆ° Excel æ–‡ä»¶ï¼Œå¼€å§‹è‡ªåŠ¨è½¬æ¢ä¸º SQLite...")

            from src.app.services.excel_to_sqlite_service import get_excel_to_sqlite_service

            excel_service = get_excel_to_sqlite_service()

            try:
                # è½¬æ¢ Excel ä¸º SQLite
                sqlite_db_path, metadata = excel_service.convert_excel_to_sqlite(
                    excel_file_path=request.connection_string,
                    db_name=request.name,
                    tenant_id=tenant_id
                )

                # ä½¿ç”¨ SQLite è¿žæŽ¥å­—ç¬¦ä¸²
                final_connection_string = excel_service.get_sqlite_connection_string(sqlite_db_path)
                final_db_type = "sqlite"
                conversion_metadata = metadata

                logger.info(f"âœ… Excel è½¬æ¢æˆåŠŸ:")
                logger.info(f"  - SQLite è·¯å¾„: {sqlite_db_path}")
                logger.info(f"  - è¡¨æ•°é‡: {metadata.get('total_tables')}")
                logger.info(f"  - æ€»è¡Œæ•°: {metadata.get('total_rows')}")
                logger.info(f"  - è½¬æ¢è€—æ—¶: {metadata.get('conversion_time_seconds')}s")

            except Exception as e:
                logger.error(f"âŒ Excel è½¬æ¢å¤±è´¥: {e}")
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Excel æ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}"
                )

        # å¦‚æžœéœ€è¦è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“ï¼ˆPostgreSQLï¼‰
        elif request.create_db_if_not_exists and request.db_type == "postgresql":
            success, message = await _create_database_if_not_exists(
                request.connection_string,
                request.db_type
            )
            logger.info(f"è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“ç»“æžœ: success={success}, message={message}")
            if not success:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=message
                )

        # å…ˆåŠ å¯†è¿žæŽ¥å­—ç¬¦ä¸²
        encrypted_string = data_source_service.encryption_service.encrypt_connection_string(
            final_connection_string
        )

        # åˆ›å»ºæ•°æ®æºè¿žæŽ¥(ç›´æŽ¥ä½¿ç”¨åŠ å¯†åŽçš„å­—ç¬¦ä¸²)
        new_connection = DataSourceConnection(
            tenant_id=tenant_id,
            name=request.name,
            db_type=final_db_type,
            _connection_string=encrypted_string,  # ç›´æŽ¥è®¾ç½®åŠ å¯†åŽçš„å­—ç¬¦ä¸²åˆ°ç§æœ‰å­—æ®µ
            status=DataSourceConnectionStatus.TESTING
        )

        # è§£æžè¿žæŽ¥å­—ç¬¦ä¸²èŽ·å–è¿žæŽ¥ä¿¡æ¯
        parsed_info = data_source_service._parse_connection_string(
            final_connection_string,
            final_db_type
        )
        new_connection.host = parsed_info.get("host")
        new_connection.port = parsed_info.get("port")
        new_connection.database_name = parsed_info.get("database")

        # ä¿å­˜åˆ°æ•°æ®åº“
        db.add(new_connection)
        db.commit()
        db.refresh(new_connection)

        logger.info(f"Created data source '{request.name}' for tenant '{tenant_id}'")

        # è‡ªåŠ¨æµ‹è¯•è¿žæŽ¥å¹¶æ›´æ–°çŠ¶æ€
        try:
            logger.info(f"Auto-testing connection for data source '{request.name}'...")
            test_result = await connection_test_service.test_connection(
                connection_string=request.connection_string,
                db_type=request.db_type
            )

            # æ›´æ–°è¿žæŽ¥çš„æµ‹è¯•ç»“æžœå’ŒçŠ¶æ€
            new_connection.update_test_result(test_result.to_dict())
            db.commit()
            db.refresh(new_connection)

            logger.info(f"Connection test completed: {test_result.success}")
        except Exception as e:
            logger.warning(f"Auto-test failed for data source '{request.name}': {e}")
            # å³ä½¿æµ‹è¯•å¤±è´¥,ä¹Ÿè¿”å›žåˆ›å»ºçš„æ•°æ®æº(çŠ¶æ€ä¿æŒä¸ºTESTING)

        return DataSourceResponse.from_orm(new_connection)

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Failed to create data source: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create data source: {str(e)}"
        )


# ============================================================================
# åŠ¨æ€è·¯å¾„ç«¯ç‚¹ï¼ˆå¿…é¡»åœ¨å›ºå®šè·¯å¾„ç«¯ç‚¹ä¹‹åŽå®šä¹‰ï¼‰
# ============================================================================

@router.get("/{connection_id}", summary="èŽ·å–æ•°æ®æºè¿žæŽ¥è¯¦æƒ…", response_model=DataSourceResponse)
async def get_data_source(
    connection_id: str,
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db: Session = Depends(get_db)
):
    """
    æ ¹æ®IDèŽ·å–æ•°æ®æºè¿žæŽ¥è¯¦æƒ…
    ç¡®ä¿ç§Ÿæˆ·éš”ç¦»
    """
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    try:
        connection = await data_source_service.get_data_source_by_id(
            data_source_id=connection_id,
            tenant_id=tenant_id,
            db=db
        )

        if not connection:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Data source connection not found"
            )

        return DataSourceResponse.from_orm(connection)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch data source {connection_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch data source: {str(e)}"
        )


@router.put("/{connection_id}", summary="æ›´æ–°æ•°æ®æºè¿žæŽ¥", response_model=DataSourceResponse)
async def update_data_source(
    connection_id: str,
    request: DataSourceUpdateRequest,
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db: Session = Depends(get_db)
):
    """
    æ›´æ–°æ•°æ®æºè¿žæŽ¥ä¿¡æ¯
    æ”¯æŒéƒ¨åˆ†æ›´æ–°ï¼Œè‡ªåŠ¨é‡æ–°åŠ å¯†è¿žæŽ¥å­—ç¬¦ä¸²
    """
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    try:
        # èŽ·å–çŽ°æœ‰è¿žæŽ¥
        connection = await data_source_service.get_data_source_by_id(
            data_source_id=connection_id,
            tenant_id=tenant_id,
            db=db
        )

        if not connection:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Data source connection not found"
            )

        # æ›´æ–°å­—æ®µ
        update_data = request.dict(exclude_unset=True)

        if "name" in update_data:
            # æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(DataSourceConnection).filter(
                DataSourceConnection.tenant_id == tenant_id,
                DataSourceConnection.name == update_data["name"],
                DataSourceConnection.id != connection_id
            ).first()

            if existing:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Data source name already exists for this tenant"
                )

            connection.name = update_data["name"]

        if "connection_string" in update_data:
            # åŠ å¯†æ–°çš„è¿žæŽ¥å­—ç¬¦ä¸²
            encrypted_string = data_source_service.encryption_service.encrypt_connection_string(
                update_data["connection_string"]
            )
            connection.connection_string = encrypted_string

            # è§£æžæ–°è¿žæŽ¥å­—ç¬¦ä¸²
            parsed_info = data_source_service._parse_connection_string(
                update_data["connection_string"],
                connection.db_type
            )
            connection.host = parsed_info.get("host")
            connection.port = parsed_info.get("port")
            connection.database_name = parsed_info.get("database")

        if "db_type" in update_data:
            connection.db_type = update_data["db_type"]

        if "is_active" in update_data:
            if update_data["is_active"]:
                connection.status = DataSourceConnectionStatus.ACTIVE
            else:
                connection.status = DataSourceConnectionStatus.INACTIVE

        connection.updated_at = datetime.now()

        db.commit()
        db.refresh(connection)

        logger.info(f"Updated data source {connection_id}")
        return DataSourceResponse.from_orm(connection)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update data source {connection_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update data source: {str(e)}"
        )


@router.delete("/{connection_id}", summary="åˆ é™¤æ•°æ®æºè¿žæŽ¥")
async def delete_data_source(
    connection_id: str,
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db: Session = Depends(get_db)
):
    """
    åˆ é™¤æ•°æ®æºè¿žæŽ¥ï¼ˆè½¯åˆ é™¤ï¼‰

    ðŸ†• SQLite æ–‡ä»¶è‡ªåŠ¨æ¸…ç†:
    - å¦‚æžœæ•°æ®æºæ˜¯ä»Ž Excel è½¬æ¢çš„ SQLite æ•°æ®åº“
    - åˆ é™¤æ•°æ®æºæ—¶ä¼šåŒæ—¶åˆ é™¤å¯¹åº”çš„ SQLite æ–‡ä»¶
    - é‡Šæ”¾ç£ç›˜ç©ºé—´
    """
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    try:
        # ðŸ†• åœ¨åˆ é™¤å‰å…ˆèŽ·å–æ•°æ®æºä¿¡æ¯ï¼Œç”¨äºŽæ¸…ç† SQLite æ–‡ä»¶
        connection = await data_source_service.get_data_source_by_id(
            data_source_id=connection_id,
            tenant_id=tenant_id,
            db=db
        )

        sqlite_file_deleted = False
        if connection:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Ž Excel è½¬æ¢çš„ SQLite æ•°æ®æº
            # ç‰¹å¾ï¼šdb_type ä¸º sqliteï¼Œè¿žæŽ¥å­—ç¬¦ä¸²åŒ…å«æˆ‘ä»¬çš„å­˜å‚¨è·¯å¾„
            if connection.db_type == "sqlite":
                from pathlib import Path
                from src.app.services.excel_to_sqlite_service import get_excel_to_sqlite_service

                try:
                    # èŽ·å–è§£å¯†åŽçš„è¿žæŽ¥å­—ç¬¦ä¸²
                    decrypted_connection = connection.connection_string

                    # æå– SQLite æ–‡ä»¶è·¯å¾„
                    # æ ¼å¼: sqlite:///path/to/database.db
                    if "sqlite:///" in decrypted_connection:
                        sqlite_file_path = decrypted_connection.replace("sqlite:///", "")

                        # éªŒè¯æ–‡ä»¶æ˜¯å¦åœ¨æˆ‘ä»¬çš„å­˜å‚¨ç›®å½•ä¸­ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
                        excel_service = get_excel_to_sqlite_service()
                        storage_path_str = str(excel_service.SQLITE_STORAGE_PATH.absolute())

                        if sqlite_file_path.startswith(storage_path_str):
                            # å®‰å…¨åˆ é™¤ SQLite æ–‡ä»¶
                            sqlite_file = Path(sqlite_file_path)
                            if sqlite_file.exists():
                                sqlite_file.unlink()
                                sqlite_file_deleted = True
                                logger.info(f"ðŸ—‘ï¸ Deleted SQLite database file: {sqlite_file_path}")
                            else:
                                logger.warning(f"SQLite file not found (may have been already deleted): {sqlite_file_path}")
                        else:
                            logger.warning(f"SQLite file path outside storage directory, skipping deletion: {sqlite_file_path}")

                except Exception as e:
                    # SQLite æ–‡ä»¶åˆ é™¤å¤±è´¥ä¸åº”é˜»æ­¢æ•°æ®æºåˆ é™¤
                    logger.warning(f"Failed to delete SQLite file for data source {connection_id}: {e}")
                    logger.warning(f"Proceeding with data source deletion anyway...")

        # æ‰§è¡Œæ•°æ®æºè½¯åˆ é™¤
        success = await data_source_service.delete_data_source(
            data_source_id=connection_id,
            tenant_id=tenant_id,
            db=db
        )

        if success:
            logger.info(f"Deleted data source {connection_id}")
            return {
                "message": f"Data source connection {connection_id} has been deactivated",
                "connection_id": connection_id,
                "status": "deactivated",
                "sqlite_file_deleted": sqlite_file_deleted
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Data source connection not found"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete data source {connection_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete data source: {str(e)}"
        )


# æ³¨æ„ï¼š/test å¿…é¡»åœ¨ /{connection_id}/test ä¹‹å‰å®šä¹‰
@router.post("/test", summary="æµ‹è¯•è¿žæŽ¥å­—ç¬¦ä¸²")
async def test_connection_string(request: ConnectionTestRequest):
    """
    æµ‹è¯•è¿žæŽ¥å­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    """
    try:
        test_result = await connection_test_service.test_connection(
            connection_string=request.connection_string,
            db_type=request.db_type
        )

        logger.info(f"Tested connection string for {request.db_type}: {test_result.success}")
        return test_result.to_dict()

    except Exception as e:
        logger.error(f"Failed to test connection string: {e}")
        return {
            "success": False,
            "message": f"Connection test failed: {str(e)}",
            "error_code": "TEST_EXECUTION_ERROR",
            "timestamp": datetime.now().isoformat()
        }


@router.post("/{connection_id}/test", summary="æµ‹è¯•æ•°æ®æºè¿žæŽ¥")
async def test_data_source_connection(
    connection_id: str,
    tenant_id: str = None,  # å®žé™…åº”è¯¥ä»Žè®¤è¯ä¸­é—´ä»¶èŽ·å–
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•çŽ°æœ‰æ•°æ®æºè¿žæŽ¥æ˜¯å¦æœ‰æ•ˆ
    è‡ªåŠ¨æ›´æ–°è¿žæŽ¥çŠ¶æ€å’Œæµ‹è¯•ç»“æžœ
    """
    if not tenant_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="tenant_id is required"
        )

    try:
        # èŽ·å–æ•°æ®æºè¿žæŽ¥
        connection = await data_source_service.get_data_source_by_id(
            data_source_id=connection_id,
            tenant_id=tenant_id,
            db=db
        )

        if not connection:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Data source connection not found"
            )

        # å¯¹äºŽæ–‡ä»¶ç±»åž‹çš„æ•°æ®æºï¼ˆxlsx, xls, csvï¼‰ï¼Œä¸éœ€è¦æµ‹è¯•æ•°æ®åº“è¿žæŽ¥
        # åªéœ€è¦æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_types = ['xlsx', 'xls', 'csv']
        if connection.db_type in file_types:
            # æ–‡ä»¶ç±»åž‹çš„æ•°æ®æºï¼Œç›´æŽ¥æ ‡è®°ä¸ºæˆåŠŸï¼ˆæ–‡ä»¶å·²ä¸Šä¼ ï¼‰
            test_result_dict = {
                "success": True,
                "message": f"æ–‡ä»¶æ•°æ®æºå·²å°±ç»ª ({connection.db_type.upper()})",
                "error_code": None,
                "response_time_ms": 0,
                "details": {
                    "file_type": connection.db_type,
                    "database_name": connection.database_name
                },
                "timestamp": datetime.now().isoformat()
            }
            connection.update_test_result(test_result_dict)
            db.commit()
            logger.info(f"File data source {connection_id} marked as ready")
            return test_result_dict

        # å°è¯•èŽ·å–è§£å¯†åŽçš„è¿žæŽ¥å­—ç¬¦ä¸²
        try:
            decrypted_connection_string = connection.connection_string
        except RuntimeError as decrypt_error:
            logger.error(f"Failed to decrypt connection string for data source {connection_id}: {decrypt_error}")
            # è¿”å›žè§£å¯†å¤±è´¥çš„æµ‹è¯•ç»“æžœ
            test_result_dict = {
                "success": False,
                "message": "Failed to decrypt connection string",
                "error_code": "DECRYPTION_ERROR",
                "response_time_ms": 0,
                "details": {
                    "error": "åŠ å¯†å¯†é’¥å¯èƒ½å·²æ›´æ”¹ï¼Œæ— æ³•è§£å¯†è¿žæŽ¥å­—ç¬¦ä¸²ã€‚è¯·åˆ é™¤æ­¤æ•°æ®æºå¹¶é‡æ–°æ·»åŠ ã€‚"
                },
                "timestamp": datetime.now().isoformat()
            }
            connection.update_test_result(test_result_dict)
            db.commit()
            return test_result_dict

        # æµ‹è¯•è¿žæŽ¥å­—ç¬¦ä¸²
        test_result = await connection_test_service.test_connection(
            connection_string=decrypted_connection_string,
            db_type=connection.db_type
        )

        # æ›´æ–°è¿žæŽ¥çš„æµ‹è¯•ç»“æžœå’ŒçŠ¶æ€
        connection.update_test_result(test_result.to_dict())
        db.commit()

        logger.info(f"Tested data source {connection_id}: {test_result.success}")
        return test_result.to_dict()

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to test data source {connection_id}: {e}")
        return {
            "success": False,
            "message": f"Connection test failed: {str(e)}",
            "error_code": "TEST_EXECUTION_ERROR",
            "timestamp": datetime.now().isoformat()
        }


# ============================================================================
# è¾…åŠ©å‡½æ•° - è¢«ä¸Šé¢çš„ç«¯ç‚¹ä½¿ç”¨
# ============================================================================

async def _validate_overview_access(tenant_id: str, user_id: str, db: Session):
    """
    éªŒè¯æ¦‚è§ˆè®¿é—®æƒé™
    """
    # éªŒè¯ç§Ÿæˆ·æ˜¯å¦å­˜åœ¨ä¸”æ´»è·ƒ
    tenant = db.query(Tenant).filter(
        Tenant.id == tenant_id,
        Tenant.status == TenantStatus.ACTIVE
    ).first()

    if not tenant:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid or inactive tenant"
        )

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šç”¨æˆ·æƒé™éªŒè¯
    # ä¾‹å¦‚ï¼šæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æŸ¥çœ‹æ¦‚è§ˆçš„æƒé™


async def _get_secure_database_stats(tenant_id: str, db: Session) -> Dict[str, int]:
    """
    å®‰å…¨åœ°èŽ·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼Œå¼ºåŒ–ç§Ÿæˆ·éš”ç¦»
    """
    try:
        # ä½¿ç”¨å¼ºåŒ–çš„ç§Ÿæˆ·éš”ç¦»æŸ¥è¯¢
        db_connections = db.query(DataSourceConnection).filter(
            DataSourceConnection.tenant_id == tenant_id,
            DataSourceConnection.status != DataSourceConnectionStatus.INACTIVE
        ).with_entities(
            DataSourceConnection.id,
            DataSourceConnection.status,
            DataSourceConnection.tenant_id  # é¢å¤–çš„ç§Ÿæˆ·IDç”¨äºŽéªŒè¯
        ).all()

        # åŒé‡éªŒè¯ï¼šç¡®ä¿æ‰€æœ‰ç»“æžœéƒ½å±žäºŽè¯¥ç§Ÿæˆ·
        verified_connections = []
        for conn in db_connections:
            if conn.tenant_id != tenant_id:
                logger.error(f"Security breach: database {conn.id} from tenant {conn.tenant_id} "
                           f"included in stats for tenant {tenant_id}")
                continue
            verified_connections.append(conn)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        db_total = len(verified_connections)
        db_active = len([conn for conn in verified_connections
                        if conn.status == DataSourceConnectionStatus.ACTIVE])
        db_error = len([conn for conn in verified_connections
                       if conn.status == DataSourceConnectionStatus.ERROR])

        return {
            "total": db_total,
            "active": db_active,
            "error": db_error
        }

    except Exception as e:
        logger.error(f"Failed to get database stats for tenant {tenant_id}: {e}")
        # è¿”å›žå®‰å…¨çš„é»˜è®¤å€¼
        return {
            "total": 0,
            "active": 0,
            "error": 0
        }


async def _get_secure_document_stats(tenant_id: str, db: Session) -> Dict[str, int]:
    """
    å®‰å…¨åœ°èŽ·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # TODO: å®žçŽ°æ–‡æ¡£ç»Ÿè®¡æŸ¥è¯¢ï¼Œéœ€è¦Documentæ¨¡åž‹
        # ç¡®ä¿æŸ¥è¯¢åŒ…å«å¼ºåŒ–çš„ç§Ÿæˆ·éš”ç¦»
        return {
            "total": 0,
            "ready": 0,
            "processing": 0,
            "error": 0
        }

    except Exception as e:
        logger.error(f"Failed to get document stats for tenant {tenant_id}: {e}")
        return {
            "total": 0,
            "ready": 0,
            "processing": 0,
            "error": 0
        }


async def _get_secure_storage_stats(tenant_id: str, db: Session) -> Dict[str, any]:
    """
    å®‰å…¨åœ°èŽ·å–å­˜å‚¨é…é¢å’Œä½¿ç”¨æƒ…å†µç»Ÿè®¡
    """
    try:
        # èŽ·å–ç§Ÿæˆ·çš„å­˜å‚¨é…é¢é…ç½®
        tenant = db.query(Tenant).filter(
            Tenant.id == tenant_id,
            Tenant.status == TenantStatus.ACTIVE
        ).with_entities(
            Tenant.storage_quota_mb
        ).first()

        if not tenant:
            # ç§Ÿæˆ·ä¸å­˜åœ¨ï¼Œè¿”å›žå®‰å…¨çš„é»˜è®¤å€¼
            return {
                "used_mb": 0,
                "quota_mb": 1024,  # é»˜è®¤é…é¢
                "usage_percentage": 0.0,
                "quota_exceeded": False
            }

        storage_quota_mb = tenant.storage_quota_mb or 1024  # é»˜è®¤1GB

        # TODO: å®žçŽ°å®žé™…å­˜å‚¨ä½¿ç”¨é‡è®¡ç®—
        # è¿™é‡Œåº”è¯¥ä»ŽMinIOèŽ·å–å®žé™…çš„å­˜å‚¨ä½¿ç”¨é‡
        storage_used_mb = 0  # å®žé™…å®žçŽ°éœ€è¦æŸ¥è¯¢MinIO

        # å®‰å…¨åœ°è®¡ç®—ä½¿ç”¨ç™¾åˆ†æ¯”ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
        usage_percentage = 0.0
        if storage_quota_mb > 0:
            usage_percentage = min(100.0, round((storage_used_mb / storage_quota_mb) * 100, 2))

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é…é¢
        quota_exceeded = storage_used_mb > storage_quota_mb

        # å¦‚æžœé…é¢ä½¿ç”¨çŽ‡è¿‡é«˜ï¼Œè®°å½•è­¦å‘Š
        if usage_percentage > 90:
            logger.warning(f"High storage usage for tenant {tenant_id}: {usage_percentage}%")

        return {
            "used_mb": storage_used_mb,
            "quota_mb": storage_quota_mb,
            "usage_percentage": usage_percentage,
            "quota_exceeded": quota_exceeded
        }

    except Exception as e:
        logger.error(f"Failed to get storage stats for tenant {tenant_id}: {e}")
        # è¿”å›žå®‰å…¨çš„é»˜è®¤å€¼
        return {
            "used_mb": 0,
            "quota_mb": 1024,
            "usage_percentage": 0.0,
            "quota_exceeded": False
        }


async def _get_secure_recent_activity(tenant_id: str, db: Session) -> List[Dict[str, any]]:
    """
    å®‰å…¨åœ°èŽ·å–æœ€è¿‘æ´»åŠ¨è®°å½•ï¼ŒåŒ…å«å¼ºåŒ–çš„ç§Ÿæˆ·éš”ç¦»
    """
    try:
        # èŽ·å–æœ€è¿‘çš„æ•°æ®åº“è¿žæŽ¥æ´»åŠ¨
        recent_connections = db.query(DataSourceConnection).filter(
            DataSourceConnection.tenant_id == tenant_id
        ).with_entities(
            DataSourceConnection.id,
            DataSourceConnection.name,
            DataSourceConnection.status,
            DataSourceConnection.created_at,
            DataSourceConnection.updated_at,
            DataSourceConnection.tenant_id  # ç”¨äºŽéªŒè¯
        ).order_by(DataSourceConnection.updated_at.desc()).limit(5).all()

        recent_activity = []
        for conn in recent_connections:
            # åŒé‡éªŒè¯ï¼šç¡®ä¿æ´»åŠ¨è®°å½•å±žäºŽè¯¥ç§Ÿæˆ·
            if conn.tenant_id != tenant_id:
                logger.error(f"Security breach: activity for database {conn.id} from tenant {conn.tenant_id} "
                           f"included in activity for tenant {tenant_id}")
                continue

            # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯ï¼Œåªè¿”å›žå¿…è¦çš„æ´»åŠ¨æ•°æ®
            activity = {
                "id": conn.id,
                "type": "database",
                "action": "updated" if conn.updated_at != conn.created_at else "created",
                "item_name": conn.name,
                "timestamp": conn.updated_at.isoformat() if conn.updated_at else conn.created_at.isoformat(),
                "status": "success" if conn.status == DataSourceConnectionStatus.ACTIVE else "error"
            }
            recent_activity.append(activity)

        return recent_activity

    except Exception as e:
        logger.error(f"Failed to get recent activity for tenant {tenant_id}: {e}")
        return []


async def _validate_search_parameters(
    q: str,
    type: str,
    status: Optional[str],
    date_from: Optional[str],
    date_to: Optional[str],
    page: int,
    limit: int
):
    """
    éªŒè¯æœç´¢å‚æ•°çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§
    """
    # éªŒè¯æœç´¢æŸ¥è¯¢
    if q:
        if len(q) > 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Search query too long (max 100 characters)"
            )
        # æ£€æŸ¥æ½œåœ¨çš„æ³¨å…¥æ”»å‡»æ¨¡å¼
        dangerous_patterns = ["'", '"', ';', '--', '/*', '*/', 'xp_', 'sp_']
        for pattern in dangerous_patterns:
            if pattern in q.lower():
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid search query pattern"
                )

    # éªŒè¯ç±»åž‹å‚æ•°
    if type not in ["all", "database", "document"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid type parameter"
        )

    # éªŒè¯åˆ†é¡µå‚æ•°
    if page < 1 or page > 1000:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid page parameter"
        )

    if limit < 1 or limit > 100:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid limit parameter (max 100)"
        )


async def _validate_tenant_search_access(tenant_id: str, user_id: str, db: Session):
    """
    éªŒè¯ç§Ÿæˆ·æœç´¢è®¿é—®æƒé™
    """
    # éªŒè¯ç§Ÿæˆ·æ˜¯å¦å­˜åœ¨ä¸”æ´»è·ƒ
    tenant = db.query(Tenant).filter(
        Tenant.id == tenant_id,
        Tenant.status == TenantStatus.ACTIVE
    ).first()

    if not tenant:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid or inactive tenant"
        )

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šç”¨æˆ·æƒé™éªŒè¯
    # ä¾‹å¦‚ï¼šæ£€æŸ¥ç”¨æˆ·æ˜¯å¦å±žäºŽè¯¥ç§Ÿæˆ·


def _sanitize_search_query(query: str) -> str:
    """
    æ¸…ç†æœç´¢æŸ¥è¯¢ï¼Œé˜²æ­¢æ³¨å…¥æ”»å‡»
    """
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    import re
    sanitized = re.sub(r"[\'\"\\\;\-\-\/\*\*]", "", query)
    return sanitized.strip()


def _validate_and_parse_statuses(status: str) -> List[str]:
    """
    éªŒè¯å’Œè§£æžçŠ¶æ€å‚æ•°
    """
    valid_statuses = ["ACTIVE", "INACTIVE", "ERROR", "TESTING"]
    status_list = status.split(',') if ',' in status else [status]

    validated_statuses = []
    for s in status_list:
        s = s.strip().upper()
        if s in valid_statuses:
            validated_statuses.append(s)

    return validated_statuses


def _validate_and_parse_date_range(date_from: Optional[str], date_to: Optional[str]) -> Dict[str, Optional[datetime]]:
    """
    éªŒè¯å’Œè§£æžæ—¥æœŸèŒƒå›´
    """
    from datetime import datetime

    result = {"from": None, "to": None}

    try:
        if date_from:
            result["from"] = datetime.fromisoformat(date_from.replace('Z', '+00:00'))
        if date_to:
            result["to"] = datetime.fromisoformat(date_to.replace('Z', '+00:00'))

        # éªŒè¯æ—¥æœŸèŒƒå›´åˆç†æ€§
        if result["from"] and result["to"] and result["from"] > result["to"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid date range: start date cannot be after end date"
            )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use ISO format."
        )

    return result


async def _validate_bulk_database_delete_permission(
    tenant_id: str,
    user_id: str,
    item_ids: List[str],
    db: Session
):
    """
    éªŒè¯æ‰¹é‡åˆ é™¤æ•°æ®åº“è¿žæŽ¥çš„æƒé™

    Args:
        tenant_id: ç§Ÿæˆ·ID
        user_id: ç”¨æˆ·ID
        item_ids: è¦åˆ é™¤çš„æ•°æ®æºIDåˆ—è¡¨
        db: æ•°æ®åº“ä¼šè¯
    """
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡äº†è¯¥ç§Ÿæˆ·çš„æ•°æ®æºåˆ é™¤é™åˆ¶
    active_connections = db.query(DataSourceConnection).filter(
        DataSourceConnection.tenant_id == tenant_id,
        DataSourceConnection.status != DataSourceConnectionStatus.INACTIVE
    ).count()

    # å¦‚æžœåˆ é™¤åŽä¼šå¯¼è‡´æ•°æ®æºæ•°é‡è¿‡å°‘ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šæƒé™
    if active_connections - len(item_ids) < 1:
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç‰¹æ®Šæƒé™æ£€æŸ¥
        logger.warning(f"User {user_id} attempting to delete last {len(item_ids)} "
                       f"data sources for tenant {tenant_id}")

    # æ£€æŸ¥æœ€è¿‘åˆ é™¤é¢‘çŽ‡ï¼Œé˜²æ­¢æ¶æ„æ‰¹é‡åˆ é™¤
    recent_deletions = db.query(DataSourceConnection).filter(
        DataSourceConnection.tenant_id == tenant_id,
        DataSourceConnection.status == DataSourceConnectionStatus.INACTIVE,
        DataSourceConnection.updated_at >= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    ).count()

    if recent_deletions > 10:  # åŒä¸€å¤©å†…åˆ é™¤è¶…è¿‡10ä¸ªéœ€è¦é¢å¤–éªŒè¯
        logger.warning(f"High frequency deletion detected for tenant {tenant_id}: "
                       f"{recent_deletions} deletions today")


async def _can_delete_data_source(connection: DataSourceConnection, db: Session) -> bool:
    """
    æ£€æŸ¥æ•°æ®æºæ˜¯å¦å¯ä»¥è¢«åˆ é™¤

    Args:
        connection: æ•°æ®æºè¿žæŽ¥å¯¹è±¡
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        bool: æ˜¯å¦å¯ä»¥åˆ é™¤
    """
    # æ£€æŸ¥æ•°æ®æºçŠ¶æ€
    if connection.status == DataSourceConnectionStatus.ERROR:
        return True  # é”™è¯¯çŠ¶æ€çš„æ•°æ®æºå¯ä»¥åˆ é™¤

    if connection.status == DataSourceConnectionStatus.TESTING:
        return False  # æ­£åœ¨æµ‹è¯•çš„æ•°æ®æºä¸èƒ½åˆ é™¤

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ£€æŸ¥ï¼Œæ¯”å¦‚ï¼š
    # - æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„æŸ¥è¯¢
    # - æ£€æŸ¥æ˜¯å¦æœ‰å…³è”çš„æ–‡æ¡£
    # - æ£€æŸ¥æ˜¯å¦æœ‰å®šæ—¶ä»»åŠ¡ä¾èµ–

    return True