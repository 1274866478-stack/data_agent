"""
# [QUERY] æ™ºèƒ½æŸ¥è¯¢APIç«¯ç‚¹

## [HEADER]
**æ–‡ä»¶å**: query.py
**èŒè´£**: å®ç°è‡ªç„¶è¯­è¨€æŸ¥è¯¢APIï¼Œé›†æˆLangGraph SQL Agentå’ŒLLMæœåŠ¡ï¼Œæ”¯æŒSQL/æ–‡æ¡£/æ··åˆæŸ¥è¯¢ï¼Œæä¾›æŸ¥è¯¢å†å²ã€çŠ¶æ€è·Ÿè¸ªå’Œç¼“å­˜ç®¡ç†ï¼Œç¡®ä¿ç§Ÿæˆ·éš”ç¦»å’ŒæŸ¥è¯¢å®‰å…¨
**ä½œè€…**: Data Agent Team
**ç‰ˆæœ¬**: 1.0.0
**å˜æ›´è®°å½•**:
- v1.0.0 (2026-01-01): åˆå§‹ç‰ˆæœ¬ - å®ç°Story 3.1è§„èŒƒçš„æ™ºèƒ½æŸ¥è¯¢API

## [INPUT]
- **tenant_id: str** - ç§Ÿæˆ·IDï¼ˆä»JWT tokenä¸­æå–ï¼‰
- **user_id: str** - ç”¨æˆ·IDï¼ˆä»JWT tokenä¸­æå–ï¼Œç”¨äºæ“ä½œå®¡è®¡ï¼‰
- **query_id: str** - æŸ¥è¯¢IDï¼ˆUUIDæ ¼å¼ï¼Œè·¯å¾„å‚æ•°ï¼‰
- **query_hash: str** - æŸ¥è¯¢å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ç®¡ç†ï¼‰
- **request: QueryRequest** - æŸ¥è¯¢è¯·æ±‚æ¨¡å‹ï¼ˆPydanticæ¨¡å‹ï¼‰
  - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢é—®é¢˜
  - connection_id: æ•°æ®æºè¿æ¥IDï¼ˆå¯é€‰ï¼‰
  - data_source_ids: æ•°æ®æºIDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
  - document_ids: æ–‡æ¡£IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
  - options: æŸ¥è¯¢é€‰é¡¹ï¼ˆå¯é€‰ï¼‰
- **page: int** - åˆ†é¡µé¡µç ï¼ˆé»˜è®¤1ï¼‰
- **page_size: int** - åˆ†é¡µå¤§å°ï¼ˆé»˜è®¤10ï¼Œæœ€å¤§100ï¼‰
- **background_tasks: BackgroundTasks** - FastAPIåå°ä»»åŠ¡
- **db: Session** - æ•°æ®åº“ä¼šè¯ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥è·å–ï¼‰

## [OUTPUT]
- **query_response: QueryResponseV3** - æŸ¥è¯¢å“åº”V3æ ¼å¼
  - query_id: æŸ¥è¯¢ID
  - tenant_id: ç§Ÿæˆ·ID
  - original_query: åŸå§‹æŸ¥è¯¢
  - generated_sql: ç”Ÿæˆçš„SQLè¯­å¥ï¼ˆAgentæŸ¥è¯¢ï¼‰
  - results: æŸ¥è¯¢ç»“æœæ•°æ®
  - row_count: ç»“æœè¡Œæ•°
  - processing_time_ms: å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  - confidence_score: ç½®ä¿¡åº¦åˆ†æ•°
  - explanation: ç»“æœè§£é‡Š
  - processing_steps: å¤„ç†æ­¥éª¤åˆ—è¡¨
  - validation_result: éªŒè¯ç»“æœ
  - execution_result: æ‰§è¡Œç»“æœ
  - correction_attempts: ä¿®æ­£å°è¯•æ¬¡æ•°
- **query_status: QueryStatusResponse** - æŸ¥è¯¢çŠ¶æ€å“åº”
  - query_id: æŸ¥è¯¢ID
  - status: æŸ¥è¯¢çŠ¶æ€ï¼ˆprocessing, success, errorï¼‰
  - created_at: åˆ›å»ºæ—¶é—´
  - updated_at: æ›´æ–°æ—¶é—´
  - response_time_ms: å“åº”æ—¶é—´
  - error_message: é”™è¯¯æ¶ˆæ¯
  - progress_percentage: è¿›åº¦ç™¾åˆ†æ¯”
- **query_history: QueryHistoryResponse** - æŸ¥è¯¢å†å²å“åº”
  - queries: æŸ¥è¯¢è®°å½•åˆ—è¡¨
  - total_count: æ€»è®°å½•æ•°
  - page: å½“å‰é¡µç 
  - page_size: åˆ†é¡µå¤§å°
- **cache_response: QueryCacheResponse** - ç¼“å­˜å“åº”
  - query_hash: æŸ¥è¯¢å“ˆå¸Œ
  - cache_cleared: ç¼“å­˜æ˜¯å¦æ¸…é™¤
  - message: æ“ä½œæ¶ˆæ¯
- **error_response: HTTPException** - é”™è¯¯å“åº”ï¼ˆ400, 404, 429, 500ï¼‰

## [LINK]
**ä¸Šæ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- [../../data/database.py](../../data/database.py) - get_db(), Session
- [../../data/models.py](../../data/models.py) - QueryStatus, QueryType
- [../../middleware/tenant_context.py](../../middleware/tenant_context.py) - get_current_tenant_from_request, get_current_tenant_id
- [../../services/query_context.py](../../services/query_context.py) - get_query_context, æŸ¥è¯¢ä¸Šä¸‹æ–‡ç®¡ç†
- [../../services/llm_service.py](../../services/llm_service.py) - llm_service, LLMæœåŠ¡è°ƒç”¨
- [../../services/agent_service.py](../../services/agent_service.py) - run_agent_query, convert_agent_response_to_query_response, is_agent_available
- [../../services/data_source_service.py](../../services/data_source_service.py) - DataSourceService, æ•°æ®æºæœåŠ¡
- [../../core/jwt_utils.py](../../core/jwt_utils.py) - get_current_user_from_token, JWTè§£æ

**ä¸‹æ¸¸ä¾èµ–** (å·²è¯»å–æºç ):
- æ— ï¼ˆAPIç«¯ç‚¹æ˜¯å¶å­æ¨¡å—ï¼‰

**è°ƒç”¨æ–¹**:
- å‰ç«¯èŠå¤©ç•Œé¢ - å‘é€è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- å‰ç«¯ä»ªè¡¨æ¿ - æ˜¾ç¤ºæŸ¥è¯¢å†å²
- LangGraph Agent - SQLæ™ºèƒ½ä»£ç†æŸ¥è¯¢
- å‰ç«¯æŸ¥è¯¢ç®¡ç† - æŸ¥è¯¢çŠ¶æ€è·Ÿè¸ª

## [POS]
**è·¯å¾„**: backend/src/app/api/v1/endpoints/query.py
**æ¨¡å—å±‚çº§**: Level 3 - APIç«¯ç‚¹å±‚
**ä¾èµ–æ·±åº¦**: ç›´æ¥ä¾èµ– data/*, services/*, middleware/*, core/*ï¼›è¢«å‰ç«¯æŸ¥è¯¢æ¨¡å—è°ƒç”¨
"""

import asyncio
import uuid
import time
import traceback
from typing import Dict, Any, Optional, Callable, List
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query as QueryParam, status
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session

from src.app.data.database import get_db
from src.app.data.models import QueryStatus, QueryType
from src.app.middleware.tenant_context import get_current_tenant_from_request, get_current_tenant_id
from src.app.services.query_context import get_query_context
from src.app.services.llm_service import llm_service
from src.app.services.agent_service import (
    run_agent_query,
    convert_agent_response_to_query_response,
    is_agent_available
)
from src.app.services.data_source_service import DataSourceService
from src.app.core.jwt_utils import get_current_user_from_token
from fastapi import Request
from src.app.schemas.query import (
    QueryRequest, QueryResponseV3, QueryStatusResponse,
    QueryCacheResponse, QueryHistoryResponse, ErrorResponse
)
from src.app.core.config import get_settings
import structlog

logger = structlog.get_logger(__name__)
settings = get_settings()
router = APIRouter()


async def get_current_user_info_from_request(
    request: Request,
    tenant=Depends(get_current_tenant_from_request)
) -> Dict[str, Any]:
    """
    ä»è¯·æ±‚ä¸­è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯

    Args:
        request: FastAPIè¯·æ±‚å¯¹è±¡
        tenant: å½“å‰ç§Ÿæˆ·å¯¹è±¡

    Returns:
        Dict[str, Any]: ç”¨æˆ·ä¿¡æ¯ï¼ŒåŒ…å«user_id

    Raises:
        HTTPException: è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥
    """
    try:
        # ä»Authorization headerä¸­æå–token
        authorization = request.headers.get("Authorization")
        if not authorization:
            # å¦‚æœæ²¡æœ‰JWT tokenï¼Œå›é€€åˆ°API keyæˆ–å…¬å…±è®¿é—®
            return {
                "user_id": tenant.id,  # ä½¿ç”¨tenant.idä½œä¸ºfallback
                "auth_type": "tenant_fallback",
                "tenant_id": tenant.id
            }

        # éªŒè¯JWT tokenå¹¶æå–ç”¨æˆ·ä¿¡æ¯
        user_info = await get_current_user_from_token(authorization)

        # ç¡®ä¿ç”¨æˆ·ä¿¡æ¯å’Œç§Ÿæˆ·ä¿¡æ¯åŒ¹é…
        if user_info.get("tenant_id") != tenant.id:
            logger.warning(
                "User tenant mismatch",
                user_tenant=user_info.get("tenant_id"),
                request_tenant=tenant.id
            )
            # ä½¿ç”¨ç§Ÿæˆ·IDç¡®ä¿æ•°æ®éš”ç¦»
            user_info["tenant_id"] = tenant.id

        return {
            "user_id": user_info.get("user_id", tenant.id),
            "auth_type": "jwt",
            "tenant_id": tenant.id,
            "email": user_info.get("email"),
            "is_verified": user_info.get("is_verified", False),
            "raw_info": user_info
        }

    except Exception as e:
        logger.error(f"Failed to extract user info: {e}")
        # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œä½¿ç”¨tenant.idä½œä¸ºfallbackç¡®ä¿ç³»ç»Ÿæ­£å¸¸è¿è¡Œ
        return {
            "user_id": tenant.id,
            "auth_type": "error_fallback",
            "tenant_id": tenant.id,
            "error": str(e)
        }


class QueryService:
    """æŸ¥è¯¢å¤„ç†æœåŠ¡ï¼ŒåŒ…å«AIæœåŠ¡é‡è¯•æœºåˆ¶"""

    def __init__(self, query_context):
        self.query_context = query_context

    async def retry_with_exponential_backoff(
        self,
        func: Callable,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        backoff_factor: float = 2.0,
        retry_on: tuple = (Exception,)
    ) -> Any:
        """
        æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶

        Args:
            func: è¦é‡è¯•çš„å¼‚æ­¥å‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            backoff_factor: é€€é¿å› å­
            retry_on: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹

        Returns:
            Any: å‡½æ•°æ‰§è¡Œç»“æœ

        Raises:
            Exception: é‡è¯•æ¬¡æ•°è€—å°½åçš„æœ€åä¸€ä¸ªå¼‚å¸¸
        """
        last_exception = None

        for attempt in range(max_retries + 1):  # +1 å› ä¸ºç¬¬ä¸€æ¬¡ä¸ç®—é‡è¯•
            try:
                if attempt > 0:
                    delay = min(base_delay * (backoff_factor ** (attempt - 1)), max_delay)
                    logger.info(
                        f"AI service retry attempt {attempt}/{max_retries} after {delay:.2f}s delay",
                        tenant_id=self.query_context.tenant_id
                    )
                    await asyncio.sleep(delay)

                result = await func()

                if attempt > 0:
                    logger.info(
                        f"AI service succeeded on attempt {attempt + 1}",
                        tenant_id=self.query_context.tenant_id
                    )

                return result

            except retry_on as e:
                last_exception = e

                if attempt < max_retries:
                    logger.warning(
                        f"AI service attempt {attempt + 1} failed: {e}, will retry",
                        tenant_id=self.query_context.tenant_id,
                        attempt=attempt + 1,
                        max_retries=max_retries + 1
                    )
                else:
                    logger.error(
                        f"AI service failed after {max_retries + 1} attempts: {e}",
                        tenant_id=self.query_context.tenant_id
                    )

            except Exception as e:
                # å¯¹äºä¸éœ€è¦é‡è¯•çš„å¼‚å¸¸ï¼Œç›´æ¥æŠ›å‡º
                logger.error(
                    f"AI service encountered non-retryable error: {e}",
                    tenant_id=self.query_context.tenant_id
                )
                raise

        # é‡è¯•æ¬¡æ•°è€—å°½ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
        raise last_exception

    async def call_llm_with_retry(self, messages: list, **kwargs) -> Any:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„LLMæœåŠ¡è°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–LLMå‚æ•°

        Returns:
            Any: LLMå“åº”ç»“æœ
        """
        async def llm_call():
            return await llm_service.chat_completion(
                messages=messages,
                tenant_id=self.query_context.tenant_id,
                **kwargs
            )

        # å®šä¹‰éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
        retryable_exceptions = (
            ConnectionError,
            TimeoutError,
            OSError,
            # å¯ä»¥æ ¹æ®å®é™…LLMæœåŠ¡çš„å¼‚å¸¸ç±»å‹è¿›è¡Œè°ƒæ•´
        )

        return await self.retry_with_exponential_backoff(
            func=llm_call,
            max_retries=3,
            base_delay=1.0,
            max_delay=30.0,
            backoff_factor=2.0,
            retry_on=retryable_exceptions
        )

    async def analyze_query_type(self, question: str, context: Optional[Dict[str, Any]] = None) -> QueryType:
        """
        åˆ†ææŸ¥è¯¢ç±»å‹
        Storyè¦æ±‚ï¼šè‡ªåŠ¨åˆ†ææŸ¥è¯¢ç±»å‹ï¼ˆSQL/æ–‡æ¡£/æ··åˆï¼‰

        Args:
            question: æŸ¥è¯¢é—®é¢˜
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡

        Returns:
            QueryType: æŸ¥è¯¢ç±»å‹
        """
        question_lower = question.lower()

        # SQLæŸ¥è¯¢å…³é”®è¯
        sql_keywords = ['é”€å”®', 'æ”¶å…¥', 'æ•°é‡', 'ç»Ÿè®¡', 'æ±‡æ€»', 'è®¡ç®—', 'å¯¹æ¯”', 'åˆ†æ', 'æ•°æ®']
        # æ–‡æ¡£æŸ¥è¯¢å…³é”®è¯
        doc_keywords = ['ä»€ä¹ˆæ˜¯', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'è§£é‡Š', 'è¯´æ˜', 'ä»‹ç»', 'æ–‡æ¡£', 'æŠ¥å‘Š']

        has_sql_keywords = any(keyword in question_lower for keyword in sql_keywords)
        has_doc_keywords = any(keyword in question_lower for keyword in doc_keywords)

        # æ£€æŸ¥ä¸Šä¸‹æ–‡
        has_data_sources = context and context.get('data_source_ids')
        has_documents = context and context.get('document_ids')

        if has_sql_keywords or has_data_sources:
            if has_doc_keywords or has_documents:
                return QueryType.MIXED
            return QueryType.SQL
        elif has_doc_keywords or has_documents:
            return QueryType.DOCUMENT
        else:
            return QueryType.MIXED  # é»˜è®¤ä¸ºæ··åˆæŸ¥è¯¢

    async def process_query(
        self,
        query_id: str,
        question: str,
        context: Optional[Dict[str, Any]] = None,
        options: Optional[Dict[str, Any]] = None,
        selected_data_sources: Optional[List[Any]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†æŸ¥è¯¢è¯·æ±‚
        Storyè¦æ±‚ï¼šå®Œæ•´çš„æŸ¥è¯¢å¤„ç†æµç¨‹

        Args:
            query_id: æŸ¥è¯¢ID
            question: æŸ¥è¯¢é—®é¢˜
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            options: æŸ¥è¯¢é€‰é¡¹

        Returns:
            Dict[str, Any]: æŸ¥è¯¢ç»“æœ
        """
        start_time = datetime.utcnow()

        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.query_context.update_query_status(
                query_id=query_id,
                status=QueryStatus.PROCESSING
            )

            # åˆ†ææŸ¥è¯¢ç±»å‹
            query_type = await self.analyze_query_type(question, context)

            # æ•°æ®æºä¸æ–‡æ¡£
            if selected_data_sources is not None:
                data_sources = selected_data_sources
            else:
                data_sources = self.query_context.get_tenant_data_sources()
            documents = self.query_context.get_tenant_documents()

            # æ„é€ æ•°æ®æºæè¿°ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å½“å‰å¯ç”¨çš„æ•°æ®æº
            ds_lines = []
            for ds in data_sources:
                ds_lines.append(
                    f"- åç§°: {ds.name} | ç±»å‹: {ds.db_type} | æ•°æ®åº“: {ds.database_name or 'æœªæŒ‡å®š'}"
                )
            data_sources_summary = "\n".join(ds_lines) if ds_lines else "æ— å¯ç”¨æ•°æ®æº"

            # æ„å»ºLLMè¯·æ±‚
            messages = [
                {
                    "role": "system",
                    "content": f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œä½¿ç”¨æä¾›çš„æ•°æ®æºå’Œæ–‡æ¡£ä¿¡æ¯æ¥å›ç­”ã€‚

æŸ¥è¯¢ç±»å‹: {query_type.value}

å¯ç”¨æ•°æ®æºæ•°é‡: {len(data_sources)}
å¯ç”¨æ•°æ®æºè¯¦æƒ…:
{data_sources_summary}
å¯ç”¨æ–‡æ¡£æ•°é‡: {len(documents)}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
1. æä¾›å‡†ç¡®çš„ç­”æ¡ˆ (Accurate Answer)
2. å¼•ç”¨ç›¸å…³çš„æ•°æ®æºå’Œæ–‡æ¡£ (Data Sources)
3. æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ (Reasoning)
4. ä½¿ç”¨Markdownæ ¼å¼åŒ–ç­”æ¡ˆ (Markdown Formatting)
   - âš ï¸ é‡è¦ï¼šåœ¨ç¬¬ 4 éƒ¨åˆ†ä¸­ï¼Œä¸è¦è¾“å‡ºå¤§å‹ ASCII è¡¨æ ¼ã€‚åº”è¯¥ä¾èµ–ç¬¬ 5 éƒ¨åˆ†æ¥å¯è§†åŒ–æ•°æ®ã€‚
   - âš ï¸ ä¸è¦ä½¿ç”¨ Markdown è¡¨æ ¼ï¼ˆå¦‚ | åˆ—1 | åˆ—2 |ï¼‰æ¥å±•ç¤ºç»Ÿè®¡æ•°æ®ï¼Œè¿™äº›æ•°æ®åº”è¯¥é€šè¿‡ç¬¬ 5 éƒ¨åˆ†çš„å›¾è¡¨æ¥å¯è§†åŒ–ã€‚
5. å¯è§†åŒ– (Visualization - Required if data is available)
   - å¦‚æœç»“æœåŒ…å«ç»Ÿè®¡æ•°æ®ï¼ˆæ—¶é—´åºåˆ—ã€å¯¹æ¯”æ•°æ®ã€è¶‹åŠ¿åˆ†æç­‰ï¼‰ï¼Œä½ å¿…é¡»åœ¨æ­¤å¤„ç”Ÿæˆ ECharts JSON é…ç½®ã€‚
   - ä½¿ç”¨æ ¼å¼ï¼š[CHART_START] { ... } [CHART_END]

âš ï¸ é‡è¦ï¼šå›¾è¡¨é…ç½®æ˜¯å›å¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸è¦å› ä¸ºéµå¾ªä¸Šè¿°æ ¼å¼è€Œçœç•¥å›¾è¡¨é…ç½®ã€‚å½“éœ€è¦å¯è§†åŒ–æ—¶ï¼Œå›¾è¡¨é…ç½®å¿…é¡»åŒ…å«åœ¨å›å¤ä¸­ã€‚ä¸è¦ä½¿ç”¨ Markdown è¡¨æ ¼ä»£æ›¿å›¾è¡¨ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"é—®é¢˜: {question}\n\nä¸Šä¸‹æ–‡: {context or {}}\n\nè¯·åŸºäºå¯ç”¨æ•°æ®å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
                }
            ]

            # è°ƒç”¨LLMæœåŠ¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            try:
                llm_response = await self.call_llm_with_retry(
                    messages=messages,
                    temperature=0.3,
                    max_tokens=1000
                )

                # æ—§é€»è¾‘ä¾èµ– success å­—æ®µï¼Œè¿™é‡Œå…¼å®¹æ—  success çš„è¿”å›ï¼Œè®¤ä¸ºè°ƒç”¨æˆåŠŸ
                if hasattr(llm_response, "success") and not getattr(llm_response, "success"):
                    raise Exception(f"LLM service failed: {getattr(llm_response, 'error', 'unknown error')}")

            except Exception as e:
                # è®°å½•AIæœåŠ¡å¤±è´¥å¹¶æ›´æ–°æŸ¥è¯¢çŠ¶æ€
                self.query_context.update_query_status(
                    query_id=query_id,
                    status=QueryStatus.ERROR,
                    error_message=f"AI service error: {str(e)}",
                    error_code="AI_SERVICE_ERROR",
                    response_time_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000)
                )
                raise

            # æ„å»ºå“åº”æ•°æ®
            response_data = {
                "answer": llm_response.content,
                "citations": [],  # TODO: å®ç°çœŸå®çš„æ–‡æ¡£å¼•ç”¨
                "data_sources": [],  # TODO: å®ç°çœŸå®çš„æ•°æ®æºå¼•ç”¨
                "explainability_log": self._generate_explainability_log(question, query_type, data_sources, documents),
                "response_time_ms": int((datetime.utcnow() - start_time).total_seconds() * 1000),
                "tokens_used": llm_response.usage.get('total_tokens') if llm_response.usage else 0,
                "query_type": query_type.value
            }

            # æ›´æ–°æŸ¥è¯¢çŠ¶æ€ä¸ºæˆåŠŸ
            self.query_context.update_query_status(
                query_id=query_id,
                status=QueryStatus.SUCCESS,
                response_summary=llm_response.content[:200] + "..." if len(llm_response.content) > 200 else llm_response.content,
                response_data=response_data,
                explainability_log=response_data["explainability_log"],
                response_time_ms=response_data["response_time_ms"],
                tokens_used=response_data["tokens_used"]
            )

            return response_data

        except Exception as e:
            # æ›´æ–°æŸ¥è¯¢çŠ¶æ€ä¸ºé”™è¯¯
            error_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
            self.query_context.update_query_status(
                query_id=query_id,
                status=QueryStatus.ERROR,
                response_time_ms=error_time_ms,
                error_message=str(e),
                error_code="QUERY_PROCESSING_ERROR"
            )
            raise

    def _generate_explainability_log(self, question: str, query_type: QueryType,
                                   data_sources: list, documents: list) -> str:
        """
        ç”ŸæˆXAIå¯è§£é‡Šæ€§æ—¥å¿—
        Storyè¦æ±‚ï¼šXAIæ¨ç†è·¯å¾„æ—¥å¿—

        Args:
            question: æŸ¥è¯¢é—®é¢˜
            query_type: æŸ¥è¯¢ç±»å‹
            data_sources: æ•°æ®æºåˆ—è¡¨
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            str: è§£é‡Šæ€§æ—¥å¿—
        """
        log_lines = [
            f"# æŸ¥è¯¢æ¨ç†è·¯å¾„",
            f"",
            f"## 1. æŸ¥è¯¢åˆ†æ",
            f"- åŸå§‹é—®é¢˜: {question}",
            f"- è¯†åˆ«çš„æŸ¥è¯¢ç±»å‹: {query_type.value}",
            f"",
            f"## 2. æ•°æ®æºè¯„ä¼°",
            f"- å¯ç”¨æ•°æ®æºæ•°é‡: {len(data_sources)}",
            f"- å¯ç”¨æ–‡æ¡£æ•°é‡: {len(documents)}",
        ]

        if data_sources:
            log_lines.extend([
                f"- æ•°æ®æºåˆ—è¡¨:",
            ])
            for ds in data_sources[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                log_lines.append(f"  * {ds.name} ({ds.connection_type})")

        if documents:
            log_lines.extend([
                f"- ç›¸å…³æ–‡æ¡£åˆ—è¡¨:",
            ])
            for doc in documents[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                log_lines.append(f"  * {doc.title}")

        log_lines.extend([
            f"",
            f"## 3. å¤„ç†ç­–ç•¥",
            f"- åŸºäºæŸ¥è¯¢ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥",
            f"- æ•´åˆå¤šæºæ•°æ®è¿›è¡Œç»¼åˆåˆ†æ",
            f"",
            f"## 4. æ¨ç†è¿‡ç¨‹",
            f"- ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç†è§£å’Œç­”æ¡ˆç”Ÿæˆ",
            f"- ç¡®ä¿ç­”æ¡ˆåŸºäºå¯ç”¨çš„æ•°æ®å’Œæ–‡æ¡£",
            f"- æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹å’Œå¼•ç”¨ä¿¡æ¯",
            f"",
            f"## 5. ç­”æ¡ˆç”Ÿæˆ",
            f"- ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ",
            f"- ç¡®ä¿ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§",
            f"",
            f"ç”Ÿæˆæ—¶é—´: {datetime.utcnow().isoformat()}"
        ])

        return "\n".join(log_lines)


# åˆ›å»ºæŸ¥è¯¢æœåŠ¡çš„ä¾èµ–æ³¨å…¥
async def get_query_service(
    tenant=Depends(get_current_tenant_from_request),
    user_info: Dict[str, Any] = Depends(get_current_user_info_from_request),
    db: Session = Depends(get_db)
) -> QueryService:
    """è·å–æŸ¥è¯¢æœåŠ¡å®ä¾‹"""
    user_id = user_info["user_id"]
    query_context = get_query_context(db, tenant.id, user_id)
    return QueryService(query_context)


@router.post("/query", response_model=None)
async def create_query(
    request: QueryRequest,
    background_tasks: BackgroundTasks,
    tenant=Depends(get_current_tenant_from_request),
    user_info: Dict[str, Any] = Depends(get_current_user_info_from_request),
    db: Session = Depends(get_db),
    query_service: QueryService = Depends(get_query_service)
):
    """
    åˆ›å»ºæŸ¥è¯¢è¯·æ±‚
    Story 3.1: æ ¸å¿ƒæŸ¥è¯¢ç«¯ç‚¹ï¼Œå¤„ç†è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    é›†æˆ LangGraph SQL Agentï¼ˆä½¿ç”¨ DeepSeek ä½œä¸ºé»˜è®¤ LLMï¼‰
    """
    # ============================================================
    # ğŸ” [è¯Šæ–­] /query ç«¯ç‚¹è¢«è°ƒç”¨ - è®°å½•å®Œæ•´è¯·æ±‚ä¿¡æ¯
    # ============================================================
    logger.info("="*80)
    logger.info("ğŸ” [è¯Šæ–­] /query ç«¯ç‚¹è¢«è°ƒç”¨")
    logger.info(f"ğŸ” [è¯Šæ–­] connection_id={request.connection_id}")
    logger.info(f"ğŸ” [è¯Šæ–­] query={request.query[:100]}")
    logger.info(f"ğŸ” [è¯Šæ–­] enable_cache={request.enable_cache}")
    logger.info(f"ğŸ” [è¯Šæ–­] force_refresh={request.force_refresh}")
    logger.info("="*80)
    print(f"ğŸ” [è¯Šæ–­] /query ç«¯ç‚¹è¢«è°ƒç”¨ - connection_id={request.connection_id}, query={request.query[:100]}")
    # ============================================================

    try:
        query_id = str(uuid.uuid4())
        start_time = time.time()
        
        # è·å–ç”¨æˆ·IDï¼ˆä»JWTä¸­æ­£ç¡®æå–ï¼‰
        user_id = user_info["user_id"]
        logger.info(f"Query request - user_id: {user_id}, tenant_id: {tenant.id}, query: {request.query[:100]}")

        # åˆ›å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
        query_context = get_query_context(db, tenant.id, user_id)

        # æ£€æŸ¥é¢‘ç‡é™åˆ¶
        can_proceed, error_msg = query_context.check_rate_limits()
        if not can_proceed:
            raise HTTPException(status_code=429, detail=error_msg)

        # æ•°æ®æºæœåŠ¡å®ä¾‹
        data_source_service = DataSourceService()

        # é€‰æ‹©æ•°æ®æºï¼šä¼˜å…ˆç”¨æˆ·æŒ‡å®šï¼Œå¦åˆ™è‡ªåŠ¨å–ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æºï¼›åç»­ä»…ä½¿ç”¨è¿™ä¸€æ¡
        data_source_id = request.connection_id
        selected_source = None
        
        # ğŸ” è¯Šæ–­ï¼šè®°å½•æ‰€æœ‰æ´»è·ƒæ•°æ®æºä¿¡æ¯
        all_active_sources = await data_source_service.get_data_sources(
            tenant_id=tenant.id,
            db=db,
            active_only=True
        )
        logger.info(f"ğŸ” [æ•°æ®æºè¯Šæ–­] ç§Ÿæˆ· {tenant.id} å…±æœ‰ {len(all_active_sources)} ä¸ªæ´»è·ƒæ•°æ®æº:")
        for idx, ds in enumerate(all_active_sources):
            logger.info(f"  [{idx+1}] ID: {ds.id}, åç§°: {ds.name}, ç±»å‹: {ds.db_type}, çŠ¶æ€: {ds.status}")
        
        if not data_source_id:
            if all_active_sources:
                selected_source = all_active_sources[0]
                data_source_id = selected_source.id
                logger.info(f"âš ï¸ [æ•°æ®æºè¯Šæ–­] æœªæŒ‡å®šæ•°æ®æºï¼Œè‡ªåŠ¨ä½¿ç”¨ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº: ID={data_source_id}, åç§°={selected_source.name}, ç±»å‹={selected_source.db_type}")
            else:
                logger.warning(f"âŒ [æ•°æ®æºè¯Šæ–­] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ´»è·ƒæ•°æ®æº")
        else:
            logger.info(f"âœ… [æ•°æ®æºè¯Šæ–­] ç”¨æˆ·æŒ‡å®šäº†æ•°æ®æºID: {data_source_id}")
            
        if data_source_id and not selected_source:
            selected_source = await data_source_service.get_data_source_by_id(
                data_source_id=data_source_id,
                tenant_id=tenant.id,
                db=db
            )
            if selected_source:
                logger.info(f"âœ… [æ•°æ®æºè¯Šæ–­] æ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æº: ID={selected_source.id}, åç§°={selected_source.name}, ç±»å‹={selected_source.db_type}")
            else:
                logger.error(f"âŒ [æ•°æ®æºè¯Šæ–­] æ— æ³•æ‰¾åˆ°æŒ‡å®šçš„æ•°æ®æºID: {data_source_id}")
                
        if not selected_source:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®æºï¼Œè¯·å…ˆé€‰æ‹©æˆ–åˆ›å»ºæ•°æ®æº"
            )
        
        # ğŸ” æœ€ç»ˆç¡®è®¤ä½¿ç”¨çš„æ•°æ®æº
        logger.info(f"ğŸ¯ [æ•°æ®æºè¯Šæ–­] æœ€ç»ˆä½¿ç”¨çš„æ•°æ®æº: ID={selected_source.id}, åç§°={selected_source.name}, ç±»å‹={selected_source.db_type}, è¿æ¥å­—ç¬¦ä¸²é¢„è§ˆ={str(selected_source.connection_string)[:50] if selected_source.connection_string else 'N/A'}...")

        # å°è¯•ä½¿ç”¨ Agent å¤„ç†æŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰æ•°æ®æºï¼‰
        use_agent = is_agent_available() and data_source_id is not None
        logger.info(
            "Query /query agent decision",
            tenant_id=tenant.id,
            user_id=user_id,
            connection_id=request.connection_id,
            data_source_id=str(data_source_id) if data_source_id else None,
            agent_available=is_agent_available(),
            use_agent=use_agent,
        )
        # æ·»åŠ é¢å¤–çš„è°ƒè¯•æ—¥å¿—
        print(f"[DEBUG] Query /query - connection_id: {request.connection_id}, data_source_id: {data_source_id}, use_agent: {use_agent}")
        
        agent_success = False
        if use_agent:
            try:
                # è·å–æ•°æ®æºè¿æ¥å­—ç¬¦ä¸²
                data_source_service = DataSourceService()
                database_url = await data_source_service.get_decrypted_connection_string(
                    data_source_id=data_source_id,
                    tenant_id=tenant.id,
                    db=db
                )
                
                # ç”Ÿæˆçº¿ç¨‹IDï¼ˆç”¨äºä¼šè¯ç®¡ç†ï¼‰
                thread_id = f"{tenant.id}_{user_id}_{query_id}"
                
                # è¿è¡Œ Agent æŸ¥è¯¢
                logger.info(
                    "Using Agent to handle query",
                    tenant_id=tenant.id,
                    user_id=user_id,
                    query_preview=request.query[:100],
                    data_source_id=data_source_id,
                    database_url_preview=str(database_url)[:80] if database_url else None,
                )
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ•°æ®æºç±»å‹ä¿®æ”¹é—®é¢˜ï¼Œæ˜ç¡®å‘Šè¯‰AIåŠ©æ‰‹æ•°æ®æºç±»å‹
                enhanced_question = request.query
                if selected_source.db_type in ["xlsx", "xls", "csv"]:
                    # æ–‡ä»¶æ•°æ®æºï¼šæ˜ç¡®å‘Šè¯‰AIè¿™æ˜¯æ–‡ä»¶ï¼Œå¿…é¡»ä½¿ç”¨æ–‡ä»¶å·¥å…·
                    enhanced_question = f"""ã€é‡è¦æç¤ºï¼šå½“å‰æ•°æ®æºæ˜¯{selected_source.db_type.upper()}æ–‡ä»¶ï¼Œä¸æ˜¯SQLæ•°æ®åº“ã€‘
                    
ä½ å¿…é¡»ï¼š
1. ä½¿ç”¨ `inspect_file` å·¥å…·æŸ¥çœ‹æ–‡ä»¶ç»“æ„å’Œå·¥ä½œè¡¨åç§°ï¼ˆå¯¹äºExcelæ–‡ä»¶ï¼‰
2. ä½¿ç”¨ `analyze_dataframe` æˆ– `python_interpreter` å·¥å…·æ‰§è¡ŒPandasæŸ¥è¯¢
3. **ä¸¥ç¦ä½¿ç”¨SQLå·¥å…·ï¼ˆquery, list_tables, get_schemaï¼‰**

ç”¨æˆ·é—®é¢˜ï¼š{request.query}"""
                    logger.info(f"ğŸ”§ [æ•°æ®æºç±»å‹ä¿®å¤] æ£€æµ‹åˆ°æ–‡ä»¶æ•°æ®æºï¼ˆ{selected_source.db_type}ï¼‰ï¼Œå·²å¢å¼ºé—®é¢˜æç¤º")
                    print(f"ğŸ”§ [æ•°æ®æºç±»å‹ä¿®å¤] æ£€æµ‹åˆ°æ–‡ä»¶æ•°æ®æºï¼ˆ{selected_source.db_type}ï¼‰ï¼Œå·²å¢å¼ºé—®é¢˜æç¤º")
                    print(f"ğŸ”§ [å¢å¼ºåçš„é—®é¢˜] {enhanced_question[:200]}...")
                elif selected_source.db_type in ["postgresql", "mysql", "postgres"]:
                    # SQLæ•°æ®åº“ï¼šæ˜ç¡®å‘Šè¯‰AIè¿™æ˜¯SQLæ•°æ®åº“
                    enhanced_question = f"""ã€é‡è¦æç¤ºï¼šå½“å‰æ•°æ®æºæ˜¯{selected_source.db_type.upper()}æ•°æ®åº“ã€‘

ä½ å¿…é¡»ï¼š
1. ä½¿ç”¨ `list_tables` å·¥å…·æŸ¥çœ‹æ•°æ®åº“ä¸­æœ‰å“ªäº›è¡¨
2. ä½¿ç”¨ `get_schema` å·¥å…·è·å–è¡¨ç»“æ„
3. ä½¿ç”¨ `query_database` å·¥å…·æ‰§è¡ŒSQLæŸ¥è¯¢
4. **ä¸¥ç¦ä½¿ç”¨æ–‡ä»¶å·¥å…·ï¼ˆinspect_file, analyze_dataframeï¼‰**

ç”¨æˆ·é—®é¢˜ï¼š{request.query}"""
                    logger.info(f"ğŸ”§ [æ•°æ®æºç±»å‹ä¿®å¤] æ£€æµ‹åˆ°SQLæ•°æ®åº“ï¼ˆ{selected_source.db_type}ï¼‰ï¼Œå·²å¢å¼ºé—®é¢˜æç¤º")
                    print(f"ğŸ”§ [æ•°æ®æºç±»å‹ä¿®å¤] æ£€æµ‹åˆ°SQLæ•°æ®åº“ï¼ˆ{selected_source.db_type}ï¼‰ï¼Œå·²å¢å¼ºé—®é¢˜æç¤º")
                
                agent_response = await run_agent_query(
                    question=enhanced_question,  # ä½¿ç”¨å¢å¼ºåçš„é—®é¢˜
                    thread_id=thread_id,
                    database_url=database_url,
                    verbose=True,  # ğŸ” å¯ç”¨è¯¦ç»†æ—¥å¿—ä»¥è¯Šæ–­ç¼–é€ æ•°æ®é—®é¢˜
                    enable_echarts=True,  # å¯ç”¨ ECharts å›¾è¡¨ç”ŸæˆåŠŸèƒ½
                    db_type=selected_source.db_type  # ä¼ é€’æ•°æ®åº“ç±»å‹
                )
                if agent_response:
                    logger.info(
                        "Agent query completed",
                        tenant_id=tenant.id,
                        user_id=user_id,
                        success=getattr(agent_response, "success", None),
                        sql_preview=(agent_response.sql or "")[:120] if hasattr(agent_response, "sql") else None,
                        row_count=getattr(getattr(agent_response, "data", None), "row_count", None),
                        error=getattr(agent_response, "error", None),
                    )
                else:
                    logger.warning(
                        "Agent query returned None, will fallback",
                        tenant_id=tenant.id,
                        user_id=user_id,
                    )

                if agent_response and agent_response.success:
                    # è½¬æ¢ Agent å“åº”ä¸º QueryResponseV3 æ ¼å¼
                    processing_time_ms = int((time.time() - start_time) * 1000)
                    response_data = convert_agent_response_to_query_response(
                        agent_response=agent_response,
                        query_id=query_id,
                        tenant_id=tenant.id,
                        original_query=request.query,
                        processing_time_ms=processing_time_ms
                    )
                    agent_success = True
                    return QueryResponseV3(**response_data)
                else:
                    # Agent å¤±è´¥ï¼Œä½†å°è¯•è¿”å›éƒ¨åˆ†ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    error_msg = getattr(agent_response, "error", "Agent unavailable") if agent_response else "Agent unavailable"
                    logger.warning(
                        "Agent æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•è¿”å›éƒ¨åˆ†ç»“æœæˆ–å›é€€åˆ°æ ‡å‡†æŸ¥è¯¢å¤„ç†",
                        tenant_id=tenant.id,
                        user_id=user_id,
                        error=error_msg,
                    )
                    
                    # å¦‚æœAgentè¿”å›äº†éƒ¨åˆ†ç»“æœï¼ˆå³ä½¿success=Falseï¼‰ï¼Œå°è¯•ä½¿ç”¨å®ƒ
                    if agent_response and hasattr(agent_response, "data") and agent_response.data and agent_response.data.row_count > 0:
                        logger.info("Agentè¿”å›äº†éƒ¨åˆ†ç»“æœï¼Œå°è¯•ä½¿ç”¨è¿™äº›ç»“æœ")
                        processing_time_ms = int((time.time() - start_time) * 1000)
                        response_data = convert_agent_response_to_query_response(
                            agent_response=agent_response,
                            query_id=query_id,
                            tenant_id=tenant.id,
                            original_query=request.query,
                            processing_time_ms=processing_time_ms
                        )
                        # å³ä½¿Agentå¤±è´¥ï¼Œå¦‚æœæœ‰æ•°æ®ä¹Ÿè¿”å›
                        return QueryResponseV3(**response_data)
                    
                    # å›é€€åˆ°æ ‡å‡†å¤„ç†æµç¨‹
                    agent_success = False
                    use_agent = False
            
            except Exception as e:
                # è·å–å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
                tb_str = traceback.format_exc()
                
                # æ‰“å°å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
                logger.error(
                    "ğŸš¨ CRITICAL: Agent failed, falling back to standard query. Reason: %s",
                    str(e),
                    extra={
                        "tenant_id": tenant.id,
                        "user_id": user_id,
                        "query_id": query_id,
                        "error_type": type(e).__name__,
                        "error_message": str(e),
                    }
                )
                
                # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
                logger.error(
                    "Agent æŸ¥è¯¢å‡ºé”™ - å®Œæ•´å †æ ˆè·Ÿè¸ª:\n%s",
                    tb_str,
                    extra={
                        "tenant_id": tenant.id,
                        "user_id": user_id,
                        "query_id": query_id,
                    }
                )
                
                # æ‰“å°é”™è¯¯å¯¹è±¡çš„è¯¦ç»†ä¿¡æ¯
                logger.error(
                    "Agent æŸ¥è¯¢å‡ºé”™ - é”™è¯¯è¯¦æƒ…: type=%s, message=%s, args=%s",
                    type(e).__name__,
                    str(e),
                    repr(e.args) if hasattr(e, 'args') else 'N/A',
                    extra={
                        "tenant_id": tenant.id,
                        "user_id": user_id,
                        "query_id": query_id,
                    }
                )
                
                agent_success = False
                use_agent = False
        
        # æ ‡å‡†æŸ¥è¯¢å¤„ç†æµç¨‹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰- å¦‚æœ Agent æœªä½¿ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æµç¨‹
        if not agent_success:
            # æ£€æŸ¥ç¼“å­˜ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ query ä½œä¸º hashï¼‰
            query_hash = hash(request.query)
            
            # å¤„ç†æŸ¥è¯¢ï¼ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼‰
            response_data = await query_service.process_query(
                query_id=query_id,
                question=request.query,  # ä½¿ç”¨ query å­—æ®µ
                context=None,
                options=None,
                selected_data_sources=[selected_source]
            )
            
            # æ„å»ºå“åº”ï¼ˆè½¬æ¢ä¸º QueryResponseV3 æ ¼å¼ï¼‰
            processing_time_ms = int((time.time() - start_time) * 1000)
            return QueryResponseV3(
                query_id=query_id,
                tenant_id=tenant.id,
                original_query=request.query,
                generated_sql=response_data.get("generated_sql", ""),
                results=response_data.get("results", []),
                row_count=response_data.get("row_count", 0),
                processing_time_ms=processing_time_ms,
                confidence_score=response_data.get("confidence", 0.5),
                explanation=response_data.get("answer", ""),
                processing_steps=response_data.get("processing_steps", []),
                validation_result=None,
                execution_result=None,
                correction_attempts=0
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Query processing failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")


@router.get("/query/status/{query_id}", response_model=QueryStatusResponse)
async def get_query_status(
    query_id: str,
    tenant=Depends(get_current_tenant_from_request),
    user_info: Dict[str, Any] = Depends(get_current_user_info_from_request),
    db: Session = Depends(get_db)
):
    """
    è·å–æŸ¥è¯¢çŠ¶æ€
    Story 3.1: æŸ¥è¯¢çŠ¶æ€è·Ÿè¸ªç«¯ç‚¹
    """
    try:
        # åˆ›å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç”¨æˆ·IDï¼‰
        user_id = user_info["user_id"]
        query_context = get_query_context(db, tenant.id, user_id)

        # æŸ¥è¯¢çŠ¶æ€
        from src.app.data.models import QueryLog
        query_log = db.query(QueryLog).filter(
            QueryLog.id == query_id,
            QueryLog.tenant_id == tenant.id
        ).first()

        if not query_log:
            raise HTTPException(status_code=404, detail="æŸ¥è¯¢ä¸å­˜åœ¨")

        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_percentage = None
        if query_log.status == QueryStatus.PROCESSING:
            # ç®€åŒ–çš„è¿›åº¦è®¡ç®—
            progress_percentage = 50.0  # å¤„ç†ä¸­çš„æŸ¥è¯¢é»˜è®¤50%
        elif query_log.status == QueryStatus.SUCCESS:
            progress_percentage = 100.0

        return QueryStatusResponse(
            query_id=query_id,
            status=query_log.status,
            created_at=query_log.created_at,
            updated_at=query_log.updated_at,
            response_time_ms=query_log.response_time_ms,
            error_message=query_log.error_message,
            progress_percentage=progress_percentage
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get query status failed: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢çŠ¶æ€å¤±è´¥: {str(e)}")


@router.delete("/query/cache/{query_hash}", response_model=QueryCacheResponse)
async def clear_query_cache(
    query_hash: str,
    tenant=Depends(get_current_tenant_from_request),
    db: Session = Depends(get_db)
):
    """
    æ¸…é™¤æŸ¥è¯¢ç¼“å­˜
    Story 3.1: æ¸…é™¤æŸ¥è¯¢ç¼“å­˜
    """
    try:
        # åˆ›å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
        query_context = get_query_context(db, tenant.id, tenant.id)

        # æ¸…é™¤ç¼“å­˜
        success, message = query_context.clear_query_cache(query_hash)

        return QueryCacheResponse(
            query_hash=query_hash,
            cache_cleared=success,
            message=message
        )

    except Exception as e:
        logger.error(f"Clear query cache failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")


@router.get("/query/history", response_model=QueryHistoryResponse)
async def get_query_history(
    page: int = QueryParam(1, ge=1, description="é¡µç "),
    page_size: int = QueryParam(10, ge=1, le=100, description="æ¯é¡µå¤§å°"),
    tenant=Depends(get_current_tenant_from_request),
    db: Session = Depends(get_db)
):
    """
    è·å–æŸ¥è¯¢å†å²
    Story 3.1: æŸ¥è¯¢å†å²è®°å½•
    """
    try:
        # åˆ›å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
        query_context = get_query_context(db, tenant.id, tenant.id)

        # è·å–æŸ¥è¯¢å†å²
        history = query_context.get_query_history(page, page_size)

        return QueryHistoryResponse(
            queries=history["queries"],
            total_count=history["total_count"],
            page=history["page"],
            page_size=history["page_size"]
        )

    except Exception as e:
        logger.error(f"Get query history failed: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {str(e)}")


# é”™è¯¯å¤„ç†å™¨ - å·²ç§»é™¤ï¼Œå› ä¸º APIRouter ä¸æ”¯æŒ exception_handler
# å¼‚å¸¸å¤„ç†åº”è¯¥åœ¨ main.py ä¸­å¤„ç†
# @router.exception_handler(HTTPException)
# async def http_exception_handler(request, exc):
#     """HTTPå¼‚å¸¸å¤„ç†"""
#     return JSONResponse(
#         status_code=exc.status_code,
#         content=ErrorResponse(
#             error_code=f"HTTP_{exc.status_code}",
#             message=exc.detail,
#             timestamp=datetime.utcnow()
#         ).dict()
#     )


# @router.exception_handler(Exception)
# async def general_exception_handler(request, exc):
#     """é€šç”¨å¼‚å¸¸å¤„ç†"""
#     logger.error(f"Unhandled exception in query endpoint: {exc}")
#     return JSONResponse(
#         status_code=500,
#         content=ErrorResponse(
#             error_code="INTERNAL_SERVER_ERROR",
#             message="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
#             timestamp=datetime.utcnow()
#         ).dict()
#     )