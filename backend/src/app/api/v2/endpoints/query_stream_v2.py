# -*- coding: utf-8 -*-
"""
Query Stream V2 Endpoint - æµå¼æŸ¥è¯¢ç«¯ç‚¹
======================================

æµå¼å“åº”ç«¯ç‚¹ï¼Œä½¿ç”¨ Server-Sent Events (SSE) åè®®ã€‚

API: POST /api/v2/query/stream

ç‰¹æ€§:
    - å®æ—¶æµå¼è¾“å‡º
    - å¤„ç†æ­¥éª¤æ¨é€
    - å¯å–æ¶ˆçš„é•¿æ—¶é—´æŸ¥è¯¢

ä½œè€…: BMad Master
ç‰ˆæœ¬: 2.0.0
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, AsyncGenerator, Callable
import logging
import json
import time
import asyncio
import re
from dataclasses import dataclass, field
from datetime import datetime

# ç¼“å­˜æœåŠ¡å¯¼å…¥
from src.app.services.cache_service import (
    get_cache_manager,
    TenantCacheKeyGenerator
)

# æ•°æ®åº“ä¾èµ–å¯¼å…¥
from src.app.data.database import SessionLocal

logger = logging.getLogger(__name__)

# ============================================================================
# ä¼šè¯çŠ¶æ€ç®¡ç†
# ============================================================================

@dataclass
class StreamSessionState:
    """æµå¼ä¼šè¯çŠ¶æ€"""
    session_id: str
    tenant_id: str
    user_id: str
    query: str
    status: str = "running"  # running, paused, completed, error
    accumulated_answer: str = ""
    current_progress: int = 0
    processing_steps: List[Dict[str, Any]] = field(default_factory=list)
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    abort_controller: Optional[asyncio.Event] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "session_id": self.session_id,
            "tenant_id": self.tenant_id,
            "user_id": self.user_id,
            "query": self.query,
            "status": self.status,
            "accumulated_answer": self.accumulated_answer,
            "current_progress": self.current_progress,
            "processing_steps": self.processing_steps,
            "created_at": self.created_at,
            "updated_at": self.updated_at
        }

# å…¨å±€ä¼šè¯çŠ¶æ€å­˜å‚¨ (ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis)
_active_sessions: Dict[str, StreamSessionState] = {}


def get_session_state(session_id: str) -> Optional[StreamSessionState]:
    """è·å–ä¼šè¯çŠ¶æ€"""
    return _active_sessions.get(session_id)


def set_session_state(state: StreamSessionState):
    """è®¾ç½®ä¼šè¯çŠ¶æ€"""
    _active_sessions[state.session_id] = state


def remove_session_state(session_id: str):
    """ç§»é™¤ä¼šè¯çŠ¶æ€"""
    _active_sessions.pop(session_id, None)

# ============================================================================
# å›¾è¡¨é…ç½®æå–å‡½æ•°
# ============================================================================

def extract_chart_config_from_answer(answer: str) -> Optional[str]:
    """ä» AI å›ç­”ä¸­æå–å›¾è¡¨é…ç½® JSON

    Args:
        answer: AI çš„æ–‡æœ¬å›ç­”

    Returns:
        JSON å­—ç¬¦ä¸²æ ¼å¼çš„å›¾è¡¨é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    if not answer or not answer.strip():
        return None

    # ç­–ç•¥0 (æœ€é«˜ä¼˜å…ˆçº§): å°è¯•åŒ¹é… [CHART_START]...[CHART_END] æ ¼å¼
    # è¿™æ˜¯ V1 API å’Œ AI ç”Ÿæˆå›¾è¡¨æ—¶ä½¿ç”¨çš„æ ‡å‡†æ ¼å¼
    chart_marker_pattern = r'\[CHART_START\]([\s\S]*?)\[CHART_END\]'
    marker_match = re.search(chart_marker_pattern, answer)
    if marker_match:
        json_str = marker_match.group(1).strip()
        try:
            parsed = json.loads(json_str)
            # ECharts é…ç½®é€šå¸¸åŒ…å« series, xAxis, yAxis, title ç­‰å­—æ®µ
            if any(key in parsed for key in ['series', 'xAxis', 'yAxis', 'title', 'legend', 'grid', 'tooltip']):
                logger.info(f"[å›¾è¡¨æå–] æˆåŠŸä» [CHART_START]...[CHART_END] æ ¼å¼æå– ECharts é…ç½®")
                return json.dumps(parsed, ensure_ascii=False)
            # ç®€åŒ–æ ¼å¼å›¾è¡¨é…ç½®
            elif any(key in parsed for key in ['chart_type', 'data', 'x_axis', 'y_axis']):
                logger.info(f"[å›¾è¡¨æå–] æˆåŠŸä» [CHART_START]...[CHART_END] æ ¼å¼æå–ç®€åŒ–å›¾è¡¨é…ç½®")
                return json.dumps(parsed, ensure_ascii=False)
        except json.JSONDecodeError as e:
            logger.warning(f"[å›¾è¡¨æå–] [CHART_START] JSON è§£æå¤±è´¥: {e}")

    # ç­–ç•¥1: å°è¯•åŒ¹é… ```json ... ``` ä»£ç å—
    json_pattern = r'```(?:json|JSON)\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, answer)

    if match:
        json_str = match.group(1).strip()
        # å¤„ç†åŒå¤§æ‹¬å·é—®é¢˜ï¼ˆPython f-string æ¨¡æ¿æ ¼å¼ï¼‰
        json_str = json_str.replace('{{', '{').replace('}}', '}')
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
        try:
            parsed = json.loads(json_str)
            # éªŒè¯æ˜¯å¦æ˜¯å›¾è¡¨é…ç½®
            if any(key in parsed for key in ['chart_type', 'series', 'data', 'title', 'x_axis', 'y_axis']):
                return json.dumps(parsed, ensure_ascii=False)
        except json.JSONDecodeError:
            pass

    # ç­–ç•¥2: å°è¯•åŒ¹é…ä»»æ„ä»£ç å—ä¸­çš„ JSON
    code_block_pattern = r'```\s*([\s\S]*?)\s*```'
    for match in re.finditer(code_block_pattern, answer):
        json_str = match.group(1).strip()
        # æ£€æŸ¥æ˜¯å¦åƒ JSONï¼ˆä»¥ { æˆ– [ å¼€å¤´ï¼‰
        if json_str.startswith('{') or json_str.startswith('['):
            # å¤„ç†åŒå¤§æ‹¬å·
            json_str = json_str.replace('{{', '{').replace('}}', '}')
            try:
                parsed = json.loads(json_str)
                if any(key in parsed for key in ['chart_type', 'series', 'data', 'title']):
                    return json.dumps(parsed, ensure_ascii=False)
            except json.JSONDecodeError:
                pass

    return None


# ============================================================================
# æ€§èƒ½ç›‘æ§è¾…åŠ©å‡½æ•°
# ============================================================================

def log_performance(
    step: str,
    tenant_id: str,
    user_id: str,
    duration_ms: float,
    metadata: Optional[Dict[str, Any]] = None
):
    """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
    logger.info(
        "Performance metric",
        extra={
            "metric_type": "query_performance",
            "step": step,
            "tenant_id": tenant_id,
            "user_id": user_id,
            "duration_ms": round(duration_ms, 2),
            "metadata": metadata or {}
        }
    )

# ============================================================================
# è·¯ç”±å™¨
# ============================================================================

router = APIRouter(prefix="/query", tags=["query-v2-stream"])

# ============================================================================
# è¯·æ±‚æ¨¡å‹
# ============================================================================

class StreamQueryRequestV2(BaseModel):
    """æµå¼æŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="è‡ªç„¶è¯­è¨€æŸ¥è¯¢", min_length=1)
    connection_id: Optional[str] = Field(None, description="æ•°æ®æºè¿æ¥ ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ ID")
    max_results: int = Field(100, ge=1, le=1000, description="æœ€å¤§ç»“æœæ•°")
    include_chart: bool = Field(False, description="æ˜¯å¦ç”Ÿæˆå›¾è¡¨")

# ============================================================================
# ç«¯ç‚¹
# ============================================================================

@router.post("/stream")
async def create_stream_query_v2(
    request: StreamQueryRequestV2,
    tenant_id: str = "default_tenant",
    user_id: str = "default_user"
):
    """
    æµå¼æŸ¥è¯¢ç«¯ç‚¹ (Server-Sent Events)

    è¿”å› SSE æ ¼å¼çš„æµå¼å“åº”ã€‚

    ## äº‹ä»¶ç±»å‹
    - `step`: å¤„ç†æ­¥éª¤æ›´æ–°
    - `progress`: è¿›åº¦æ›´æ–° (0-100)
    - `data`: éƒ¨åˆ†æ•°æ®
    - `error`: é”™è¯¯ä¿¡æ¯
    - `done`: å®Œæˆä¿¡å·

    ## ä½¿ç”¨ç¤ºä¾‹
    ```javascript
    const eventSource = new EventSource('/api/v2/query/stream?query=xxx');

    eventSource.addEventListener('step', (e) => {
        console.log('Step:', e.data);
    });

    eventSource.addEventListener('done', (e) => {
        console.log('Final result:', e.data);
        eventSource.close();
    });
    ```
    """
    # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
    request_start_time = time.time()

    async def event_generator() -> AsyncGenerator[str, None]:
        """SSE äº‹ä»¶ç”Ÿæˆå™¨"""

        def send_event(event_type: str, data: Dict[str, Any]):
            """å‘é€ SSE äº‹ä»¶ï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰"""
            event_data = json.dumps(data, ensure_ascii=False)
            yield f"event: {event_type}\n"
            yield f"data: {event_data}\n\n"

        try:
            # æ­¥éª¤æ—¶é—´è®°å½•
            step_timings: Dict[str, float] = {}
            overall_start = time.time()  # åˆå§‹åŒ–æ€»å¼€å§‹æ—¶é—´

            # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
            session_id = request.session_id or f"stream_{int(time.time() * 1000)}"
            abort_event = asyncio.Event()
            session_state = StreamSessionState(
                session_id=session_id,
                tenant_id=tenant_id,
                user_id=user_id,
                query=request.query,
                status="running",
                abort_controller=abort_event
            )
            set_session_state(session_state)

            # å‘é€å¼€å§‹äº‹ä»¶ï¼ˆåŒ…å« session_idï¼‰
            for event in send_event("start", {
                "query": request.query,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "timestamp": time.time()
            }):
                yield event

            # æ­¥éª¤ 1: æ¥æ”¶æŸ¥è¯¢ï¼ˆä¿ç•™ï¼Œä½œä¸ºå”¯ä¸€çš„åˆå§‹åŒ–æ­¥éª¤ï¼‰
            step_start = time.time()
            step_timings["receive_query"] = (time.time() - step_start) * 1000

            for event in send_event("step", {
                "step": 1,
                "message": "ç†è§£é—®é¢˜",
                "detail": f"æ­£åœ¨åˆ†æ: {request.query[:50]}...",
                "status": "running"
            }):
                yield event

            for event in send_event("progress", {"value": 10}):
                yield event

            # ğŸ”§ åˆ é™¤äº†æ­¥éª¤ 2ï¼ˆç§Ÿæˆ·éš”ç¦»éªŒè¯ï¼‰å’Œæ­¥éª¤ 3ï¼ˆAgentV2 å¤„ç†ï¼‰
            # è¿™äº›æ˜¯å†…éƒ¨æ­¥éª¤ï¼Œå¯¹ç”¨æˆ·æ— ä»·å€¼

            # ç¼“å­˜æ£€æŸ¥ï¼ˆå†…éƒ¨å¤„ç†ï¼Œä¸å‘é€æ­¥éª¤ï¼‰
            step_start = time.time()
            cache_manager = get_cache_manager()
            cache_hit = False
            cached_data = None

            if cache_manager is not None:
                cache_key = TenantCacheKeyGenerator.generate_v2_query_key(
                    tenant_id, user_id, request.query, request.session_id
                )
                cached_data = await cache_manager.cache.get(cache_key)
                cache_hit = cached_data is not None

            step_timings["cache_check"] = (time.time() - step_start) * 1000

            if cache_hit and cached_data:
                # ç¼“å­˜å‘½ä¸­ - æµå¼è¿”å›ç¼“å­˜ç»“æœ
                step_timings["agent_execution"] = 0

                # ä»ç¼“å­˜æ•°æ®ä¸­æå–ç­”æ¡ˆ
                cached_answer = cached_data.get("answer", "")
                processing_steps = cached_data.get("processing_steps", [])

                for event in send_event("progress", {"value": 80}):
                    yield event

                # åˆ†å—å‘é€ç­”æ¡ˆ
                step_start = time.time()
                chunk_size = 200
                for i in range(0, len(cached_answer), chunk_size):
                    chunk = cached_answer[i:i+chunk_size]
                    progress = 80 + int((i / len(cached_answer)) * 15)

                    for event in send_event("data", {
                        "chunk": chunk,
                        "progress": progress
                    }):
                        yield event

                step_timings["answer_streaming"] = (time.time() - step_start) * 1000

                # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                total_processing_time_ms = (time.time() - overall_start) * 1000

                # å®Œæˆäº‹ä»¶
                log_performance(
                    step="stream_query_cache_hit",
                    tenant_id=tenant_id,
                    user_id=user_id,
                    duration_ms=total_processing_time_ms,
                    metadata={
                        "query_length": len(request.query),
                        "answer_length": len(cached_answer),
                        "step_timings": step_timings,
                        "cache_hit": True
                    }
                )

                for event in send_event("done", {
                    "success": True,
                    "answer": cached_answer,
                    "processing_steps": processing_steps,
                    "tenant_id": tenant_id,
                    "processing_time_ms": round(total_processing_time_ms, 2),
                    "step_timings": {k: round(v, 2) for k, v in step_timings.items()},
                    "from_cache": True
                }):
                    yield event

                for event in send_event("progress", {"value": 100}):
                    yield event

            else:
                # ç¼“å­˜æœªå‘½ä¸­ - æ‰§è¡Œ AgentV2 æŸ¥è¯¢
                step_start = time.time()
                try:
                    from AgentV2.core import get_default_factory

                    agent_factory = get_default_factory()

                    # è·å–æ•°æ®åº“ä¼šè¯ç”¨äºæŸ¥è¯¢æ•°æ®æºé…ç½®
                    db_session = SessionLocal()
                    try:
                        agent = agent_factory.get_or_create_agent(
                            tenant_id=tenant_id,
                            user_id=user_id,
                            session_id=request.session_id,
                            connection_id=request.connection_id,
                            db_session=db_session,
                            force_refresh=True  # ğŸ”§ å¼ºåˆ¶åˆ·æ–°ä»¥ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ç³»ç»Ÿæç¤ºè¯
                        )

                        # ğŸ”§ ä½¿ç”¨åŸå§‹ç”¨æˆ·æŸ¥è¯¢ï¼ˆCHART_GUIDANCE_TEMPLATE å·²åŒ…å«å›¾è¡¨ç”ŸæˆæŒ‡ä»¤ï¼‰
                        agent_input = {
                            "messages": [
                                {"role": "user", "content": request.query}
                            ]
                        }

                        # ğŸ”§ åˆ é™¤äº† AgentV2 å¤„ç†æ­¥éª¤çš„å‘é€ï¼Œç›´æ¥è¿›å…¥å®é™…å·¥å…·è°ƒç”¨
                        for event in send_event("progress", {"value": 20}):
                            yield event

                        # ğŸ”§ğŸ”§ğŸ”§ ä½¿ç”¨ astream_events å®ç°çœŸæ­£çš„ token çº§åˆ«æµå¼è¾“å‡º
                        # å‚è€ƒ: LangGraph æ–‡æ¡£ - Streaming Events
                        # astream_events å¯ä»¥æ•è· LLM ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¯ä¸ª token
                        all_messages = []
                        accumulated_answer = ""
                        step_count = 0
                        processing_step_number = 1  # ğŸ”§ ä»æ­¥éª¤1å¼€å§‹è®¡æ•°ï¼ˆåˆ é™¤äº†æ­¥éª¤2ã€3ï¼‰
                        last_progress_update = time.time()
                        current_tool_call = None  # è·Ÿè¸ªå½“å‰å·¥å…·è°ƒç”¨

                        async for event in agent.astream_events(
                            agent_input,
                            config={"configurable": {"thread_id": request.session_id}},
                            version="v2"
                        ):
                            event_kind = event.get("event", "")
                            event_data = event.get("data", {})

                            # ğŸ”§ å¤„ç† LLM æµå¼è¾“å‡º (token çº§åˆ«)
                            if event_kind == "on_chat_model_stream":
                                chunk = event_data.get("chunk")
                                if chunk and hasattr(chunk, "content") and chunk.content:
                                    # ç´¯ç§¯ç­”æ¡ˆ
                                    accumulated_answer += chunk.content
                                    
                                    # è®¡ç®—è¿›åº¦ (30% -> 80%)
                                    step_count += 1
                                    progress = 30 + min(int((step_count / 100) * 50), 50)
                                    
                                    # å®æ—¶å‘é€æ¯ä¸ª token
                                    for sse in send_event("data", {
                                        "chunk": chunk.content,
                                        "progress": progress
                                    }):
                                        yield sse
                                    
                                    # å®šæœŸå‘é€è¿›åº¦æ›´æ–°ï¼ˆæ¯ 0.5 ç§’ï¼‰
                                    now = time.time()
                                    if now - last_progress_update > 0.5:
                                        for sse in send_event("progress", {"value": progress}):
                                            yield sse
                                        last_progress_update = now

                            # ğŸ”§ å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
                            elif event_kind == "on_tool_start":
                                tool_name = event.get("name", "unknown")
                                tool_input = event_data.get("input", {})
                                
                                processing_step_number += 1
                                step_data = {
                                    "step": processing_step_number,
                                    "message": f"è°ƒç”¨å·¥å…·: {tool_name}",
                                    "status": "running",
                                    "duration": 0
                                }
                                
                                # æ ¹æ®å·¥å…·ç±»å‹æ·»åŠ å†…å®¹è¯¦æƒ…
                                if "sql" in tool_name.lower() or "query" in tool_name.lower():
                                    sql_query = tool_input.get("query") or tool_input.get("sql", "")
                                    if sql_query:
                                        step_data["content_type"] = "sql"
                                        step_data["content_data"] = {"sql": sql_query}
                                        step_data["detail"] = f"æ‰§è¡ŒæŸ¥è¯¢: {sql_query[:100]}..."
                                elif "schema" in tool_name.lower():
                                    step_data["message"] = "è·å–æ•°æ®åº“ç»“æ„"
                                    step_data["detail"] = f"è¡¨: {tool_input.get('table_name', 'unknown')}"
                                elif "list" in tool_name.lower() and "table" in tool_name.lower():
                                    step_data["message"] = "åˆ—å‡ºæ•°æ®åº“è¡¨"
                                    step_data["detail"] = "æ­£åœ¨è·å–è¡¨åˆ—è¡¨..."
                                elif "chart" in tool_name.lower():
                                    step_data["message"] = "ç”Ÿæˆå›¾è¡¨"
                                    step_data["detail"] = "æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨..."
                                
                                current_tool_call = step_data
                                for sse in send_event("step", step_data):
                                    yield sse

                            # ğŸ”§ å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸ
                            elif event_kind == "on_tool_end":
                                if current_tool_call:
                                    raw_output = event_data.get("output", "")
                                    
                                    # ğŸ”§ ä¿®å¤ï¼šLangGraph çš„ on_tool_end è¿”å›çš„æ˜¯ ToolMessage å¯¹è±¡
                                    # éœ€è¦ä» content å±æ€§è·å–å®é™…çš„å­—ç¬¦ä¸²è¾“å‡º
                                    if hasattr(raw_output, 'content'):
                                        tool_output = raw_output.content
                                        logger.info(f"[V2 Stream] on_tool_end: ToolMessage detected, content_len={len(tool_output) if tool_output else 0}")
                                    else:
                                        tool_output = raw_output if isinstance(raw_output, str) else str(raw_output)
                                        logger.info(f"[V2 Stream] on_tool_end: raw output, type={type(raw_output).__name__}")
                                    
                                    current_tool_call["status"] = "completed"
                                    current_tool_call["duration"] = 100  # ä¼°ç®—æ—¶é—´
                                    
                                    # ğŸ”§ å¢å¼ºï¼šæ ¹æ®å·¥å…·ç±»å‹æå–æœ‰ç”¨ä¿¡æ¯åˆ° detail
                                    tool_message = current_tool_call.get("message", "")
                                    if tool_output and isinstance(tool_output, str):
                                        try:
                                            import json as json_module
                                            output_data = json_module.loads(tool_output)
                                            
                                            # åˆ—å‡ºæ•°æ®åº“è¡¨ - æ˜¾ç¤ºè¡¨ååˆ—è¡¨
                                            if "åˆ—å‡ºæ•°æ®åº“è¡¨" in tool_message or "list" in tool_message.lower():
                                                if isinstance(output_data, list):
                                                    table_names = [t.get("table_name", t.get("name", str(t))) if isinstance(t, dict) else str(t) for t in output_data[:10]]
                                                    current_tool_call["detail"] = f"æ‰¾åˆ° {len(output_data)} å¼ è¡¨: {', '.join(table_names)}"
                                                    if len(output_data) > 10:
                                                        current_tool_call["detail"] += "..."
                                            
                                            # è·å–æ•°æ®åº“ç»“æ„ - æ˜¾ç¤ºåˆ—ä¿¡æ¯
                                            elif "è·å–æ•°æ®åº“ç»“æ„" in tool_message or "schema" in tool_message.lower():
                                                if isinstance(output_data, dict):
                                                    columns = output_data.get("columns", [])
                                                    if columns:
                                                        col_names = [c.get("name", str(c)) if isinstance(c, dict) else str(c) for c in columns[:5]]
                                                        current_tool_call["detail"] = f"åŒ…å« {len(columns)} åˆ—: {', '.join(col_names)}"
                                                        if len(columns) > 5:
                                                            current_tool_call["detail"] += "..."
                                        except (json_module.JSONDecodeError, TypeError):
                                            pass
                                    
                                    for sse in send_event("step", current_tool_call):
                                        yield sse
                                    
                                    # ğŸ”§ ä»å·¥å…·è¾“å‡ºä¸­æå–è¡¨æ ¼æ•°æ®
                                    if tool_output and isinstance(tool_output, str):
                                        # å°è¯•è§£æä¸º JSON è¡¨æ ¼æ•°æ®
                                        try:
                                            import json as json_module
                                            output_data = json_module.loads(tool_output)
                                            logger.info(f"[V2 Stream] å·¥å…·è¾“å‡ºè§£ææˆåŠŸï¼Œç±»å‹: {type(output_data).__name__}")
                                            
                                            # æ£€æµ‹æ˜¯å¦ä¸ºè¡¨æ ¼æ ¼å¼ï¼ˆåŒ…å« columns å’Œ data/rowsï¼‰
                                            if isinstance(output_data, dict):
                                                columns = output_data.get("columns", [])
                                                rows = output_data.get("data", output_data.get("rows", []))
                                                row_count = output_data.get("row_count", len(rows) if isinstance(rows, list) else 0)
                                                logger.info(f"[V2 Stream] æ£€æµ‹è¡¨æ ¼æ•°æ®: columns={len(columns)}, rows={len(rows) if rows else 0}, row_count={row_count}")
                                                
                                                if columns and rows:
                                                    # å‘é€è¡¨æ ¼æ•°æ®æ­¥éª¤
                                                    processing_step_number += 1
                                                    table_step = {
                                                        "step": processing_step_number,
                                                        "message": "æŸ¥è¯¢ç»“æœ",
                                                        "status": "completed",
                                                        "duration": 50,
                                                        "content_type": "table",
                                                        "content_data": {
                                                            "table": {
                                                                "columns": columns,
                                                                "rows": rows[:50],  # é™åˆ¶å‰50è¡Œ
                                                                "row_count": row_count
                                                            }
                                                        }
                                                    }
                                                    for sse in send_event("step", table_step):
                                                        yield sse
                                                    logger.info(f"[V2 Stream] å‘é€è¡¨æ ¼æ•°æ®: {row_count} è¡Œ, {len(columns)} åˆ—")
                                            
                                            # æ£€æµ‹æ˜¯å¦ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆç›´æ¥æ˜¯è¡Œæ•°ç»„ï¼‰
                                            elif isinstance(output_data, list) and len(output_data) > 0:
                                                if isinstance(output_data[0], dict):
                                                    columns = list(output_data[0].keys())
                                                    rows = output_data
                                                    row_count = len(rows)
                                                    
                                                    # å‘é€è¡¨æ ¼æ•°æ®æ­¥éª¤
                                                    processing_step_number += 1
                                                    table_step = {
                                                        "step": processing_step_number,
                                                        "message": "æŸ¥è¯¢ç»“æœ",
                                                        "status": "completed",
                                                        "duration": 50,
                                                        "content_type": "table",
                                                        "content_data": {
                                                            "table": {
                                                                "columns": columns,
                                                                "rows": rows[:50],
                                                                "row_count": row_count
                                                            }
                                                        }
                                                    }
                                                    for sse in send_event("step", table_step):
                                                        yield sse
                                                    logger.info(f"[V2 Stream] å‘é€è¡¨æ ¼æ•°æ® (åˆ—è¡¨): {row_count} è¡Œ")
                                        except (json_module.JSONDecodeError, TypeError):
                                            # ä¸æ˜¯ JSON æ ¼å¼ï¼Œè·³è¿‡
                                            pass
                                    
                                    current_tool_call = None

                            # ğŸ”§ å¤„ç† LLM è°ƒç”¨ç»“æŸï¼ˆæ”¶é›†æœ€ç»ˆæ¶ˆæ¯ï¼‰
                            elif event_kind == "on_chat_model_end":
                                output = event_data.get("output")
                                if output:
                                    all_messages.append(output)

                        step_timings["agent_execution"] = (time.time() - step_start) * 1000

                        # ä»æµå¼æ¶ˆæ¯ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
                        answer = accumulated_answer

                        # ğŸ”§ å§‹ç»ˆå°è¯•æå–å›¾è¡¨é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        # ä¸å†æ£€æŸ¥ include_chart æ ‡å¿—ï¼Œå› ä¸º AI å¯èƒ½ä¼šæ ¹æ®é—®é¢˜ç±»å‹è‡ªä¸»å†³å®šç”Ÿæˆå›¾è¡¨
                        chart_config = extract_chart_config_from_answer(answer)
                        if chart_config:
                            logger.info(f"[V2 Stream] æˆåŠŸæå–å›¾è¡¨é…ç½®: {chart_config[:100]}...")

                        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                        total_processing_time_ms = (time.time() - overall_start) * 1000

                        # å®Œæˆäº‹ä»¶
                        processing_steps = [
                            "æ¥æ”¶æŸ¥è¯¢",
                            "ç§Ÿæˆ·éš”ç¦»éªŒè¯",
                            "AgentV2 å¤„ç†",
                            "DeepSeek LLM è°ƒç”¨",
                            "è¿”å›ç»“æœ"
                        ]

                        # è®°å½•æ€§èƒ½æ—¥å¿—
                        log_performance(
                            step="stream_query_complete",
                            tenant_id=tenant_id,
                            user_id=user_id,
                            duration_ms=total_processing_time_ms,
                            metadata={
                                "query_length": len(request.query),
                                "answer_length": len(answer),
                                "step_timings": step_timings,
                                "processing_steps": processing_steps,
                                "connection_id": request.connection_id
                            }
                        )

                        # å­˜å‚¨åˆ°ç¼“å­˜ï¼ˆå¦‚æœç¼“å­˜ç®¡ç†å™¨å¯ç”¨ï¼‰
                        if cache_manager is not None and answer:
                            cache_key = TenantCacheKeyGenerator.generate_v2_query_key(
                                tenant_id, user_id, request.query, request.session_id
                            )
                            cache_data = {
                                "answer": answer,
                                "processing_steps": processing_steps,
                                "query": request.query
                            }
                            await cache_manager.cache.set(cache_key, cache_data, ttl=600)
                            logger.debug(f"æŸ¥è¯¢ç»“æœå·²ç¼“å­˜: {cache_key}")

                        for event in send_event("done", {
                            "success": True,
                            "answer": answer,
                            "chart_config": chart_config,  # ğŸ”§ æ·»åŠ å›¾è¡¨é…ç½®
                            "processing_steps": processing_steps,
                            "tenant_id": tenant_id,
                            "processing_time_ms": round(total_processing_time_ms, 2),
                            "step_timings": {k: round(v, 2) for k, v in step_timings.items()},
                            "connection_id": request.connection_id
                        }):
                            yield event

                        for event in send_event("progress", {"value": 100}):
                            yield event
                    finally:
                        db_session.close()

                except ImportError:
                    # AgentV2 ä¸å¯ç”¨
                    total_processing_time_ms = (time.time() - overall_start) * 1000

                    log_performance(
                        step="stream_query_import_error",
                        tenant_id=tenant_id,
                        user_id=user_id,
                        duration_ms=total_processing_time_ms,
                        metadata={"error": "AgentV2 not available"}
                    )

                    for event in send_event("error", {
                        "error": "AgentV2 not available",
                        "detail": "æµå¼æŸ¥è¯¢åŠŸèƒ½éœ€è¦ AgentV2 æ¨¡å—"
                    }):
                        yield event

        except Exception as e:
            total_processing_time_ms = (time.time() - overall_start) * 1000

            log_performance(
                step="stream_query_error",
                tenant_id=tenant_id,
                user_id=user_id,
                duration_ms=total_processing_time_ms,
                metadata={"error": str(e), "error_type": type(e).__name__}
            )

            logger.error(f"Stream query error: {e}")
            for event in send_event("error", {
                "error": str(e),
                "error_type": "internal_error"
            }):
                yield event

        finally:
            # æ¸…ç†ä¼šè¯çŠ¶æ€
            if 'session_state' in locals():
                if session_state.status == "running":
                    session_state.status = "completed"
                session_state.updated_at = time.time()
                # ä¿ç•™ä¼šè¯çŠ¶æ€ä¸€æ®µæ—¶é—´ä»¥ä¾¿å®¢æˆ·ç«¯æŸ¥è¯¢çŠ¶æ€
                # å¯ä»¥åœ¨ä¹‹åçš„ä»»åŠ¡ä¸­æ·»åŠ å®šæ—¶æ¸…ç†æœºåˆ¶

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )


@router.get("/stream/health")
async def stream_health_check():
    """æµå¼ç«¯ç‚¹å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "version": "2.0.0",
        "streaming": "enabled",
        "protocol": "Server-Sent Events (SSE)"
    }


# ============================================================================
# ä¼šè¯ç®¡ç†ç«¯ç‚¹
# ============================================================================

@router.get("/stream/session/{session_id}")
async def get_session_status(session_id: str):
    """
    è·å–æµå¼ä¼šè¯çŠ¶æ€

    Args:
        session_id: ä¼šè¯ID

    Returns:
        ä¼šè¯çŠ¶æ€ä¿¡æ¯
    """
    session_state = get_session_state(session_id)

    if session_state is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
        )

    return session_state.to_dict()


@router.post("/stream/session/{session_id}/pause")
async def pause_stream_session(session_id: str):
    """
    æš‚åœæµå¼æŸ¥è¯¢

    Args:
        session_id: ä¼šè¯ID

    Returns:
        æ“ä½œç»“æœ
    """
    session_state = get_session_state(session_id)

    if session_state is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
        )

    if session_state.status != "running":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"åªèƒ½æš‚åœæ­£åœ¨è¿è¡Œçš„ä¼šè¯ï¼Œå½“å‰çŠ¶æ€: {session_state.status}"
        )

    # æ›´æ–°çŠ¶æ€ä¸ºæš‚åœ
    session_state.status = "paused"
    session_state.updated_at = time.time()
    set_session_state(session_state)

    # è®¾ç½®ä¸­æ­¢äº‹ä»¶ä»¥åœæ­¢æµå¼è¾“å‡º
    if session_state.abort_controller:
        session_state.abort_controller.set()

    logger.info(f"ä¼šè¯ {session_id} å·²æš‚åœ")

    return {
        "success": True,
        "session_id": session_id,
        "status": "paused",
        "accumulated_answer": session_state.accumulated_answer,
        "current_progress": session_state.current_progress
    }


@router.post("/stream/session/{session_id}/resume")
async def resume_stream_session(
    session_id: str,
    tenant_id: str = "default_tenant",
    user_id: str = "default_user"
):
    """
    æ¢å¤æš‚åœçš„æµå¼æŸ¥è¯¢

    æ³¨æ„: ç”±äºæµå¼æŸ¥è¯¢çš„ç‰¹æ€§ï¼Œå®Œæ•´æ¢å¤éœ€è¦é‡æ–°å‘èµ·æŸ¥è¯¢ã€‚
    æ­¤ç«¯ç‚¹è¿”å›å·²ç´¯ç§¯çš„å†…å®¹ï¼Œå®¢æˆ·ç«¯å¯å†³å®šæ˜¯å¦é‡æ–°æŸ¥è¯¢ã€‚

    Args:
        session_id: ä¼šè¯ID
        tenant_id: ç§Ÿæˆ·ID
        user_id: ç”¨æˆ·ID

    Returns:
        å·²ç´¯ç§¯çš„å†…å®¹å’Œå»ºè®®æ“ä½œ
    """
    session_state = get_session_state(session_id)

    if session_state is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
        )

    if session_state.status != "paused":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"åªèƒ½æ¢å¤å·²æš‚åœçš„ä¼šè¯ï¼Œå½“å‰çŠ¶æ€: {session_state.status}"
        )

    # æ›´æ–°çŠ¶æ€
    session_state.status = "running"
    session_state.updated_at = time.time()
    set_session_state(session_state)

    logger.info(f"ä¼šè¯ {session_id} å·²æ¢å¤")

    return {
        "success": True,
        "session_id": session_id,
        "status": "running",
        "message": "ç”±äºæµå¼æŸ¥è¯¢çš„ç‰¹æ€§ï¼Œå®Œæ•´æ¢å¤éœ€è¦é‡æ–°å‘èµ·æŸ¥è¯¢",
        "accumulated_answer": session_state.accumulated_answer,
        "current_progress": session_state.current_progress,
        "recommendation": "ä½¿ç”¨ç›¸åŒå‚æ•°é‡æ–°å‘èµ· /stream æŸ¥è¯¢ä»¥è·å¾—å®Œæ•´ç»“æœ"
    }


@router.delete("/stream/session/{session_id}")
async def cancel_stream_session(session_id: str):
    """
    å–æ¶ˆæµå¼æŸ¥è¯¢

    Args:
        session_id: ä¼šè¯ID

    Returns:
        æ“ä½œç»“æœ
    """
    session_state = get_session_state(session_id)

    if session_state is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
        )

    # æ›´æ–°çŠ¶æ€ä¸ºå·²å–æ¶ˆ
    session_state.status = "cancelled"
    session_state.updated_at = time.time()

    # è®¾ç½®ä¸­æ­¢äº‹ä»¶
    if session_state.abort_controller:
        session_state.abort_controller.set()

    # ä»æ´»åŠ¨ä¼šè¯ä¸­ç§»é™¤
    remove_session_state(session_id)

    logger.info(f"ä¼šè¯ {session_id} å·²å–æ¶ˆ")

    return {
        "success": True,
        "session_id": session_id,
        "status": "cancelled",
        "accumulated_answer": session_state.accumulated_answer
    }
