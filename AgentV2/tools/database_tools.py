# -*- coding: utf-8 -*-
"""
Database Query Tools - æ•°æ®åº“æŸ¥è¯¢å·¥å…· (å¸¦ç¼“å­˜ä¼˜åŒ– + Excelæ”¯æŒ)
===============================================================

ä¸º AgentV2 æä¾›æ•°æ®åº“å’Œ Excel æ–‡ä»¶æŸ¥è¯¢èƒ½åŠ›ã€‚

æ ¸å¿ƒåŠŸèƒ½:
    - execute_query: æ‰§è¡Œåªè¯» SQL æŸ¥è¯¢ï¼ˆæ•°æ®åº“å’Œ Excelï¼‰
    - list_tables: åˆ—å‡ºæ•°æ®åº“è¡¨æˆ– Excel å·¥ä½œè¡¨
    - get_schema: è·å–è¡¨ç»“æ„æˆ– Excel åˆ—ä¿¡æ¯

ä¼˜åŒ–ç‰¹æ€§:
    - Schema ç¼“å­˜ï¼šé¿å…é‡å¤æŸ¥è¯¢è¡¨ç»“æ„
    - æŸ¥è¯¢ç»“æœç¼“å­˜ï¼šç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ç¼“å­˜
    - TTL æœºåˆ¶ï¼šç¼“å­˜è¿‡æœŸè‡ªåŠ¨åˆ·æ–°
    - å¤šæ•°æ®æºæ”¯æŒï¼šPostgreSQL, MySQL, Excel æ–‡ä»¶

ä½œè€…: BMad Master
ç‰ˆæœ¬: 3.0.0
"""

import os
import hashlib
import json
import time
from typing import Optional, List, Dict, Any, Tuple
from functools import wraps
import logging

# ä½¿ç”¨ contextvars æ›¿ä»£ threading.localï¼Œæ”¯æŒå¼‚æ­¥/å¤šçº¿ç¨‹ç¯å¢ƒ
from contextvars import ContextVar

logger = logging.getLogger(__name__)

# ============================================================================
# è¿æ¥ä¸Šä¸‹æ–‡å­˜å‚¨ (ä½¿ç”¨ contextvars æ”¯æŒå¼‚æ­¥/å¤šçº¿ç¨‹)
# ============================================================================

_connection_id_ctx: ContextVar[Optional[str]] = ContextVar("connection_id", default=None)
_db_session_ctx: ContextVar[Optional[Any]] = ContextVar("db_session", default=None)
_tenant_id_ctx: ContextVar[Optional[str]] = ContextVar("tenant_id", default=None)

def _set_connection_context(
    connection_id: Optional[str] = None,
    db_session: Optional[Any] = None,
    tenant_id: Optional[str] = None
) -> None:
    """è®¾ç½®è¿æ¥ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå·¥å…·è°ƒç”¨ï¼‰"""
    _connection_id_ctx.set(connection_id)
    _db_session_ctx.set(db_session)
    _tenant_id_ctx.set(tenant_id)
    logger.info(f"[CONTEXT_SET] connection_id={connection_id}, tenant_id={tenant_id}")


def _get_connection_context() -> Tuple[Optional[str], Optional[Any], Optional[str]]:
    """è·å–è¿æ¥ä¸Šä¸‹æ–‡"""
    connection_id = _connection_id_ctx.get()
    db_session = _db_session_ctx.get()
    tenant_id = _tenant_id_ctx.get()
    logger.info(f"[CONTEXT_GET] connection_id={connection_id}, tenant_id={tenant_id}")
    return (connection_id, db_session, tenant_id)


def _clear_connection_context() -> None:
    """æ¸…é™¤è¿æ¥ä¸Šä¸‹æ–‡"""
    _connection_id_ctx.set(None)
    _db_session_ctx.set(None)
    _tenant_id_ctx.set(None)
    logger.info("[CONTEXT_CLEAR] Connection context cleared")

# ============================================================================
# ç¼“å­˜ç®¡ç†
# ============================================================================

class SimpleCache:
    """å¢å¼ºçš„å†…å­˜ç¼“å­˜ï¼Œæ”¯æŒç»Ÿè®¡"""

    def __init__(self, ttl: int = 300, name: str = "cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜

        Args:
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
            name: ç¼“å­˜åç§°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        """
        self._cache: Dict[str, tuple] = {}  # key -> (value, expire_time)
        self.ttl = ttl
        self.name = name
        self._hits = 0
        self._misses = 0
        self._sets = 0

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key in self._cache:
            value, expire_time = self._cache[key]
            if time.time() < expire_time:
                self._hits += 1
                return value
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self._cache[key]
        self._misses += 1
        return None

    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜"""
        expire_time = time.time() + self.ttl
        self._cache[key] = (value, expire_time)
        self._sets += 1

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._hits = 0
        self._misses = 0
        self._sets = 0

    def has(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        return self.get(key) is not None

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self._hits + self._misses
        hit_rate = self._hits / total_requests if total_requests > 0 else 0
        return {
            "name": self.name,
            "size": len(self._cache),
            "hits": self._hits,
            "misses": self._misses,
            "sets": self._sets,
            "hit_rate": hit_rate,
            "ttl": self.ttl
        }


# å…¨å±€ç¼“å­˜å®ä¾‹
_schema_cache = SimpleCache(ttl=600, name="schema_cache")  # Schema ç¼“å­˜ 10 åˆ†é’Ÿ
_query_cache = SimpleCache(ttl=300, name="query_cache")    # æŸ¥è¯¢ç»“æœç¼“å­˜ 5 åˆ†é’Ÿ (å»¶é•¿ TTL)


def _normalize_sql(sql: str) -> str:
    """
    æ ‡å‡†åŒ– SQL æŸ¥è¯¢ç”¨äºç¼“å­˜é”®ç”Ÿæˆ

    å¤„ç†:
        - è½¬æ¢ä¸ºå°å†™
        - ç§»é™¤å¤šä½™ç©ºæ ¼
        - ç§»é™¤æ³¨é‡Š
        - ç»Ÿä¸€åˆ†å·ä½¿ç”¨

    Args:
        sql: åŸå§‹ SQL æŸ¥è¯¢

    Returns:
        æ ‡å‡†åŒ–åçš„ SQL
    """
    import re

    # è½¬æ¢ä¸ºå°å†™
    normalized = sql.lower().strip()

    # ç§»é™¤æ³¨é‡Š
    normalized = re.sub(r'--.*$', '', normalized, flags=re.MULTILINE)
    normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)

    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    normalized = ' '.join(normalized.split())

    # ç¡®ä¿ä»¥åˆ†å·ç»“å°¾
    if not normalized.endswith(';'):
        normalized += ';'

    return normalized


def _make_cache_key(*args, **kwargs) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    # å¦‚æœå‚æ•°åŒ…å« SQLï¼Œå…ˆæ ‡å‡†åŒ–
    if args and 'select' in str(args[0]).lower():
        args = (_normalize_sql(args[0]),) + args[1:]

    key_str = json.dumps([args, kwargs], sort_keys=True, default=str)
    return hashlib.md5(key_str.encode()).hexdigest()


def get_cache_stats() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "schema_cache": _schema_cache.get_stats(),
        "query_cache": _query_cache.get_stats()
    }

# ============================================================================
# æ•°æ®åº“è¿æ¥ç®¡ç†
# ============================================================================

def get_database_url(
    connection_id: Optional[str] = None
) -> Tuple[str, Optional["DataSourceConnectionInfo"]]:
    """
    è·å–æ•°æ®åº“è¿æ¥ URL æˆ– Excel æ–‡ä»¶è·¯å¾„

    Args:
        connection_id: å¯é€‰çš„æ•°æ®æºè¿æ¥ ID

    Returns:
        (connection_url, connection_info) å…ƒç»„
        - connection_url: è¿æ¥å­—ç¬¦ä¸²ï¼ˆæ•°æ®åº“ç”¨ URLï¼ŒExcel ç”¨ "excel://" å‰ç¼€ï¼‰
        - connection_info: æ•°æ®æºè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœä»æ•°æ®åº“è·å–ï¼‰
    """
    # ä»è¿æ¥ä¸Šä¸‹æ–‡è·å–æ•°æ®åº“ä¼šè¯å’Œç§Ÿæˆ· ID
    _, db_session, tenant_id = _get_connection_context()

    # å¦‚æœæ²¡æœ‰æä¾› connection_idï¼Œå°è¯•è·å–ç§Ÿæˆ·çš„é»˜è®¤æ´»è·ƒæ•°æ®æº
    if not connection_id:
        if db_session and tenant_id:
            try:
                # Docker ç¯å¢ƒè·¯å¾„è®¾ç½®ï¼ˆç¡®ä¿ /app/src åœ¨ sys.path å¼€å¤´ï¼‰
                import sys
                if "/app/src" in sys.path:
                    sys.path.remove("/app/src")
                sys.path.insert(0, "/app/src")
                logger.debug("Ensured /app/src is at sys.path[0] for default data source query")

                from app.data.models import DataSourceConnection

                # æŸ¥è¯¢ç§Ÿæˆ·çš„ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                connection = db_session.query(DataSourceConnection).filter(
                    DataSourceConnection.tenant_id == tenant_id,
                    DataSourceConnection.status == "active"
                ).first()

                if connection:
                    connection_id = str(connection.id)
                    logger.info(f"è‡ªåŠ¨è·å–ç§Ÿæˆ· {tenant_id} çš„é»˜è®¤æ•°æ®æº: {connection.name} (ID: {connection_id})")
                else:
                    logger.warning(f"ç§Ÿæˆ· {tenant_id} æ²¡æœ‰é…ç½®æ´»è·ƒæ•°æ®æºï¼Œä½¿ç”¨ç³»ç»Ÿå…ƒæ•°æ®åº“")
                    return os.environ.get("DATABASE_URL"), None
            except ImportError as e:
                logger.error(f"Failed to import DataSourceConnection model: {e}")
                return os.environ.get("DATABASE_URL"), None
            except Exception as e:
                logger.error(f"Failed to query default data source: {e}")
                return os.environ.get("DATABASE_URL"), None
        else:
            # æ²¡æœ‰æ•°æ®åº“ä¼šè¯ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤æ•°æ®åº“
            return os.environ.get("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/data_agent"), None

    # ä»è¿™é‡Œå¼€å§‹æœ‰ connection_id
    if not db_session or not tenant_id:
        logger.warning(
            f"connection_id provided but db_session/tenant_id not available. "
            f"Using default database. connection_id={connection_id}"
        )
        return os.environ.get("DATABASE_URL"), None

    try:
        # å¯¼å…¥æ•°æ®æºæœåŠ¡ï¼ˆDocker ç¯å¢ƒä½¿ç”¨ /app è·¯å¾„ï¼‰
        import sys

        # è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        logger.info(f"BEFORE: /app/src in sys.path = {('/app/src' in sys.path)}")
        logger.info(f"BEFORE: os.path.exists('/app/src') = {os.path.exists('/app/src')}")
        logger.info(f"BEFORE: os.path.exists('/app/src/app/services') = {os.path.exists('/app/src/app/services')}")
        logger.info(f"BEFORE: sys.path[0] = {sys.path[0]}")

        # ç¡®ä¿ /app/src åœ¨ sys.path çš„æœ€å‰é¢ï¼ˆå› ä¸º app åŒ…åœ¨ /app/src/app/...ï¼‰
        # Python å¯¼å…¥ app.services æ—¶ï¼Œä¼šåœ¨ sys.path ä¸­æŸ¥æ‰¾ app/ ç›®å½•
        if "/app/src" in sys.path:
            sys.path.remove("/app/src")
        sys.path.insert(0, "/app/src")
        logger.info("ACTION: Ensured /app/src is at sys.path[0]")

        logger.info(f"AFTER: sys.path[0] = {sys.path[0]}")

        # æ£€æŸ¥ /app/src/app ç›®å½•å†…å®¹
        if os.path.exists("/app/src/app"):
            all_files = os.listdir("/app/src/app")
            logger.info(f"Files in /app/src/app (total {len(all_files)}): {all_files}")
            # æ£€æŸ¥ services æ˜¯å¦å­˜åœ¨
            if "services" in all_files:
                logger.info("âœ“ services directory found in /app/src/app")
                # æ£€æŸ¥ __init__.py æ–‡ä»¶
                init_files = []
                for pyc_init in ["/app/src/app/__init__.py", "/app/src/app/services/__init__.py"]:
                    if os.path.exists(pyc_init):
                        init_files.append(pyc_init + " âœ“")
                    else:
                        init_files.append(pyc_init + " âœ—")
                logger.info(f"__init__.py files: {init_files}")
            else:
                logger.error("âœ— services directory NOT found in /app/src/app")
        else:
            logger.error(f"Directory /app/src/app does not exist!")

        # æ£€æŸ¥æ˜¯å¦èƒ½ç›´æ¥è®¿é—® services ç›®å½•
        if os.path.exists("/app/src/app/services/data_source_service.py"):
            logger.info("âœ“ data_source_service.py file exists")
        else:
            logger.error("âœ— data_source_service.py file NOT found at /app/src/app/services/")

        # æ¸…é™¤å¯èƒ½çš„é™ˆæ—§å¯¼å…¥ç¼“å­˜ï¼ˆåŒ…æ‹¬ app æ¨¡å—æœ¬èº«ï¼‰
        stale_keys = [k for k in sys.modules.keys() if k.startswith("app.")]
        if stale_keys:
            logger.info(f"Removing stale imports from sys.modules: {stale_keys}")
            for key in stale_keys:
                del sys.modules[key]
        # ç¡®ä¿åˆ é™¤ app æ¨¡å—æœ¬èº«ï¼ˆå¯èƒ½æ˜¯å‘½åç©ºé—´åŒ…ï¼‰
        if "app" in sys.modules:
            logger.info(f"Removing 'app' module from sys.modules (was: {type(sys.modules['app'])})")
            del sys.modules["app"]

        # å°è¯•å¯¼å…¥
        logger.info("Attempting to import app.services.data_source_service...")
        try:
            from app.services.data_source_service import data_source_service
            from app.services.data_source_service import DataSourceConnectionInfo
            logger.info("Successfully imported data_source_service")
        except ImportError as e:
            # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            logger.error(f"ImportError: {e}")
            logger.error(f"sys.path[0:3] = {sys.path[0:3]}")
            # æ£€æŸ¥ app æ¨¡å—æ˜¯å¦åœ¨ sys.modules ä¸­
            if "app" in sys.modules:
                logger.error(f"app module already in sys.modules: {sys.modules['app']}")
            if "app.services" in sys.modules:
                logger.error(f"app.services already in sys.modules: {sys.modules['app.services']}")
            raise

        # åŒæ­¥åŒ…è£…ï¼šå› ä¸º data_source_service æ˜¯å¼‚æ­¥çš„
        import asyncio

        def get_connection_info():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                logger.info(f"Calling data_source_service.get_data_source_connection_info(connection_id={connection_id}, tenant_id={tenant_id})")
                result = loop.run_until_complete(
                    data_source_service.get_data_source_connection_info(
                        connection_id=connection_id,
                        tenant_id=tenant_id,
                        db=db_session
                    )
                )
                logger.info(f"data_source_service returned: {result}")
                return result
            except Exception as e:
                logger.error(f"Error in get_connection_info: {e}", exc_info=True)
                raise
            finally:
                loop.close()

        connection_info = get_connection_info()
        logger.info(f"Retrieved connection_info: type={connection_info.connection_type}, file_path={connection_info.file_path}")

        # æ ¹æ®æ•°æ®æºç±»å‹è¿”å›ä¸åŒçš„è¿æ¥ä¿¡æ¯
        if connection_info.connection_type == "excel":
            # Excel æ–‡ä»¶ï¼šè¿”å›ç‰¹æ®Šæ ‡è®°å’Œæ–‡ä»¶ä¿¡æ¯
            logger.info(f"Using Excel data source: {connection_info.file_path}, sheets={connection_info.sheets}")
            return f"excel://{connection_info.file_path}", connection_info
        else:
            # æ•°æ®åº“ï¼šè¿”å›è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
            logger.info(f"Using database data source: type={connection_info.connection_type}")
            return connection_info.connection_string, connection_info

    except ImportError as e:
        logger.error(f"Failed to import data_source_service: {e}")
        return os.environ.get("DATABASE_URL"), None
    except Exception as e:
        logger.error(f"Failed to get connection info for {connection_id}: {e}")
        return os.environ.get("DATABASE_URL"), None


def _is_excel_connection(database_url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ Excel è¿æ¥"""
    return database_url.startswith("excel://")


def _is_sqlite_connection(database_url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ SQLite è¿æ¥"""
    return database_url.startswith("sqlite:///")


def create_db_connection(database_url: str):
    """
    åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆæ”¯æŒ PostgreSQL å’Œ SQLiteï¼‰

    Args:
        database_url: æ•°æ®åº“è¿æ¥ URL

    Returns:
        æ•°æ®åº“è¿æ¥å¯¹è±¡
    """
    if _is_sqlite_connection(database_url):
        import sqlite3
        # SQLite è¿æ¥æ ¼å¼: sqlite:///path/to/database.db
        db_path = database_url.replace("sqlite:///", "")
        logger.info(f"Creating SQLite connection: {db_path}")
        return sqlite3.connect(db_path)
    else:
        # PostgreSQL è¿æ¥
        import psycopg2
        logger.info(f"Creating PostgreSQL connection")
        return psycopg2.connect(database_url)


def _get_excel_file_path(database_url: str) -> str:
    """ä» Excel è¿æ¥ URL ä¸­æå–æ–‡ä»¶è·¯å¾„"""
    return database_url[8:]  # å»æ‰ "excel://" å‰ç¼€


# ============================================================================
# Excel æŸ¥è¯¢å·¥å…·
# ============================================================================

def execute_excel_query(
    query: str,
    file_path: str,
    sheet_name: Optional[str] = None
) -> str:
    """
    æ‰§è¡Œ Excel æ–‡ä»¶æŸ¥è¯¢ï¼ˆä½¿ç”¨ pandasï¼‰

    Args:
        query: ç±»ä¼¼ SQL çš„æŸ¥è¯¢æˆ– pandas ä»£ç 
        file_path: Excel æ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        æŸ¥è¯¢ç»“æœçš„ JSON å­—ç¬¦ä¸²
    """
    import json
    import pandas as pd

    try:
        # è¯»å– Excel æ–‡ä»¶
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')
        else:
            # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, engine='openpyxl')
            sheet_name = "Sheet1"

        logger.info(f"Excel file loaded: {file_path}, sheet: {sheet_name}, shape: {df.shape}")

        # ç®€å•çš„ SQL è§£æå’Œè½¬æ¢
        result_df = _parse_sql_to_pandas(query, df)

        # è½¬æ¢ä¸ºç»“æœæ ¼å¼
        columns = result_df.columns.tolist()
        rows = result_df.values.tolist()

        # å¤„ç† NaN å€¼
        rows = [[None if pd.isna(v) else v for v in row] for row in rows]

        result = {
            "columns": columns,
            "rows": rows,
            "row_count": len(rows),
            "success": True,
            "data_source": "excel",
            "sheet_name": sheet_name
        }

        logger.info(f"Excel query executed: {len(rows)} rows returned")
        return json.dumps(result, ensure_ascii=False, default=str)

    except FileNotFoundError:
        logger.error(f"Excel file not found: {file_path}")
        return json.dumps({
            "error": f"Excel file not found: {file_path}",
            "error_type": "file_not_found"
        }, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Excel query error: {e}")
        return json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)


def _parse_sql_to_pandas(query: str, df: "Any") -> "Any":
    """
    ç®€å•çš„ SQL åˆ° pandas è½¬æ¢

    æ”¯æŒçš„åŸºæœ¬ SQL è¯­æ³•:
    - SELECT col1, col2 FROM table
    - SELECT * FROM table
    - SELECT ... FROM table WHERE col = value
    - SELECT ... FROM table WHERE col LIKE value
    - SELECT ... FROM table ORDER BY col
    - SELECT ... FROM table LIMIT n

    Args:
        query: SQL æŸ¥è¯¢è¯­å¥
        df: pandas DataFrame

    Returns:
        è¿‡æ»¤åçš„ DataFrame
    """
    import re

    query_upper = query.upper().strip()
    result_df = df.copy()

    # æå– SELECT åˆ—
    select_match = re.search(r'SELECT\s+(.+?)\s+FROM', query_upper)
    if select_match:
        columns_str = select_match.group(1).strip()
        if columns_str != '*':
            # è§£æåˆ—åï¼ˆå¤„ç†é€—å·åˆ†éš”çš„åˆ—ï¼‰
            columns = [col.strip().lower() for col in columns_str.split(',')]
            # åŒ¹é…å®é™…åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            actual_columns = {col.lower(): col for col in df.columns}
            selected_columns = []
            for col in columns:
                if col in actual_columns:
                    selected_columns.append(actual_columns[col])
            if selected_columns:
                result_df = result_df[selected_columns]

    # æå– WHERE æ¡ä»¶
    where_match = re.search(r'WHERE\s+(.+?)(?:\s+ORDER\s+BY|\s+LIMIT|;|$)', query_upper)
    if where_match:
        where_clause = where_match.group(1).strip()
        result_df = _apply_where_clause(result_df, where_clause)

    # æå– ORDER BY
    order_match = re.search(r'ORDER\s+BY\s+(\w+)(?:\s+(ASC|DESC))?', query_upper)
    if order_match:
        col_name = order_match.group(1).lower()
        direction = order_match.group(2) if order_match.group(2) else 'ASC'

        # åŒ¹é…å®é™…åˆ—å
        actual_columns = {col.lower(): col for col in result_df.columns}
        if col_name in actual_columns:
            actual_col = actual_columns[col_name]
            ascending = direction == 'ASC'
            result_df = result_df.sort_values(by=actual_col, ascending=ascending)

    # æå– LIMIT
    limit_match = re.search(r'LIMIT\s+(\d+)', query_upper)
    if limit_match:
        limit = int(limit_match.group(1))
        result_df = result_df.head(limit)

    return result_df


def _apply_where_clause(df: "Any", where_clause: str) -> "Any":
    """
    åº”ç”¨ WHERE æ¡ä»¶åˆ° DataFrame

    æ”¯æŒçš„æ¡ä»¶:
    - col = value
    - col LIKE value
    - col > value, col < value, col >= value, col <= value
    - col AND col
    - col OR col

    Args:
        df: pandas DataFrame
        where_clause: WHERE å­å¥

    Returns:
        è¿‡æ»¤åçš„ DataFrame
    """
    import re

    result_df = df.copy()

    # åˆ†å‰² AND/OR æ¡ä»¶
    conditions = re.split(r'\s+(AND|OR)\s+', where_clause, flags=re.IGNORECASE)
    current_op = 'AND'

    for i, cond in enumerate(conditions):
        cond = cond.strip()
        if cond.upper() in ('AND', 'OR'):
            current_op = cond.upper()
            continue

        # è§£ææ¡ä»¶ - ğŸ”§ ä¿®å¤ï¼šå°†æ›´é•¿çš„æ“ä½œç¬¦æ”¾åœ¨å‰é¢
        match = re.match(
            r'(\w+)\s*(>=|<=|<>|=|LIKE|>|<)\s*[\'"]?([^\'"]+)[\'"]?',
            cond,
            re.IGNORECASE
        )
        if match:
            col_name = match.group(1).lower()
            operator = match.group(2).upper()
            value = match.group(3).strip()

            # åŒ¹é…å®é™…åˆ—å
            actual_columns = {col.lower(): col for col in result_df.columns}
            if col_name not in actual_columns:
                continue

            actual_col = actual_columns[col_name]

            # åº”ç”¨æ¡ä»¶
            mask = None
            if operator == '=':
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] == value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) == value
            elif operator == 'LIKE':
                mask = result_df[actual_col].astype(str).str.contains(
                    value.replace('%', ''),
                    case=False,
                    na=False
                )
            elif operator == '>':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] > value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) > value
            elif operator == '<':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] < value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) < value
            elif operator == '>=':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] >= value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) >= value
            elif operator == '<=':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] <= value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) <= value
            elif operator == '<>':
                # ä¸ç­‰äº
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] != value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) != value

            if mask is not None:
                if i == 0 or current_op == 'AND':
                    result_df = result_df[mask]
                else:  # OR
                    result_df = pd.concat([result_df, df[mask]]).drop_duplicates()

    return result_df


# ============================================================================
# æ•°æ®åº“æŸ¥è¯¢å·¥å…·
# ============================================================================

# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def execute_query(query: str, connection_id: Optional[str] = None) -> str:
    """
    æ‰§è¡Œæ•°æ®æŸ¥è¯¢ (æ”¯æŒæ•°æ®åº“å’Œ Excel æ–‡ä»¶)

    è¿™ä¸ªå·¥å…·ç”¨äºæ‰§è¡Œåªè¯»çš„æ•°æ®æŸ¥è¯¢ï¼Œè·å–æ•°æ®ã€‚
    è‡ªåŠ¨æ£€æµ‹æ•°æ®æºç±»å‹å¹¶ä½¿ç”¨ç›¸åº”çš„æŸ¥è¯¢æ–¹æ³•ã€‚

    Args:
        query: SQL SELECT æŸ¥è¯¢è¯­å¥ï¼ˆæˆ–ç”¨äº Excel çš„ç±» SQL æŸ¥è¯¢ï¼‰
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        æŸ¥è¯¢ç»“æœçš„ JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«åˆ—ä¿¡æ¯å’Œè¡Œæ•°æ®

    Example:
        >>> execute_query("SELECT * FROM users LIMIT 10")
        '{"columns": ["id", "name"], "rows": [[1, "Alice"], [2, "Bob"]], "row_count": 2}'
    """
    import json
    import re
    import threading

    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ SELECT æŸ¥è¯¢
    query_upper = query.upper().strip()

    # æ£€æŸ¥å±é™©å…³é”®å­—
    dangerous_keywords = [
        "DROP", "DELETE", "INSERT", "UPDATE", "TRUNCATE",
        "ALTER", "CREATE", "GRANT", "REVOKE"
    ]

    for keyword in dangerous_keywords:
        if re.search(rf"\b{keyword}\b", query_upper):
            return json.dumps({
                "error": f"Security Alert: {keyword} operations are not allowed",
                "error_type": "forbidden_operation"
            }, ensure_ascii=False)

    # æ£€æŸ¥æ˜¯å¦ä»¥ SELECT æˆ–å…¶ä»–å…è®¸çš„å…³é”®å­—å¼€å¤´
    allowed_starts = ["SELECT", "WITH", "SHOW", "EXPLAIN", "DESCRIBE", "DESC"]
    if not any(query_upper.startswith(start) for start in allowed_starts):
        return json.dumps({
            "error": "Query must start with SELECT, WITH, SHOW, EXPLAIN, or DESCRIBE",
            "error_type": "invalid_query_type"
        }, ensure_ascii=False)

    # æ¸…ç†å’Œä¿®å¤ SQL
    cleaned_query = clean_and_validate_sql(query)
    if cleaned_query != query:
        logger.info(f"SQL cleaned: {query[:50]}... -> {cleaned_query[:50]}...")

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    # Agent è°ƒç”¨å·¥å…·æ—¶ä¸ä¼šä¼ é€’ connection_idï¼Œéœ€è¦ä»è¿æ¥ä¸Šä¸‹æ–‡è·å–
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    # æ£€æŸ¥æŸ¥è¯¢ç»“æœç¼“å­˜ (ä½¿ç”¨æ ‡å‡†åŒ–çš„ SQL ä½œä¸ºç¼“å­˜é”®)
    cache_key = _make_cache_key(cleaned_query, connection_id)
    cached_result = _query_cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"Query result cache HIT: {cleaned_query[:50]}...")
        return cached_result
    else:
        logger.info(f"Query result cache MISS: {cleaned_query[:50]}...")

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)
    logger.info(f"Using connection: connection_id={connection_id}, url_type={'excel' if _is_excel_connection(database_url) else 'database'}")

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œä½¿ç”¨ Excel æŸ¥è¯¢
    if _is_excel_connection(database_url):
        logger.info(f"Detected Excel data source, using Excel query")
        file_path = _get_excel_file_path(database_url)
        
        # ğŸ”¥ ä¿®å¤ï¼šä» SQL æŸ¥è¯¢ä¸­è§£æè¡¨åï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„ table_name
        # å°è¯•ä» SQL ä¸­æå–è¡¨å
        extracted_table_name = _extract_table_name_from_query(cleaned_query)
        
        # å¦‚æœæˆåŠŸæå–è¡¨åï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™å›é€€åˆ° connection_info.table_name
        if extracted_table_name:
            sheet_name = extracted_table_name
            logger.info(f"Using extracted table name from SQL: '{sheet_name}'")
        else:
            sheet_name = connection_info.table_name if connection_info else None
            logger.warning(f"Could not extract table name from SQL, using default: '{sheet_name}'")

        result = execute_excel_query(cleaned_query, file_path, sheet_name)

        # å­˜å‚¨åˆ°ç¼“å­˜
        _query_cache.set(cache_key, result)
        return result

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    # æŸ¥è¯¢ç»“æœå’Œé”™è¯¯å®¹å™¨
    result_container = {"result": None, "error": None}

    def execute_with_timeout():
        """åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­æ‰§è¡ŒæŸ¥è¯¢"""
        try:
            # åˆ›å»ºæ•°æ®åº“è¿æ¥
            conn = create_db_connection(database_url)
            cursor = conn.cursor()

            # è®¾ç½®è¯­å¥è¶…æ—¶ï¼ˆPostgreSQL statement_timeoutï¼‰
            # SQLite ä¸æ”¯æŒæ­¤è¯­å¥ï¼Œéœ€è¦è·³è¿‡
            if not _is_sqlite_connection(database_url):
                cursor.execute("SET statement_timeout = 30000")  # 30ç§’

            # æ‰§è¡ŒæŸ¥è¯¢
            cursor.execute(cleaned_query)

            # è·å–ç»“æœ
            columns = [desc[0] for desc in cursor.description] if cursor.description else []
            rows = cursor.fetchall()

            # å…³é—­è¿æ¥
            cursor.close()
            conn.close()

            # æ„å»ºç»“æœ
            result_container["result"] = {
                "columns": columns,
                "rows": rows,
                "row_count": len(rows),
                "success": True
            }

            logger.info(f"Query executed successfully: {len(rows)} rows returned")

        except Exception as e:
            result_container["error"] = e

    # ä½¿ç”¨çº¿ç¨‹æ‰§è¡ŒæŸ¥è¯¢ï¼Œ30ç§’è¶…æ—¶
    query_thread = threading.Thread(target=execute_with_timeout)
    query_thread.daemon = True
    query_thread.start()
    query_thread.join(timeout=30)

    # æ£€æŸ¥ç»“æœ
    if query_thread.is_alive():
        # æŸ¥è¯¢è¶…æ—¶
        logger.error("Query execution timeout (30s)")
        return json.dumps({
            "error": "Query execution timeout after 30 seconds",
            "error_type": "timeout_error",
            "query": cleaned_query[:100]
        }, ensure_ascii=False)

    if result_container["error"] is not None:
        error = result_container["error"]
        logger.error(f"Query execution error: {error}")
        logger.error(f"Failed query: {cleaned_query[:200]}")
        return json.dumps({
            "error": str(error),
            "error_type": "execution_error",
            "query": cleaned_query[:100],  # æˆªæ–­æŸ¥è¯¢
            "suggestion": get_query_suggestion(str(error), cleaned_query)
        }, ensure_ascii=False)

    if result_container["result"] is not None:
        result_json = json.dumps(result_container["result"], ensure_ascii=False, default=str)

        # å­˜å‚¨åˆ°ç¼“å­˜ (ç¼“å­˜ 5 åˆ†é’Ÿ)
        _query_cache.set(cache_key, result_json)
        logger.info(f"Query result cached: {cleaned_query[:50]}...")

        return result_json

    # æœªçŸ¥çš„é”™è¯¯
    return json.dumps({
        "error": "Unknown error during query execution",
        "error_type": "unknown_error"
    }, ensure_ascii=False)


def clean_and_validate_sql(query: str) -> str:
    """
    æ¸…ç†å’ŒéªŒè¯ SQL æŸ¥è¯¢

    ä¿®å¤å¸¸è§çš„ LLM ç”Ÿæˆé”™è¯¯ï¼š
    - LIMIT å­å¥åçš„é”™è¯¯å†…å®¹ï¼ˆWHERE, AND, OR ç­‰ï¼‰
    - tenants è¡¨çš„ tenant_id åˆ—é”™è¯¯ï¼ˆåº”ä½¿ç”¨ idï¼‰
    - å¤šä½™çš„åˆ†å·
    - ä¸å®Œæ•´çš„æŸ¥è¯¢

    Args:
        query: åŸå§‹ SQL æŸ¥è¯¢

    Returns:
        æ¸…ç†åçš„ SQL æŸ¥è¯¢
    """
    import re

    # ç§»é™¤å‰åç©ºæ ¼
    sql = query.strip()

    # ä¿®å¤ 0: tenants è¡¨çš„ tenant_id â†’ id è‡ªåŠ¨æ›¿æ¢ï¼ˆå¸¸è§é”™è¯¯ï¼‰
    # åŒ¹é… FROM tenants ... WHERE tenant_id æˆ– JOIN tenants ... WHERE tenant_id
    if re.search(r'\btenants\b', sql, re.IGNORECASE):
        # æ›¿æ¢ tenants è¡¨ä¸Šçš„ tenant_id ä¸º id
        # åŒ¹é… WHERE tenant_id = æˆ– AND tenant_id = ç­‰
        sql = re.sub(
            r'(WHERE|AND|OR)\s+tenant_id\s*=',
            r'\1 id =',
            sql,
            flags=re.IGNORECASE
        )
        # å¦‚æœä¿®æ”¹äº† SQLï¼Œè®°å½•æ—¥å¿—
        if 'tenant_id' in query.lower() and 'tenant_id' not in sql.lower():
            logger.info("Auto-fixed: tenants.tenant_id â†’ tenants.id")

    # ä¿®å¤ 1: ç§»é™¤ LIMIT åé¢çš„ä»»ä½•å†…å®¹ï¼ˆLLM å¸¸è§é”™è¯¯ï¼‰
    # åŒ¹é… LIMIT å­å¥ï¼Œç„¶åæˆªæ–­ï¼Œç§»é™¤åé¢çš„ WHERE, AND, OR ç­‰
    # è¿™ä¸ªæ­£åˆ™åŒ¹é… LIMIT æ•°å­—ï¼Œç„¶ååé¢ä¸èƒ½æœ‰ WHERE/AND/OR/GROUP/ORDER/HAVING
    limit_pattern = r'\bLIMIT\s+(\d+)'
    match = re.search(limit_pattern, sql, re.IGNORECASE)
    if match:
        # æ‰¾åˆ° LIMIT å­å¥ï¼Œæˆªæ–­åˆ°æ•°å­—ç»“æŸçš„åœ°æ–¹
        # éœ€è¦æ‰¾åˆ° LIMIT åé¢çš„æ•°å­—
        limit_end = match.end()
        # æ£€æŸ¥ LIMIT åé¢æ˜¯å¦è¿˜æœ‰å…¶ä»–å­å¥ï¼ˆWHERE, AND, OR, GROUP BY, HAVING ç­‰ï¼‰
        # å¦‚æœæœ‰ï¼Œæˆªæ–­å®ƒä»¬
        remaining_sql = sql[limit_end:].strip()
        # å¦‚æœå‰©ä½™å†…å®¹ä»¥ WHERE, AND, OR, GROUP, HAVING å¼€å¤´ï¼Œè¯´æ˜æ˜¯é”™è¯¯çš„
        if re.match(r'^(WHERE|AND|OR|GROUP BY|HAVING)', remaining_sql, re.IGNORECASE):
            # æˆªæ–­åˆ° LIMIT æ•°å­—ç»“æŸçš„åœ°æ–¹
            sql = sql[:limit_end].rstrip()
            logger.info(f"Removed content after LIMIT: {remaining_sql[:50]}...")

    # ä¿®å¤ 2: ç§»é™¤æœ«å°¾çš„åˆ†å·ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
    sql = re.sub(r';+$', '', sql)

    # ä¿®å¤ 3: ç¡®ä¿æŸ¥è¯¢ä»¥åˆ†å·ç»“å°¾ï¼ˆå¯¹äºå•æ¡æŸ¥è¯¢ï¼‰
    if not sql.endswith(';'):
        sql += ';'

    # ä¿®å¤ 4: ç§»é™¤æ³¨é‡Šåçš„å±é™©å‘½ä»¤ï¼ˆé¢å¤–å®‰å…¨æ£€æŸ¥ï¼‰
    # ç§»é™¤ -- åé¢çš„å†…å®¹åˆ°è¡Œå°¾
    sql = re.sub(r'--.*$', '', sql, flags=re.MULTILINE)
    # ç§»é™¤ /* */ å—æ³¨é‡Š
    sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)

    # ä¿®å¤ 5: æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    sql = ' '.join(sql.split())

    return sql


def _extract_table_name_from_query(query: str) -> Optional[str]:
    """
    ä» SQL æŸ¥è¯¢ä¸­æå– FROM å­å¥çš„è¡¨å

    æ”¯æŒç®€å•çš„ SELECT æŸ¥è¯¢ï¼Œæå–ç¬¬ä¸€ä¸ª FROM åé¢çš„è¡¨åã€‚
    æ”¯æŒå¸¦emojiã€ä¸­æ–‡ã€ç‰¹æ®Šå­—ç¬¦çš„è¡¨åã€‚

    Args:
        query: SQL æŸ¥è¯¢è¯­å¥

    Returns:
        æå–çš„è¡¨åï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› None
    """
    import re

    try:
        # ç§»é™¤æ³¨é‡Šå’Œæ¢è¡Œç¬¦ï¼Œç®€åŒ–è§£æ
        cleaned = re.sub(r'--.*$', '', query, flags=re.MULTILINE)
        cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)
        cleaned = ' '.join(cleaned.split())

        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆåŒ¹é…åŒå¼•å·å†…çš„è¡¨åï¼ˆæ”¯æŒemojiã€ä¸­æ–‡ã€ç©ºæ ¼ï¼‰
        # æ¨¡å¼ï¼šFROM "table name" æˆ– FROM "è¡¨å"
        double_quote_pattern = r'\bFROM\s+"([^"]+)"'
        match = re.search(double_quote_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (double-quoted) from query: '{table_name}'")
            return table_name

        # ğŸ”§ åŒ¹é…å•å¼•å·å†…çš„è¡¨å
        single_quote_pattern = r"\bFROM\s+'([^']+)'"
        match = re.search(single_quote_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (single-quoted) from query: '{table_name}'")
            return table_name

        # ğŸ”§ æœ€åï¼šåŒ¹é…ä¸å¸¦å¼•å·çš„è¡¨åï¼ˆæ”¯æŒemojiå’Œä¸­æ–‡ï¼Œä½†ä¸èƒ½æœ‰ç©ºæ ¼ï¼‰
        # ä½¿ç”¨ Unicode å±æ€§åŒ¹é…ä»»ä½•å•è¯å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€emojiç­‰ï¼‰
        unquoted_pattern = r'\bFROM\s+([^\s;]+?)(?:\s+WHERE|\s+ORDER|\s+GROUP|\s+LIMIT|\s+HAVING|;|$)'
        match = re.search(unquoted_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (unquoted) from query: '{table_name}'")
            return table_name

        logger.warning(f"Could not extract table name from query: {query[:100]}...")
        return None
    except Exception as e:
        logger.warning(f"Error extracting table name: {e}")
        return None


def get_query_suggestion(error_msg: str, query: str) -> Optional[str]:
    """
    æ ¹æ®é”™è¯¯ä¿¡æ¯æä¾›æŸ¥è¯¢å»ºè®®

    Args:
        error_msg: é”™è¯¯ä¿¡æ¯
        query: åŸå§‹æŸ¥è¯¢

    Returns:
        å»ºè®®ä¿¡æ¯
    """
    import re

    error_lower = error_msg.lower()

    if 'limit' in error_lower and 'and' in error_lower:
        return "LIMIT å­å¥åä¸åº”æœ‰ AND æ¡ä»¶ã€‚è¯·å°† LIMIT æ”¾åœ¨æŸ¥è¯¢æœ€åã€‚"

    if 'column' in error_lower and 'does not exist' in error_lower:
        # æå–ä¸å­˜åœ¨çš„åˆ—å
        match = re.search(r'column "(.*?)" does not exist', error_msg)
        if match:
            col = match.group(1)
            return f"åˆ— '{col}' ä¸å­˜åœ¨ã€‚è¯·ä½¿ç”¨ list_tables å’Œ get_schema æŸ¥çœ‹å¯ç”¨çš„è¡¨å’Œåˆ—ã€‚"

    if 'relation' in error_lower and 'does not exist' in error_lower:
        match = re.search(r'relation "(.*?)" does not exist', error_msg)
        if match:
            table = match.group(1)
            return f"è¡¨ '{table}' ä¸å­˜åœ¨ã€‚è¯·ä½¿ç”¨ list_tables æŸ¥çœ‹å¯ç”¨çš„è¡¨ã€‚"

    if 'syntax error' in error_lower:
        return "SQL è¯­æ³•é”™è¯¯ã€‚è¯·ç¡®ä¿æŸ¥è¯¢æ ¼å¼æ­£ç¡®ï¼Œå»ºè®®ä½¿ç”¨ç®€å•çš„ SELECT è¯­å¥ã€‚"

    return None


# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def list_tables(connection_id: Optional[str] = None) -> str:
    """
    åˆ—å‡ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨æˆ– Excel æ–‡ä»¶ä¸­çš„æ‰€æœ‰å·¥ä½œè¡¨

    Args:
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        è¡¨åˆ—è¡¨çš„ JSON å­—ç¬¦ä¸²

    Example:
        >>> list_tables()
        '{"tables": ["users", "orders", "products"], "table_count": 3}'
    """
    import json

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    logger.info(f"list_tables: connection_id={connection_id}")

    # æ¸…é™¤æ—§ç¼“å­˜ä»¥ç¡®ä¿ä½¿ç”¨æ–°çš„è¿æ¥ä¿¡æ¯
    if connection_id:
        _schema_cache.clear()

    # æ£€æŸ¥ç¼“å­˜
    cache_key = _make_cache_key("list_tables", connection_id)
    cached = _schema_cache.get(cache_key)
    if cached is not None:
        logger.info("list_tables: è¿”å›ç¼“å­˜ç»“æœ")
        return cached

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œè¿”å›å·¥ä½œè¡¨åˆ—è¡¨
    if _is_excel_connection(database_url):
        logger.info("list_tables: Detected Excel data source, listing sheets")
        try:
            file_path = _get_excel_file_path(database_url)
            import pandas as pd

            excel_file = pd.ExcelFile(file_path, engine='openpyxl')
            sheets = excel_file.sheet_names

            result = {
                "tables": sheets,
                "table_count": len(sheets),
                "success": True,
                "data_source": "excel",
                "file_path": file_path
            }

            result_str = json.dumps(result, ensure_ascii=False)
            _schema_cache.set(cache_key, result_str)
            logger.info(f"list_tables: Excel æ–‡ä»¶ï¼Œ{len(sheets)} ä¸ªå·¥ä½œè¡¨: {sheets}")

            return result_str

        except Exception as e:
            logger.error(f"List Excel sheets error: {e}")
            error_str = json.dumps({
                "error": str(e),
                "error_type": "execution_error"
            }, ensure_ascii=False)
            return error_str

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        conn = create_db_connection(database_url)
        cursor = conn.cursor()

        # PostgreSQL æŸ¥è¯¢æ‰€æœ‰è¡¨
        cursor.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_type = 'BASE TABLE'
            ORDER BY table_name
        """)

        tables = [row[0] for row in cursor.fetchall()]

        cursor.close()
        conn.close()

        result = {
            "tables": tables,
            "table_count": len(tables),
            "success": True
        }

        result_str = json.dumps(result, ensure_ascii=False)
        # å­˜å…¥ç¼“å­˜
        _schema_cache.set(cache_key, result_str)
        logger.info(f"list_tables: æŸ¥è¯¢æˆåŠŸï¼Œç¼“å­˜ç»“æœ ({len(tables)} ä¸ªè¡¨)")

        return result_str

    except Exception as e:
        logger.error(f"List tables error: {e}")
        error_str = json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)
        return error_str


# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def get_schema(table_name: str, connection_id: Optional[str] = None) -> str:
    """
    è·å–è¡¨çš„ç»“æ„ä¿¡æ¯æˆ– Excel å·¥ä½œè¡¨çš„åˆ—ä¿¡æ¯

    Args:
        table_name: è¡¨åæˆ–å·¥ä½œè¡¨å
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        è¡¨ç»“æ„çš„ JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«åˆ—åã€ç±»å‹ã€æ˜¯å¦å¯ç©ºç­‰ä¿¡æ¯

    Example:
        >>> get_schema("users")
        '{"table_name": "users", "columns": [{"name": "id", "type": "integer", "nullable": false}], "column_count": 1}'
    """
    import json

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    # æ£€æŸ¥ç¼“å­˜
    cache_key = _make_cache_key("get_schema", table_name, connection_id)
    cached = _schema_cache.get(cache_key)
    if cached is not None:
        logger.info(f"get_schema({table_name}): è¿”å›ç¼“å­˜ç»“æœ")
        return cached

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œè¿”å›åˆ—ä¿¡æ¯
    if _is_excel_connection(database_url):
        logger.info(f"get_schema({table_name}): Detected Excel data source, getting columns")
        try:
            file_path = _get_excel_file_path(database_url)
            import pandas as pd

            # è¯»å– Excel å·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=table_name, engine='openpyxl', nrows=0)

            # è·å–åˆ—ä¿¡æ¯
            columns = []
            for col in df.columns:
                # æ¨æ–­æ•°æ®ç±»å‹
                dtype_str = str(df[col].dtype)
                # ç®€åŒ–ç±»å‹åç§°
                if dtype_str.startswith('int'):
                    col_type = 'integer'
                elif dtype_str.startswith('float'):
                    col_type = 'float'
                elif dtype_str == 'bool':
                    col_type = 'boolean'
                elif dtype_str.startswith('datetime'):
                    col_type = 'datetime'
                else:
                    col_type = 'text'

                columns.append({
                    "name": col,
                    "type": col_type,
                    "nullable": True  # Excel ä¸å¼ºåˆ¶ NOT NULL
                })

            result = {
                "table_name": table_name,
                "columns": columns,
                "column_count": len(columns),
                "success": True,
                "data_source": "excel"
            }

            result_str = json.dumps(result, ensure_ascii=False)
            _schema_cache.set(cache_key, result_str)
            logger.info(f"get_schema({table_name}): Excel å·¥ä½œè¡¨ï¼Œ{len(columns)} åˆ—")

            return result_str

        except Exception as e:
            logger.error(f"Get Excel schema error: {e}")
            error_str = json.dumps({
                "error": str(e),
                "error_type": "execution_error"
            }, ensure_ascii=False)
            return error_str

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        conn = create_db_connection(database_url)
        cursor = conn.cursor()

        # PostgreSQL æŸ¥è¯¢è¡¨ç»“æ„
        cursor.execute("""
            SELECT
                column_name,
                data_type,
                is_nullable,
                column_default
            FROM information_schema.columns
            WHERE table_name = %s
            AND table_schema = 'public'
            ORDER BY ordinal_position
        """, (table_name,))

        columns = []
        for row in cursor.fetchall():
            columns.append({
                "name": row[0],
                "type": row[1],
                "nullable": row[2] == "YES",
                "default": row[3]
            })

        cursor.close()
        conn.close()

        result = {
            "table_name": table_name,
            "columns": columns,
            "column_count": len(columns),
            "success": True
        }

        result_str = json.dumps(result, ensure_ascii=False)
        # å­˜å…¥ç¼“å­˜
        _schema_cache.set(cache_key, result_str)
        logger.info(f"get_schema({table_name}): æŸ¥è¯¢æˆåŠŸï¼Œç¼“å­˜ç»“æœ ({len(columns)} åˆ—)")

        return result_str

    except Exception as e:
        logger.error(f"Get schema error: {e}")
        error_str = json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)
        return error_str


# ============================================================================
# å·¥å…·é›†åˆ
# ============================================================================

def get_database_tools(
    connection_id: Optional[str] = None,
    db_session: Optional[Any] = None,
    tenant_id: Optional[str] = None
) -> List:
    """
    è·å–æ‰€æœ‰æ•°æ®åº“å·¥å…·ï¼ˆæ”¯æŒ Excel å’Œæ•°æ®åº“ï¼‰

    Args:
        connection_id: æ•°æ®æºè¿æ¥ IDï¼ˆå¯é€‰ï¼‰
        db_session: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºæŸ¥è¯¢æ•°æ®æºé…ç½®ï¼‰
        tenant_id: ç§Ÿæˆ· ID

    Returns:
        LangChain Tool åˆ—è¡¨
    """
    # è®¾ç½®è¿æ¥ä¸Šä¸‹æ–‡ä¾›å·¥å…·ä½¿ç”¨ï¼ˆè™½ç„¶ä¹‹åä¼šä½¿ç”¨é—­åŒ…ï¼Œä½†ä¿ç•™è®¾ç½®ä»¥å…¼å®¹ï¼‰
    _set_connection_context(connection_id, db_session, tenant_id)

    # ğŸ”§ æ–°æ–¹æ¡ˆï¼šä½¿ç”¨é—­åŒ…é¢„ç»‘å®š connection_id
    # åŒæ—¶åœ¨åŒ…è£…å‡½æ•°ä¸­è®¾ç½® context å˜é‡ï¼Œç¡®ä¿ _get_connection_context() èƒ½å¤Ÿè·å–åˆ°æ­£ç¡®çš„å€¼

    from langchain_core.tools import StructuredTool

    # åˆ›å»ºåŒ…è£…å‡½æ•°ï¼Œé¢„ç»‘å®š connection_id å¹¶è®¾ç½® context
    def make_execute_query(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped(query: str) -> str:
            # è®¾ç½® context å˜é‡ï¼Œç¡®ä¿ get_database_url èƒ½å¤Ÿè·å–åˆ°æ­£ç¡®çš„å€¼
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return execute_query(query, fixed_connection_id)
        wrapped.__name__ = "execute_query"
        wrapped.__doc__ = execute_query.__doc__
        return wrapped

    def make_list_tables(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped() -> str:
            # è®¾ç½® context å˜é‡
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return list_tables(fixed_connection_id)
        wrapped.__name__ = "list_tables"
        wrapped.__doc__ = list_tables.__doc__
        return wrapped

    def make_get_schema(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped(table_name: str) -> str:
            # è®¾ç½® context å˜é‡
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return get_schema(table_name, fixed_connection_id)
        wrapped.__name__ = "get_schema"
        wrapped.__doc__ = get_schema.__doc__
        return wrapped

    # åˆ›å»ºå¸¦é¢„ç»‘å®š connection_id çš„åŒ…è£…å‡½æ•°
    bound_execute_query = make_execute_query(connection_id, db_session, tenant_id)
    bound_list_tables = make_list_tables(connection_id, db_session, tenant_id)
    bound_get_schema = make_get_schema(connection_id, db_session, tenant_id)

    # åˆ›å»º StructuredTool å¯¹è±¡
    tools = [
        StructuredTool.from_function(
            func=bound_execute_query,
            name="execute_query",
            description=execute_query.__doc__
        ),
        StructuredTool.from_function(
            func=bound_list_tables,
            name="list_tables",
            description=list_tables.__doc__
        ),
        StructuredTool.from_function(
            func=bound_get_schema,
            name="get_schema",
            description=get_schema.__doc__
        )
    ]

    logger.info(f"[get_database_tools] Created {len(tools)} tools with connection_id={connection_id}")
    return tools


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    import asyncio

    print("=" * 60)
    print("Database Tools æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯• 1: åˆ—å‡ºè¡¨
    print("\n[TEST 1] åˆ—å‡ºæ•°æ®åº“è¡¨")
    result = list_tables.invoke({})
    print(f"ç»“æœ: {result}")

    # æµ‹è¯• 2: è·å–è¡¨ç»“æ„
    print("\n[TEST 2] è·å–è¡¨ç»“æ„")
    result = get_schema.invoke({"table_name": "tenants"})
    print(f"ç»“æœ: {result}")

    # æµ‹è¯• 3: æ‰§è¡ŒæŸ¥è¯¢
    print("\n[TEST 3] æ‰§è¡ŒæŸ¥è¯¢")
    result = execute_query.invoke({"query": "SELECT * FROM tenants LIMIT 1"})
    print(f"ç»“æœ: {result}")
