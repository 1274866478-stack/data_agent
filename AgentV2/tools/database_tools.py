# -*- coding: utf-8 -*-
"""
Database Query Tools - æ•°æ®åº“æŸ¥è¯¢å·¥å…· (å¸¦ç¼“å­˜ä¼˜åŒ– + Excelæ”¯æŒ)
===============================================================

ä¸º AgentV2 æä¾›æ•°æ®åº“å’Œ Excel æ–‡ä»¶æŸ¥è¯¢èƒ½åŠ›ã€‚

æ ¸å¿ƒåŠŸèƒ½:
    - execute_query: æ‰§è¡Œåªè¯» SQL æŸ¥è¯¢ï¼ˆæ•°æ®åº“å’Œ Excelï¼‰
    - list_tables: åˆ—å‡ºæ•°æ®åº“è¡¨æˆ– Excel å·¥ä½œè¡¨
    - get_schema: è·å–è¡¨ç»“æ„æˆ– Excel åˆ—ä¿¡æ¯

ä¼˜åŒ–ç‰¹æ€§:
    - list_schema_files: åˆ—å‡ºè¯­ä¹‰å±‚æ–‡æ¡£
    - read_schema_file: è¯»å–è¯­ä¹‰å±‚æ–‡æ¡£å†…å®¹
    - search_schema: æœç´¢è¯­ä¹‰å±‚æ–‡æ¡£
    - SchemaFSValidator: æ–‡ä»¶ç³»ç»Ÿå®‰å…¨éªŒè¯

ä¼˜åŒ–ç‰¹æ€§:
    - Schema ç¼“å­˜ï¼šé¿å…é‡å¤æŸ¥è¯¢è¡¨ç»“æ„
    - æŸ¥è¯¢ç»“æœç¼“å­˜ï¼šç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ç¼“å­˜
    - TTL æœºåˆ¶ï¼šç¼“å­˜è¿‡æœŸè‡ªåŠ¨åˆ·æ–°
    - å¤šæ•°æ®æºæ”¯æŒï¼šPostgreSQL, MySQL, Excel æ–‡ä»¶
    - æ–‡ä»¶ç³»ç»Ÿå®‰å…¨è®¿é—®ï¼šä¸¥æ ¼é™åˆ¶è·¯å¾„éå†

ä½œè€…: BMad Master
ç‰ˆæœ¬: 3.1.0
"""

import os
import hashlib
import json
import time
from typing import Optional, List, Dict, Any, Tuple
from functools import wraps, lru_cache
import logging

# ä½¿ç”¨ contextvars æ›¿ä»£ threading.localï¼Œæ”¯æŒå¼‚æ­¥/å¤šçº¿ç¨‹ç¯å¢ƒ
from contextvars import ContextVar

logger = logging.getLogger(__name__)

# ============================================================================
# è¿æ¥ä¸Šä¸‹æ–‡å­˜å‚¨ (ä½¿ç”¨ contextvars æ”¯æŒå¼‚æ­¥/å¤šçº¿ç¨‹)
# ============================================================================

_connection_id_ctx: ContextVar[Optional[str]] = ContextVar("connection_id", default=None)
_db_session_ctx: ContextVar[Optional[Any]] = ContextVar("db_session", default=None)
_tenant_id_ctx: ContextVar[Optional[str]] = ContextVar("tenant_id", default=None)

# ğŸ”§ æ–°å¢ï¼šè·Ÿè¸ª list_tables æ˜¯å¦è¢«è°ƒç”¨
_list_tables_called_ctx: ContextVar[bool] = ContextVar("list_tables_called", default=False)

def _set_connection_context(
    connection_id: Optional[str] = None,
    db_session: Optional[Any] = None,
    tenant_id: Optional[str] = None
) -> None:
    """è®¾ç½®è¿æ¥ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå·¥å…·è°ƒç”¨ï¼‰"""
    _connection_id_ctx.set(connection_id)
    _db_session_ctx.set(db_session)
    _tenant_id_ctx.set(tenant_id)
    logger.info(f"[CONTEXT_SET] connection_id={connection_id}, tenant_id={tenant_id}")


def _get_connection_context() -> Tuple[Optional[str], Optional[Any], Optional[str]]:
    """è·å–è¿æ¥ä¸Šä¸‹æ–‡"""
    connection_id = _connection_id_ctx.get()
    db_session = _db_session_ctx.get()
    tenant_id = _tenant_id_ctx.get()
    logger.info(f"[CONTEXT_GET] connection_id={connection_id}, tenant_id={tenant_id}")
    return (connection_id, db_session, tenant_id)


def _clear_connection_context() -> None:
    """æ¸…é™¤è¿æ¥ä¸Šä¸‹æ–‡"""
    _connection_id_ctx.set(None)
    _db_session_ctx.set(None)
    _tenant_id_ctx.set(None)
    logger.info("[CONTEXT_CLEAR] Connection context cleared")


# ğŸ”§ æ–°å¢ï¼šlist_tables è°ƒç”¨æ ‡å¿—ç®¡ç†å‡½æ•°
def _set_list_tables_called(value: bool = True) -> None:
    """è®¾ç½® list_tables å·²è°ƒç”¨æ ‡å¿—"""
    _list_tables_called_ctx.set(value)
    logger.info(f"[LIST_TABLES_FLAG] Set to {value}")


def _get_list_tables_called() -> bool:
    """è·å– list_tables å·²è°ƒç”¨æ ‡å¿—"""
    return _list_tables_called_ctx.get()


def _reset_list_tables_flag() -> None:
    """é‡ç½® list_tables è°ƒç”¨æ ‡å¿—"""
    _list_tables_called_ctx.set(False)
    logger.info("[LIST_TABLES_FLAG] Reset to False")

# ============================================================================
# æ•°æ®ç±»å‹åºåˆ—åŒ–å·¥å…·
# ============================================================================

def _serialize_value(value: Any) -> Any:
    """
    å°†æ•°æ®åº“å€¼è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–çš„æ ¼å¼

    å¤„ç† PostgreSQL å¤æ‚æ•°æ®ç±»å‹:
    - Decimal -> float
    - datetime/date -> ISO æ ¼å¼å­—ç¬¦ä¸²
    - UUID -> å­—ç¬¦ä¸²
    - NaN/None -> null

    Args:
        value: æ•°æ®åº“è¿”å›çš„åŸå§‹å€¼

    Returns:
        JSON å¯åºåˆ—åŒ–çš„å€¼
    """
    import decimal
    import uuid
    from datetime import date, datetime, time
    import pandas as pd

    if value is None:
        return None
    elif isinstance(value, decimal.Decimal):
        # ä¿ç•™ç²¾åº¦ï¼Œè½¬æ¢ä¸º float
        return float(value)
    elif isinstance(value, (datetime, date, time)):
        # è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
        return value.isoformat()
    elif isinstance(value, uuid.UUID):
        # UUID è½¬å­—ç¬¦ä¸²
        return str(value)
    elif isinstance(value, bytes):
        # å­—èŠ‚æ•°ç»„è½¬ base64 å­—ç¬¦ä¸²
        import base64
        return base64.b64encode(value).decode('utf-8')
    elif pd.isna(value):
        # pandas NaN è½¬ä¸º None
        return None
    # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
    return value


def _serialize_row(row: tuple, columns: list = None) -> list:
    """
    åºåˆ—åŒ–å•è¡Œæ•°æ®

    Args:
        row: æ•°æ®åº“è¡Œæ•°æ®ï¼ˆå…ƒç»„ï¼‰
        columns: åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        åºåˆ—åŒ–åçš„åˆ—è¡¨
    """
    return [_serialize_value(v) for v in row]


def _serialize_rows(rows: list, columns: list = None) -> list:
    """
    åºåˆ—åŒ–å¤šè¡Œæ•°æ®

    Args:
        rows: æ•°æ®åº“è¡Œæ•°æ®åˆ—è¡¨
        columns: åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        åºåˆ—åŒ–åçš„äºŒç»´åˆ—è¡¨
    """
    return [_serialize_row(row, columns) for row in rows]


# ============================================================================
# ç¼“å­˜ç®¡ç†
# ============================================================================

class SimpleCache:
    """å¢å¼ºçš„å†…å­˜ç¼“å­˜ï¼Œæ”¯æŒç»Ÿè®¡"""

    def __init__(self, ttl: int = 300, name: str = "cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜

        Args:
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
            name: ç¼“å­˜åç§°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        """
        self._cache: Dict[str, tuple] = {}  # key -> (value, expire_time)
        self.ttl = ttl
        self.name = name
        self._hits = 0
        self._misses = 0
        self._sets = 0

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key in self._cache:
            value, expire_time = self._cache[key]
            if time.time() < expire_time:
                self._hits += 1
                return value
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self._cache[key]
        self._misses += 1
        return None

    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜"""
        expire_time = time.time() + self.ttl
        self._cache[key] = (value, expire_time)
        self._sets += 1

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._hits = 0
        self._misses = 0
        self._sets = 0

    def has(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        return self.get(key) is not None

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self._hits + self._misses
        hit_rate = self._hits / total_requests if total_requests > 0 else 0
        return {
            "name": self.name,
            "size": len(self._cache),
            "hits": self._hits,
            "misses": self._misses,
            "sets": self._sets,
            "hit_rate": hit_rate,
            "ttl": self.ttl
        }


# å…¨å±€ç¼“å­˜å®ä¾‹
_schema_cache = SimpleCache(ttl=600, name="schema_cache")  # Schema ç¼“å­˜ 10 åˆ†é’Ÿ
_query_cache = SimpleCache(ttl=300, name="query_cache")    # æŸ¥è¯¢ç»“æœç¼“å­˜ 5 åˆ†é’Ÿ (å»¶é•¿ TTL)


def _normalize_sql(sql: str) -> str:
    """
    æ ‡å‡†åŒ– SQL æŸ¥è¯¢ç”¨äºç¼“å­˜é”®ç”Ÿæˆ

    å¤„ç†:
        - è½¬æ¢ä¸ºå°å†™
        - ç§»é™¤å¤šä½™ç©ºæ ¼
        - ç§»é™¤æ³¨é‡Š
        - ç»Ÿä¸€åˆ†å·ä½¿ç”¨

    Args:
        sql: åŸå§‹ SQL æŸ¥è¯¢

    Returns:
        æ ‡å‡†åŒ–åçš„ SQL
    """
    import re

    # è½¬æ¢ä¸ºå°å†™
    normalized = sql.lower().strip()

    # ç§»é™¤æ³¨é‡Š
    normalized = re.sub(r'--.*$', '', normalized, flags=re.MULTILINE)
    normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)

    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    normalized = ' '.join(normalized.split())

    # ç¡®ä¿ä»¥åˆ†å·ç»“å°¾
    if not normalized.endswith(';'):
        normalized += ';'

    return normalized


def _make_cache_key(*args, **kwargs) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    # å¦‚æœå‚æ•°åŒ…å« SQLï¼Œå…ˆæ ‡å‡†åŒ–
    if args and 'select' in str(args[0]).lower():
        args = (_normalize_sql(args[0]),) + args[1:]

    key_str = json.dumps([args, kwargs], sort_keys=True, default=str)
    return hashlib.md5(key_str.encode()).hexdigest()


def get_cache_stats() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "schema_cache": _schema_cache.get_stats(),
        "query_cache": _query_cache.get_stats()
    }

# ============================================================================
# æ•°æ®åº“è¿æ¥ç®¡ç†
# ============================================================================

def get_database_url(
    connection_id: Optional[str] = None
) -> Tuple[str, Optional["DataSourceConnectionInfo"]]:
    """
    è·å–æ•°æ®åº“è¿æ¥ URL æˆ– Excel æ–‡ä»¶è·¯å¾„

    Args:
        connection_id: å¯é€‰çš„æ•°æ®æºè¿æ¥ ID

    Returns:
        (connection_url, connection_info) å…ƒç»„
        - connection_url: è¿æ¥å­—ç¬¦ä¸²ï¼ˆæ•°æ®åº“ç”¨ URLï¼ŒExcel ç”¨ "excel://" å‰ç¼€ï¼‰
        - connection_info: æ•°æ®æºè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœä»æ•°æ®åº“è·å–ï¼‰
    """
    # ä»è¿æ¥ä¸Šä¸‹æ–‡è·å–æ•°æ®åº“ä¼šè¯å’Œç§Ÿæˆ· ID
    _, db_session, tenant_id = _get_connection_context()

    # å¦‚æœæ²¡æœ‰æä¾› connection_idï¼Œå°è¯•è·å–ç§Ÿæˆ·çš„é»˜è®¤æ´»è·ƒæ•°æ®æº
    if not connection_id:
        if db_session and tenant_id:
            try:
                # Docker ç¯å¢ƒè·¯å¾„è®¾ç½®ï¼ˆç¡®ä¿ /app/src åœ¨ sys.path å¼€å¤´ï¼‰
                import sys
                if "/app/src" in sys.path:
                    sys.path.remove("/app/src")
                sys.path.insert(0, "/app/src")
                logger.debug("Ensured /app/src is at sys.path[0] for default data source query")

                from app.data.models import DataSourceConnection

                # æŸ¥è¯¢ç§Ÿæˆ·çš„ç¬¬ä¸€ä¸ªæ´»è·ƒæ•°æ®æº
                connection = db_session.query(DataSourceConnection).filter(
                    DataSourceConnection.tenant_id == tenant_id,
                    DataSourceConnection.status == "active"
                ).first()

                if connection:
                    connection_id = str(connection.id)
                    logger.info(f"è‡ªåŠ¨è·å–ç§Ÿæˆ· {tenant_id} çš„é»˜è®¤æ•°æ®æº: {connection.name} (ID: {connection_id})")
                else:
                    logger.warning(f"ç§Ÿæˆ· {tenant_id} æ²¡æœ‰é…ç½®æ´»è·ƒæ•°æ®æºï¼Œä½¿ç”¨ç³»ç»Ÿå…ƒæ•°æ®åº“")
                    return os.environ.get("DATABASE_URL"), None
            except ImportError as e:
                logger.error(f"Failed to import DataSourceConnection model: {e}")
                return os.environ.get("DATABASE_URL"), None
            except Exception as e:
                logger.error(f"Failed to query default data source: {e}")
                return os.environ.get("DATABASE_URL"), None
        else:
            # æ²¡æœ‰æ•°æ®åº“ä¼šè¯ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤æ•°æ®åº“
            return os.environ.get("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/data_agent"), None

    # ä»è¿™é‡Œå¼€å§‹æœ‰ connection_id
    if not db_session or not tenant_id:
        logger.warning(
            f"connection_id provided but db_session/tenant_id not available. "
            f"Using default database. connection_id={connection_id}"
        )
        return os.environ.get("DATABASE_URL"), None

    try:
        # å¯¼å…¥æ•°æ®æºæœåŠ¡ï¼ˆDocker ç¯å¢ƒä½¿ç”¨ /app è·¯å¾„ï¼‰
        import sys

        # è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        logger.info(f"BEFORE: /app/src in sys.path = {('/app/src' in sys.path)}")
        logger.info(f"BEFORE: os.path.exists('/app/src') = {os.path.exists('/app/src')}")
        logger.info(f"BEFORE: os.path.exists('/app/src/app/services') = {os.path.exists('/app/src/app/services')}")
        logger.info(f"BEFORE: sys.path[0] = {sys.path[0]}")

        # ç¡®ä¿ /app/src åœ¨ sys.path çš„æœ€å‰é¢ï¼ˆå› ä¸º app åŒ…åœ¨ /app/src/app/...ï¼‰
        # Python å¯¼å…¥ app.services æ—¶ï¼Œä¼šåœ¨ sys.path ä¸­æŸ¥æ‰¾ app/ ç›®å½•
        if "/app/src" in sys.path:
            sys.path.remove("/app/src")
        sys.path.insert(0, "/app/src")
        logger.info("ACTION: Ensured /app/src is at sys.path[0]")

        logger.info(f"AFTER: sys.path[0] = {sys.path[0]}")

        # æ£€æŸ¥ /app/src/app ç›®å½•å†…å®¹
        if os.path.exists("/app/src/app"):
            all_files = os.listdir("/app/src/app")
            logger.info(f"Files in /app/src/app (total {len(all_files)}): {all_files}")
            # æ£€æŸ¥ services æ˜¯å¦å­˜åœ¨
            if "services" in all_files:
                logger.info("âœ“ services directory found in /app/src/app")
                # æ£€æŸ¥ __init__.py æ–‡ä»¶
                init_files = []
                for pyc_init in ["/app/src/app/__init__.py", "/app/src/app/services/__init__.py"]:
                    if os.path.exists(pyc_init):
                        init_files.append(pyc_init + " âœ“")
                    else:
                        init_files.append(pyc_init + " âœ—")
                logger.info(f"__init__.py files: {init_files}")
            else:
                logger.error("âœ— services directory NOT found in /app/src/app")
        else:
            logger.error(f"Directory /app/src/app does not exist!")

        # æ£€æŸ¥æ˜¯å¦èƒ½ç›´æ¥è®¿é—® services ç›®å½•
        if os.path.exists("/app/src/app/services/data_source_service.py"):
            logger.info("âœ“ data_source_service.py file exists")
        else:
            logger.error("âœ— data_source_service.py file NOT found at /app/src/app/services/")

        # æ¸…é™¤å¯èƒ½çš„é™ˆæ—§å¯¼å…¥ç¼“å­˜ï¼ˆåŒ…æ‹¬ app æ¨¡å—æœ¬èº«ï¼‰
        stale_keys = [k for k in sys.modules.keys() if k.startswith("app.")]
        if stale_keys:
            logger.info(f"Removing stale imports from sys.modules: {stale_keys}")
            for key in stale_keys:
                del sys.modules[key]
        # ç¡®ä¿åˆ é™¤ app æ¨¡å—æœ¬èº«ï¼ˆå¯èƒ½æ˜¯å‘½åç©ºé—´åŒ…ï¼‰
        if "app" in sys.modules:
            logger.info(f"Removing 'app' module from sys.modules (was: {type(sys.modules['app'])})")
            del sys.modules["app"]

        # å°è¯•å¯¼å…¥
        logger.info("Attempting to import app.services.data_source_service...")
        try:
            from app.services.data_source_service import data_source_service
            from app.services.data_source_service import DataSourceConnectionInfo
            logger.info("Successfully imported data_source_service")
        except ImportError as e:
            # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            logger.error(f"ImportError: {e}")
            logger.error(f"sys.path[0:3] = {sys.path[0:3]}")
            # æ£€æŸ¥ app æ¨¡å—æ˜¯å¦åœ¨ sys.modules ä¸­
            if "app" in sys.modules:
                logger.error(f"app module already in sys.modules: {sys.modules['app']}")
            if "app.services" in sys.modules:
                logger.error(f"app.services already in sys.modules: {sys.modules['app.services']}")
            raise

        # åŒæ­¥åŒ…è£…ï¼šå› ä¸º data_source_service æ˜¯å¼‚æ­¥çš„
        import asyncio

        def get_connection_info():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                logger.info(f"Calling data_source_service.get_data_source_connection_info(connection_id={connection_id}, tenant_id={tenant_id})")
                result = loop.run_until_complete(
                    data_source_service.get_data_source_connection_info(
                        connection_id=connection_id,
                        tenant_id=tenant_id,
                        db=db_session
                    )
                )
                logger.info(f"data_source_service returned: {result}")
                return result
            except Exception as e:
                logger.error(f"Error in get_connection_info: {e}", exc_info=True)
                raise
            finally:
                loop.close()

        connection_info = get_connection_info()
        logger.info(f"Retrieved connection_info: type={connection_info.connection_type}, file_path={connection_info.file_path}")

        # æ ¹æ®æ•°æ®æºç±»å‹è¿”å›ä¸åŒçš„è¿æ¥ä¿¡æ¯
        if connection_info.connection_type == "excel":
            # Excel æ–‡ä»¶ï¼šè¿”å›ç‰¹æ®Šæ ‡è®°å’Œæ–‡ä»¶ä¿¡æ¯
            logger.info(f"Using Excel data source: {connection_info.file_path}, sheets={connection_info.sheets}")
            return f"excel://{connection_info.file_path}", connection_info
        else:
            # æ•°æ®åº“ï¼šè¿”å›è§£å¯†åçš„è¿æ¥å­—ç¬¦ä¸²
            logger.info(f"Using database data source: type={connection_info.connection_type}")
            return connection_info.connection_string, connection_info

    except ImportError as e:
        logger.error(f"Failed to import data_source_service: {e}")
        return os.environ.get("DATABASE_URL"), None
    except Exception as e:
        logger.error(f"Failed to get connection info for {connection_id}: {e}")
        return os.environ.get("DATABASE_URL"), None


def _is_excel_connection(database_url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ Excel è¿æ¥"""
    return database_url.startswith("excel://")


def _is_sqlite_connection(database_url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ SQLite è¿æ¥"""
    return database_url.startswith("sqlite:///")


def create_db_connection(database_url: str):
    """
    åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆæ”¯æŒ PostgreSQL å’Œ SQLiteï¼‰

    Args:
        database_url: æ•°æ®åº“è¿æ¥ URL

    Returns:
        æ•°æ®åº“è¿æ¥å¯¹è±¡
    """
    if _is_sqlite_connection(database_url):
        import sqlite3
        # SQLite è¿æ¥æ ¼å¼: sqlite:///path/to/database.db
        db_path = database_url.replace("sqlite:///", "")
        logger.info(f"Creating SQLite connection: {db_path}")
        return sqlite3.connect(db_path)
    else:
        # PostgreSQL è¿æ¥
        import psycopg2
        logger.info(f"Creating PostgreSQL connection")
        return psycopg2.connect(database_url)


def _get_excel_file_path(database_url: str) -> str:
    """ä» Excel è¿æ¥ URL ä¸­æå–æ–‡ä»¶è·¯å¾„"""
    return database_url[8:]  # å»æ‰ "excel://" å‰ç¼€


# ============================================================================
# è¡¨åæ˜ å°„å·¥å…·ï¼ˆExcel å·¥ä½œè¡¨åæ˜ å°„ï¼‰
# ============================================================================

# å¤‡ç”¨å·¥ä½œè¡¨åç§°é…ç½®
SHEET_ALTERNATIVES = {
    "Products": ["äº§å“è¡¨", "å•†å“è¡¨", "products", "Products"],
    "Customers": ["customers", "Customers", "ç”¨æˆ·è¡¨", "å®¢æˆ·è¡¨"],
    "Orders": ["è®¢å•è¡¨", "orders", "Orders"],
    "Categories": ["åˆ†ç±»è¡¨", "categories", "Categories"],
    "OrderDetails": ["è®¢å•æ˜ç»†", "order_details", "OrderDetails", "è®¢å•è¯¦æƒ…"],
}


def _sheet_exists(file_path: str, sheet_name: str) -> bool:
    """éªŒè¯å·¥ä½œè¡¨æ˜¯å¦å­˜åœ¨äº Excel æ–‡ä»¶ä¸­

    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°

    Returns:
        å·¥ä½œè¡¨æ˜¯å¦å­˜åœ¨
    """
    try:
        import pandas as pd
        xl = pd.ExcelFile(file_path)
        return sheet_name in xl.sheet_names
    except Exception as e:
        logger.warning(f"éªŒè¯å·¥ä½œè¡¨å¤±è´¥: {e}")
        return False


@lru_cache(maxsize=32)
def _get_excel_sheet_mapping(english_table: str, file_path: str = None) -> Optional[str]:
    """å°†è‹±æ–‡è¡¨åæ˜ å°„åˆ° Excel å·¥ä½œè¡¨åï¼ˆæ”¯æŒå¤‡ç”¨åç§°å›é€€ï¼‰

    ä»è¯­ä¹‰å±‚ YAML æ–‡ä»¶è¯»å– excel_sheet é…ç½®ï¼Œå®ç°è‹±æ–‡è¡¨ååˆ°ä¸­æ–‡å·¥ä½œè¡¨åçš„æ˜ å°„ã€‚
    å½“ä¸»æ˜ å°„çš„å·¥ä½œè¡¨ä¸å­˜åœ¨æ—¶ï¼Œè‡ªåŠ¨å°è¯•å¤‡ç”¨åç§°ã€‚

    Args:
        english_table: è‹±æ–‡è¡¨åï¼ˆå¦‚ "Orders", "Products", "Customers"ï¼‰
        file_path: Excel æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºéªŒè¯å·¥ä½œè¡¨å­˜åœ¨æ€§ï¼‰

    Returns:
        æ˜ å°„åçš„å·¥ä½œè¡¨åï¼Œå¦‚æœæœªæ‰¾åˆ°æ˜ å°„åˆ™è¿”å› None

    ç¤ºä¾‹:
        _get_excel_sheet_mapping("Orders", "path/to/file.xlsx") -> "è®¢å•è¡¨"
        _get_excel_sheet_mapping("Products", "path/to/file.xlsx") -> "å•†å“è¡¨" æˆ– "äº§å“è¡¨"
        _get_excel_sheet_mapping("Customers", "path/to/file.xlsx") -> "customers"
    """
    try:
        # åŠ¨æ€å¯¼å…¥ SchemaLoaderï¼Œé¿å…å¾ªç¯å¯¼å…¥
        import sys
        from pathlib import Path

        # æ·»åŠ  AgentV2 åˆ° sys.pathï¼ˆå¦‚æœå°šæœªæ·»åŠ ï¼‰
        agentv2_path = Path(__file__).parent.parent
        agentv2_str = str(agentv2_path)
        if agentv2_str not in sys.path:
            sys.path.insert(0, agentv2_str)

        from AgentV2.schema_pruning import SchemaLoader

        loader = SchemaLoader()
        _, _, yaml_mappings = loader.load_from_yaml()

        # 1. ä» YAML è·å–ä¸»æ˜ å°„
        primary = yaml_mappings.get(english_table)

        # 2. æ„å»ºå€™é€‰åˆ—è¡¨ï¼ˆä¸»æ˜ å°„ + å¤‡ç”¨åç§°ï¼‰
        candidates = [primary] if primary else []
        candidates.extend(SHEET_ALTERNATIVES.get(english_table, []))

        # 3. å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_candidates = [x for x in candidates if x and x not in seen and not seen.add(x)]

        # 4. å¦‚æœæœ‰ file_pathï¼Œè¿”å›ç¬¬ä¸€ä¸ªå­˜åœ¨çš„å·¥ä½œè¡¨
        if file_path:
            for name in unique_candidates:
                if _sheet_exists(file_path, name):
                    logger.info(f"è¡¨åæ˜ å°„: {english_table} -> {name}")
                    return name
            # æ‰€æœ‰å€™é€‰éƒ½ä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Š
            logger.warning(f"å·¥ä½œè¡¨ '{english_table}' çš„æ‰€æœ‰å€™é€‰åç§° {unique_candidates} éƒ½ä¸å­˜åœ¨äºæ–‡ä»¶ {file_path}")

        # 5. æ²¡æœ‰ file_path æˆ–éƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ä¸»æ˜ å°„
        return primary if primary else (unique_candidates[0] if unique_candidates else None)

    except Exception as e:
        logger.warning(f"è·å–è¡¨åæ˜ å°„å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸè¡¨å")
        return None


# ============================================================================
# Excel æŸ¥è¯¢å·¥å…·
# ============================================================================

def execute_excel_query(
    query: str,
    file_path: str,
    sheet_name: Optional[str] = None
) -> str:
    """
    æ‰§è¡Œ Excel æ–‡ä»¶æŸ¥è¯¢ï¼ˆä½¿ç”¨ pandasï¼‰

    Args:
        query: ç±»ä¼¼ SQL çš„æŸ¥è¯¢æˆ– pandas ä»£ç 
        file_path: Excel æ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        æŸ¥è¯¢ç»“æœçš„ JSON å­—ç¬¦ä¸²
    """
    import json
    import pandas as pd

    try:
        # è¯»å– Excel æ–‡ä»¶
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')
        else:
            # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, engine='openpyxl')
            sheet_name = "Sheet1"

        logger.info(f"Excel file loaded: {file_path}, sheet: {sheet_name}, shape: {df.shape}")

        # ç®€å•çš„ SQL è§£æå’Œè½¬æ¢
        result_df = _parse_sql_to_pandas(query, df)

        # è½¬æ¢ä¸ºç»“æœæ ¼å¼
        columns = result_df.columns.tolist()
        rows = result_df.values.tolist()

        # ğŸ”§ ä½¿ç”¨åºåˆ—åŒ–å‡½æ•°å¤„ç†æ‰€æœ‰æ•°æ®ç±»å‹
        rows = _serialize_rows(rows, columns)

        result = {
            "columns": columns,
            "rows": rows,
            "row_count": len(rows),
            "success": True,
            "data_source": "excel",
            "sheet_name": sheet_name
        }

        logger.info(f"Excel query executed: {len(rows)} rows returned")
        return json.dumps(result, ensure_ascii=False, default=str)

    except FileNotFoundError:
        logger.error(f"Excel file not found: {file_path}")
        return json.dumps({
            "error": f"Excel file not found: {file_path}",
            "error_type": "file_not_found"
        }, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Excel query error: {e}")
        return json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)


def _parse_sql_to_pandas(query: str, df: "Any") -> "Any":
    """
    ç®€å•çš„ SQL åˆ° pandas è½¬æ¢

    æ”¯æŒçš„åŸºæœ¬ SQL è¯­æ³•:
    - SELECT col1, col2 FROM table
    - SELECT * FROM table
    - SELECT ... FROM table WHERE col = value
    - SELECT ... FROM table WHERE col LIKE value
    - SELECT ... FROM table ORDER BY col
    - SELECT ... FROM table LIMIT n

    Args:
        query: SQL æŸ¥è¯¢è¯­å¥
        df: pandas DataFrame

    Returns:
        è¿‡æ»¤åçš„ DataFrame
    """
    import re

    query_upper = query.upper().strip()
    result_df = df.copy()

    # æå– SELECT åˆ—
    select_match = re.search(r'SELECT\s+(.+?)\s+FROM', query_upper)
    if select_match:
        columns_str = select_match.group(1).strip()
        if columns_str != '*':
            # è§£æåˆ—åï¼ˆå¤„ç†é€—å·åˆ†éš”çš„åˆ—ï¼‰
            columns = [col.strip().lower() for col in columns_str.split(',')]
            # åŒ¹é…å®é™…åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            actual_columns = {col.lower(): col for col in df.columns}
            selected_columns = []
            for col in columns:
                if col in actual_columns:
                    selected_columns.append(actual_columns[col])
            if selected_columns:
                result_df = result_df[selected_columns]

    # æå– WHERE æ¡ä»¶
    where_match = re.search(r'WHERE\s+(.+?)(?:\s+ORDER\s+BY|\s+LIMIT|;|$)', query_upper)
    if where_match:
        where_clause = where_match.group(1).strip()
        result_df = _apply_where_clause(result_df, where_clause)

    # æå– ORDER BY
    order_match = re.search(r'ORDER\s+BY\s+(\w+)(?:\s+(ASC|DESC))?', query_upper)
    if order_match:
        col_name = order_match.group(1).lower()
        direction = order_match.group(2) if order_match.group(2) else 'ASC'

        # åŒ¹é…å®é™…åˆ—å
        actual_columns = {col.lower(): col for col in result_df.columns}
        if col_name in actual_columns:
            actual_col = actual_columns[col_name]
            ascending = direction == 'ASC'
            result_df = result_df.sort_values(by=actual_col, ascending=ascending)

    # æå– LIMIT
    limit_match = re.search(r'LIMIT\s+(\d+)', query_upper)
    if limit_match:
        limit = int(limit_match.group(1))
        result_df = result_df.head(limit)

    return result_df


def _apply_where_clause(df: "Any", where_clause: str) -> "Any":
    """
    åº”ç”¨ WHERE æ¡ä»¶åˆ° DataFrame

    æ”¯æŒçš„æ¡ä»¶:
    - col = value
    - col LIKE value
    - col > value, col < value, col >= value, col <= value
    - col AND col
    - col OR col

    Args:
        df: pandas DataFrame
        where_clause: WHERE å­å¥

    Returns:
        è¿‡æ»¤åçš„ DataFrame
    """
    import re

    result_df = df.copy()

    # åˆ†å‰² AND/OR æ¡ä»¶
    conditions = re.split(r'\s+(AND|OR)\s+', where_clause, flags=re.IGNORECASE)
    current_op = 'AND'

    for i, cond in enumerate(conditions):
        cond = cond.strip()
        if cond.upper() in ('AND', 'OR'):
            current_op = cond.upper()
            continue

        # è§£ææ¡ä»¶ - ğŸ”§ ä¿®å¤ï¼šå°†æ›´é•¿çš„æ“ä½œç¬¦æ”¾åœ¨å‰é¢
        match = re.match(
            r'(\w+)\s*(>=|<=|<>|=|LIKE|>|<)\s*[\'"]?([^\'"]+)[\'"]?',
            cond,
            re.IGNORECASE
        )
        if match:
            col_name = match.group(1).lower()
            operator = match.group(2).upper()
            value = match.group(3).strip()

            # åŒ¹é…å®é™…åˆ—å
            actual_columns = {col.lower(): col for col in result_df.columns}
            if col_name not in actual_columns:
                continue

            actual_col = actual_columns[col_name]

            # åº”ç”¨æ¡ä»¶
            mask = None
            if operator == '=':
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] == value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) == value
            elif operator == 'LIKE':
                mask = result_df[actual_col].astype(str).str.contains(
                    value.replace('%', ''),
                    case=False,
                    na=False
                )
            elif operator == '>':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] > value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) > value
            elif operator == '<':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] < value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) < value
            elif operator == '>=':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] >= value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) >= value
            elif operator == '<=':
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] <= value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) <= value
            elif operator == '<>':
                # ä¸ç­‰äº
                try:
                    value_num = float(value)
                    mask = result_df[actual_col] != value_num
                except ValueError:
                    mask = result_df[actual_col].astype(str) != value

            if mask is not None:
                if i == 0 or current_op == 'AND':
                    result_df = result_df[mask]
                else:  # OR
                    result_df = pd.concat([result_df, df[mask]]).drop_duplicates()

    return result_df


# ============================================================================
# æ•°æ®åº“æŸ¥è¯¢å·¥å…·
# ============================================================================

# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def execute_query(query: str, connection_id: Optional[str] = None) -> str:
    """
    æ‰§è¡Œæ•°æ®æŸ¥è¯¢ (æ”¯æŒæ•°æ®åº“å’Œ Excel æ–‡ä»¶)

    è¿™ä¸ªå·¥å…·ç”¨äºæ‰§è¡Œåªè¯»çš„æ•°æ®æŸ¥è¯¢ï¼Œè·å–æ•°æ®ã€‚
    è‡ªåŠ¨æ£€æµ‹æ•°æ®æºç±»å‹å¹¶ä½¿ç”¨ç›¸åº”çš„æŸ¥è¯¢æ–¹æ³•ã€‚

    Args:
        query: SQL SELECT æŸ¥è¯¢è¯­å¥ï¼ˆæˆ–ç”¨äº Excel çš„ç±» SQL æŸ¥è¯¢ï¼‰
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        æŸ¥è¯¢ç»“æœçš„ JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«åˆ—ä¿¡æ¯å’Œè¡Œæ•°æ®

    Example:
        >>> execute_query("SELECT * FROM users LIMIT 10")
        '{"columns": ["id", "name"], "rows": [[1, "Alice"], [2, "Bob"]], "row_count": 2}'
    """
    import json
    import re
    import threading

    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ SELECT æŸ¥è¯¢
    query_upper = query.upper().strip()

    # æ£€æŸ¥å±é™©å…³é”®å­—
    dangerous_keywords = [
        "DROP", "DELETE", "INSERT", "UPDATE", "TRUNCATE",
        "ALTER", "CREATE", "GRANT", "REVOKE"
    ]

    for keyword in dangerous_keywords:
        if re.search(rf"\b{keyword}\b", query_upper):
            return json.dumps({
                "error": f"Security Alert: {keyword} operations are not allowed",
                "error_type": "forbidden_operation"
            }, ensure_ascii=False)

    # æ£€æŸ¥æ˜¯å¦ä»¥ SELECT æˆ–å…¶ä»–å…è®¸çš„å…³é”®å­—å¼€å¤´
    allowed_starts = ["SELECT", "WITH", "SHOW", "EXPLAIN", "DESCRIBE", "DESC"]
    if not any(query_upper.startswith(start) for start in allowed_starts):
        return json.dumps({
            "error": "Query must start with SELECT, WITH, SHOW, EXPLAIN, or DESCRIBE",
            "error_type": "invalid_query_type"
        }, ensure_ascii=False)

    # ğŸŸ¡ æ”¹ä¸ºè­¦å‘Šçº§åˆ«ï¼šä¸å¼ºåˆ¶æ£€æŸ¥ list_tables æ˜¯å¦è¢«è°ƒç”¨
    # åŸå› ï¼šContextVar åœ¨å¼‚æ­¥ç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´æ£€æŸ¥å¤±æ•ˆï¼Œä¸” LLM å·²é€šè¿‡æç¤ºè¯äº†è§£æ­£ç¡®æµç¨‹
    # ä»…è®°å½•æ—¥å¿—ï¼Œä¸é˜»æ­¢æ‰§è¡Œ
    if not _get_list_tables_called():
        logger.info(f"execute_query called without prior list_tables call (query: {query[:50]}...)")
        # æ³¨æ„ï¼šä¸å†è¿”å›é”™è¯¯ï¼Œå…è®¸æ‰§è¡Œç»§ç»­
        # ä¾èµ–æç¤ºè¯æŒ‡å¯¼ LLM æŒ‰æ­£ç¡®é¡ºåºè°ƒç”¨å·¥å…·

    # ğŸ”¥ é‡è¦ï¼šç§»é™¤ LLM æ‰‹åŠ¨æ·»åŠ çš„ tenant_id æ¡ä»¶
    # LLM æœ‰æ—¶ä¼šåœ¨ SQL ä¸­æ‰‹åŠ¨æ·»åŠ  tenant_idï¼Œä½†ä½ç½®å¯èƒ½ä¸æ­£ç¡®
    # ç³»ç»Ÿä¼šç”±ç§Ÿæˆ·éš”ç¦»ä¸­é—´ä»¶è‡ªåŠ¨æ³¨å…¥æ­£ç¡®çš„ tenant_id è¿‡æ»¤æ¡ä»¶
    query_with_tenant_removed = _remove_llm_added_tenant_id(query)
    if query_with_tenant_removed != query:
        logger.info(f"Removed LLM-added tenant_id: {query[:50]}... -> {query_with_tenant_removed[:50]}...")

    # æ¸…ç†å’Œä¿®å¤ SQL
    cleaned_query = clean_and_validate_sql(query_with_tenant_removed)
    if cleaned_query != query_with_tenant_removed:
        logger.info(f"SQL cleaned: {query_with_tenant_removed[:50]}... -> {cleaned_query[:50]}...")

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    # Agent è°ƒç”¨å·¥å…·æ—¶ä¸ä¼šä¼ é€’ connection_idï¼Œéœ€è¦ä»è¿æ¥ä¸Šä¸‹æ–‡è·å–
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    # æ£€æŸ¥æŸ¥è¯¢ç»“æœç¼“å­˜ (ä½¿ç”¨æ ‡å‡†åŒ–çš„ SQL ä½œä¸ºç¼“å­˜é”®)
    cache_key = _make_cache_key(cleaned_query, connection_id)
    cached_result = _query_cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"Query result cache HIT: {cleaned_query[:50]}...")
        return cached_result
    else:
        logger.info(f"Query result cache MISS: {cleaned_query[:50]}...")

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)
    logger.info(f"Using connection: connection_id={connection_id}, url_type={'excel' if _is_excel_connection(database_url) else 'database'}")

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œä½¿ç”¨ Excel æŸ¥è¯¢
    if _is_excel_connection(database_url):
        logger.info(f"Detected Excel data source, using Excel query")
        file_path = _get_excel_file_path(database_url)

        # ğŸ”¥ ä¿®å¤ï¼šä» SQL æŸ¥è¯¢ä¸­è§£æè¡¨åï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„ table_name
        # å°è¯•ä» SQL ä¸­æå–è¡¨å
        extracted_table_name = _extract_table_name_from_query(cleaned_query)

        # å¦‚æœæˆåŠŸæå–è¡¨åï¼Œåº”ç”¨è¡¨åæ˜ å°„
        if extracted_table_name:
            # ğŸ”¥ æ–°å¢ï¼šåº”ç”¨è¡¨åæ˜ å°„ï¼ˆè‹±æ–‡è¡¨å -> Excel å·¥ä½œè¡¨åï¼‰
            # ä¼ é€’ file_path ä»¥ä¾¿éªŒè¯å·¥ä½œè¡¨å­˜åœ¨å¹¶æ”¯æŒå¤‡ç”¨åç§°å›é€€
            mapped_sheet = _get_excel_sheet_mapping(extracted_table_name, file_path)
            if mapped_sheet:
                sheet_name = mapped_sheet
                logger.info(f"è¡¨åæ˜ å°„: {extracted_table_name} -> {sheet_name}")
            else:
                sheet_name = extracted_table_name
                logger.info(f"ä½¿ç”¨æå–çš„è¡¨åï¼ˆæ— æ˜ å°„ï¼‰: '{sheet_name}'")
        else:
            # å›é€€åˆ° connection_info.table_name æˆ–é»˜è®¤å€¼
            sheet_name = connection_info.table_name if connection_info else None
            logger.warning(f"æ— æ³•ä» SQL æå–è¡¨åï¼Œä½¿ç”¨é»˜è®¤å€¼: '{sheet_name}'")

        result = execute_excel_query(cleaned_query, file_path, sheet_name)

        # å­˜å‚¨åˆ°ç¼“å­˜
        _query_cache.set(cache_key, result)
        return result

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    # æŸ¥è¯¢ç»“æœå’Œé”™è¯¯å®¹å™¨
    result_container = {"result": None, "error": None}

    def execute_with_timeout():
        """åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­æ‰§è¡ŒæŸ¥è¯¢"""
        try:
            # åˆ›å»ºæ•°æ®åº“è¿æ¥
            conn = create_db_connection(database_url)
            cursor = conn.cursor()

            # è®¾ç½®è¯­å¥è¶…æ—¶ï¼ˆPostgreSQL statement_timeoutï¼‰
            # SQLite ä¸æ”¯æŒæ­¤è¯­å¥ï¼Œéœ€è¦è·³è¿‡
            if not _is_sqlite_connection(database_url):
                cursor.execute("SET statement_timeout = 30000")  # 30ç§’

            # æ‰§è¡ŒæŸ¥è¯¢
            cursor.execute(cleaned_query)

            # è·å–ç»“æœ
            columns = [desc[0] for desc in cursor.description] if cursor.description else []
            raw_rows = cursor.fetchall()

            # ğŸ”§ ä½¿ç”¨åºåˆ—åŒ–å‡½æ•°å¤„ç†æ‰€æœ‰æ•°æ®ç±»å‹ï¼ˆPostgreSQL Decimal, UUID, datetime ç­‰ï¼‰
            rows = _serialize_rows(raw_rows, columns)

            # å…³é—­è¿æ¥
            cursor.close()
            conn.close()

            # æ„å»ºç»“æœ
            result_container["result"] = {
                "columns": columns,
                "rows": rows,
                "row_count": len(rows),
                "success": True
            }

            logger.info(f"Query executed successfully: {len(rows)} rows returned")

        except Exception as e:
            result_container["error"] = e

    # ä½¿ç”¨çº¿ç¨‹æ‰§è¡ŒæŸ¥è¯¢ï¼Œ30ç§’è¶…æ—¶
    query_thread = threading.Thread(target=execute_with_timeout)
    query_thread.daemon = True
    query_thread.start()
    query_thread.join(timeout=30)

    # æ£€æŸ¥ç»“æœ
    if query_thread.is_alive():
        # æŸ¥è¯¢è¶…æ—¶
        logger.error("Query execution timeout (30s)")
        return json.dumps({
            "error": "Query execution timeout after 30 seconds",
            "error_type": "timeout_error",
            "query": cleaned_query[:100]
        }, ensure_ascii=False)

    if result_container["error"] is not None:
        error = result_container["error"]
        logger.error(f"Query execution error: {error}")
        logger.error(f"Failed query: {cleaned_query[:200]}")
        return json.dumps({
            "error": str(error),
            "error_type": "execution_error",
            "query": cleaned_query[:100],  # æˆªæ–­æŸ¥è¯¢
            "suggestion": get_query_suggestion(str(error), cleaned_query)
        }, ensure_ascii=False)

    if result_container["result"] is not None:
        result_json = json.dumps(result_container["result"], ensure_ascii=False, default=str)

        # å­˜å‚¨åˆ°ç¼“å­˜ (ç¼“å­˜ 5 åˆ†é’Ÿ)
        _query_cache.set(cache_key, result_json)
        logger.info(f"Query result cached: {cleaned_query[:50]}...")

        return result_json

    # æœªçŸ¥çš„é”™è¯¯
    return json.dumps({
        "error": "Unknown error during query execution",
        "error_type": "unknown_error"
    }, ensure_ascii=False)


def _remove_llm_added_tenant_id(query: str) -> str:
    """
    ç§»é™¤ LLM æ‰‹åŠ¨æ·»åŠ çš„ tenant_id æ¡ä»¶

    LLM æœ‰æ—¶ä¼šåœ¨ SQL ä¸­æ‰‹åŠ¨æ·»åŠ  tenant_id è¿‡æ»¤æ¡ä»¶ï¼Œä½†ä½ç½®å¯èƒ½ä¸æ­£ç¡®
    ï¼ˆå¦‚åœ¨ GROUP BY/ORDER BY ä¹‹åï¼‰ã€‚è¿™ä¸ªå‡½æ•°ä¼šç§»é™¤æ‰€æœ‰ LLM æ‰‹åŠ¨æ·»åŠ çš„
    tenant_id æ¡ä»¶ï¼Œè®©ç§Ÿæˆ·éš”ç¦»ä¸­é—´ä»¶åœ¨æ­£ç¡®çš„ä½ç½®é‡æ–°æ³¨å…¥ã€‚

    Args:
        query: åŸå§‹ SQL æŸ¥è¯¢

    Returns:
        ç§»é™¤ LLM æ·»åŠ çš„ tenant_id åçš„ SQL
    """
    import re

    sql = query.strip()
    original_sql = sql

    # æ¨¡å¼ 1: ç§»é™¤ WHERE tenant_id = 'xxx' ï¼ˆä½œä¸ºç‹¬ç«‹ WHERE æ¡ä»¶ï¼‰
    # åŒ¹é…: WHERE tenant_id = 'xxx' åé¢è·Ÿç€å…¶ä»–å†…å®¹æˆ–ç»“æŸ
    pattern1 = r'\bWHERE\s+tenant_id\s*=\s*\'[^\']*\'(\s+|$)'
    if re.search(pattern1, sql, re.IGNORECASE):
        # å¦‚æœ WHERE å­å¥åªæœ‰ tenant_idï¼Œç›´æ¥ç§»é™¤æ•´ä¸ª WHERE
        sql = re.sub(
            r'\bWHERE\s+tenant_id\s*=\s*\'[^\']*\'(\s*(?:GROUP BY|ORDER BY|LIMIT|HAVING|;|$))?',
            lambda m: '' if not m.group(1) or m.group(1).strip() in ('GROUP BY', 'ORDER BY', 'LIMIT', 'HAVING', ';') else ' AND ',
            sql,
            flags=re.IGNORECASE
        )
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ AND
        sql = re.sub(r'\bAND\s+(GROUP BY|ORDER BY|LIMIT|HAVING)', r'\1', sql, flags=re.IGNORECASE)

    # æ¨¡å¼ 2: ç§»é™¤ AND tenant_id = 'xxx' ï¼ˆä½œä¸º AND æ¡ä»¶ï¼‰
    sql = re.sub(
        r'\bAND\s+tenant_id\s*=\s*\'[^\']*\'(\s+|$)',
        '',
        sql,
        flags=re.IGNORECASE
    )

    # æ¨¡å¼ 3: ç§»é™¤ OR tenant_id = 'xxx' ï¼ˆä½œä¸º OR æ¡ä»¶ï¼‰
    sql = re.sub(
        r'\bOR\s+tenant_id\s*=\s*\'[^\']*\'(\s+|$)',
        '',
        sql,
        flags=re.IGNORECASE
    )

    # æ¨¡å¼ 4: ç§»é™¤ WHERE å­å¥ä¸­é—´çš„ tenant_id æ¡ä»¶
    # ä¾‹å¦‚: WHERE status = 'active' AND tenant_id = 'xxx' AND other = 'value'
    # å˜æˆ: WHERE status = 'active' AND other = 'value'
    sql = re.sub(
        r'\bAND\s+tenant_id\s*=\s*\'[^\']*\'(\s+(AND|OR))?',
        r'\1',
        sql,
        flags=re.IGNORECASE
    )
    sql = re.sub(
        r'\bAND\s+tenant_id\s*=\s*\'[^\']*\'\s*,',
        ',',
        sql,
        flags=re.IGNORECASE
    )

    # æ¨¡å¼ 5: å¤„ç† WHERE å¼€å¤´å°±æ˜¯ tenant_id çš„æƒ…å†µ
    # ä¾‹å¦‚: WHERE tenant_id = 'xxx' AND status = 'active'
    # å˜æˆ: WHERE status = 'active'
    sql = re.sub(
        r"\bWHERE\s+tenant_id\s*=\s*'[^']*'\s+AND\s+",
        'WHERE ',
        sql,
        flags=re.IGNORECASE
    )

    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    sql = ' '.join(sql.split())

    if sql != original_sql and 'tenant_id' in original_sql.lower():
        logger.info(f"Removed LLM-added tenant_id conditions")

    return sql


def clean_and_validate_sql(query: str) -> str:
    """
    æ¸…ç†å’ŒéªŒè¯ SQL æŸ¥è¯¢

    ä¿®å¤å¸¸è§çš„ LLM ç”Ÿæˆé”™è¯¯ï¼š
    - LIMIT å­å¥åçš„é”™è¯¯å†…å®¹ï¼ˆWHERE, AND, OR ç­‰ï¼‰
    - tenants è¡¨çš„ tenant_id åˆ—é”™è¯¯ï¼ˆåº”ä½¿ç”¨ idï¼‰
    - ORDER BY/GROUP BY åé¢é”™è¯¯è·Ÿ AND/OR æ¡ä»¶
    - WHERE å­å¥ä½ç½®é”™è¯¯ï¼ˆåœ¨ GROUP BY/ORDER BY ä¹‹åï¼‰
    - å¤šä½™çš„åˆ†å·
    - ä¸å®Œæ•´çš„æŸ¥è¯¢

    v4.3.0 ä¼˜åŒ–ï¼šæ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•ï¼Œæ–¹ä¾¿è°ƒè¯• SQL æ¸…ç†è¿‡ç¨‹

    Args:
        query: åŸå§‹ SQL æŸ¥è¯¢

    Returns:
        æ¸…ç†åçš„ SQL æŸ¥è¯¢
    """
    import re

    # ç§»é™¤å‰åç©ºæ ¼
    sql = query.strip()
    original_sql = sql  # ä¿å­˜åŸå§‹ SQL ç”¨äºæ—¥å¿—æ¯”è¾ƒ

    # ğŸ“Š è¯¦ç»†æ—¥å¿—ï¼šè®°å½•è¾“å…¥çš„ SQL
    logger.debug(f"[SQL_CLEAN] Input SQL: {sql[:200]}...")

    # ä¿®å¤ 0: tenants è¡¨çš„ tenant_id â†’ id è‡ªåŠ¨æ›¿æ¢ï¼ˆå¸¸è§é”™è¯¯ï¼‰
    # åŒ¹é… FROM tenants ... WHERE tenant_id æˆ– JOIN tenants ... WHERE tenant_id
    if re.search(r'\btenants\b', sql, re.IGNORECASE):
        # æ›¿æ¢ tenants è¡¨ä¸Šçš„ tenant_id ä¸º id
        # åŒ¹é… WHERE tenant_id = æˆ– AND tenant_id = ç­‰
        sql = re.sub(
            r'(WHERE|AND|OR)\s+tenant_id\s*=',
            r'\1 id =',
            sql,
            flags=re.IGNORECASE
        )
        # å¦‚æœä¿®æ”¹äº† SQLï¼Œè®°å½•æ—¥å¿—
        if 'tenant_id' in original_sql.lower() and 'tenant_id' not in sql.lower():
            logger.info("Auto-fixed: tenants.tenant_id â†’ tenants.id")

    # ä¿®å¤ 1: ç§»é™¤ ORDER BY/GROUP BY åé¢é”™è¯¯è·Ÿçš„ AND/OR/WHERE æ¡ä»¶
    # é”™è¯¯ç¤ºä¾‹: SELECT ... ORDER BY year AND tenant_id = '...'
    # é”™è¯¯ç¤ºä¾‹: SELECT ... ORDER BY year WHERE tenant_id = '...'
    # é”™è¯¯ç¤ºä¾‹: SELECT ... GROUP BY year OR tenant_id = '...'
    # é”™è¯¯ç¤ºä¾‹: SELECT ... GROUP BY col WHERE tenant_id = '...'
    for keyword in ['ORDER BY', 'GROUP BY']:
        # åŒ¹é…: keyword + å­—æ®µå (+ å¯é€‰ ASC/DESC) + ç©ºç™½ + (AND|OR|WHERE)
        # æ¨¡å¼: å…³é”®å­— + ç©ºç™½ + å­—æ®µå (+ å¯é€‰ ASC/DESC) + ç©ºç™½ + (AND|OR|WHERE)
        pattern = rf'\b{keyword}\s+([^\s]+(?:\s+(?:ASC|DESC))?)\s+(AND|OR|WHERE)\b'
        match = re.search(pattern, sql, re.IGNORECASE)
        if match:
            # æ‰¾åˆ° AND/OR/WHERE çš„ä½ç½®
            clause_start = match.start(2)
            # æˆªæ–­åˆ° AND/OR/WHERE ä¹‹å‰
            before_clause = sql[:clause_start].rstrip()
            remaining = sql[clause_start:]
            # ç§»é™¤é”™è¯¯æ¡ä»¶åˆ°ä¸‹ä¸€ä¸ªå…³é”®å­—æˆ–ç»“å°¾
            # æ‰¾åˆ° AND/OR/WHERE åé¢çš„ä¸‹ä¸€ä¸ªå­å¥ï¼ˆLIMIT, HAVING, ;ï¼‰
            next_clause_match = re.search(r'\b(LIMIT|HAVING|;)\b', remaining, re.IGNORECASE)
            if next_clause_match:
                # ä¿ç•™ LIMIT ç­‰å­å¥
                after_clause = remaining[next_clause_match.start():]
                sql = before_clause + after_clause
            else:
                # æ²¡æœ‰å…¶ä»–å­å¥ï¼Œç›´æ¥æˆªæ–­
                sql = before_clause
            logger.info(f"Removed incorrect {match.group(2)} clause after {keyword}: {match.group(0)[:50]}...")

    # ä¿®å¤ 2: æ£€æµ‹å¹¶ä¿®å¤ WHERE å­å¥ä½ç½®é”™è¯¯
    # é”™è¯¯ç¤ºä¾‹: SELECT ... GROUP BY year ORDER BY year WHERE tenant_id = '...'
    # WHERE å¿…é¡»åœ¨ GROUP BY/ORDER BY ä¹‹å‰
    #
    # ä½¿ç”¨åŸºäºä½ç½®çš„è§£ææ–¹æ³•ï¼š
    # 1. æ‰¾åˆ°æ‰€æœ‰å…³é”®å­—ä½ç½®
    # 2. æ£€æŸ¥ WHERE æ˜¯å¦åœ¨ GROUP BY/ORDER BY ä¹‹å
    # 3. å¦‚æœæ˜¯ï¼Œé‡æ–°æ’åˆ— SQL
    keywords_found = {}
    for kw in ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT', 'HAVING']:
        match = re.search(rf'\b{kw}\b', sql, re.IGNORECASE)
        if match:
            keywords_found[kw] = {'start': match.start(), 'end': match.end()}

    # å¦‚æœå­˜åœ¨ WHERE å’Œ (GROUP BY æˆ– ORDER BY)
    if 'WHERE' in keywords_found and ('GROUP BY' in keywords_found or 'ORDER BY' in keywords_found):
        where_pos = keywords_found['WHERE']['start']
        group_pos = keywords_found.get('GROUP BY', {'start': float('inf')})['start']
        order_pos = keywords_found.get('ORDER BY', {'start': float('inf')})['start']

        # æ£€æŸ¥ WHERE æ˜¯å¦åœ¨ GROUP BY æˆ– ORDER BY ä¹‹å
        if where_pos > group_pos or where_pos > order_pos:
            logger.info("Detected WHERE after GROUP BY/ORDER BY, fixing...")

            # ä½¿ç”¨åŸºäºä½ç½®çš„å­å¥æå–ï¼ˆæ›´å¯é ï¼‰
            # æ‰¾å‡ºæ‰€æœ‰å­å¥çš„èµ·å§‹å’Œç»“æŸä½ç½®
            clauses = {}
            for kw in ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT']:
                if kw in keywords_found:
                    start = keywords_found[kw]['start']
                    kw_end = keywords_found[kw]['end']
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå…³é”®å­—çš„èµ·å§‹ä½ç½®ä½œä¸ºå½“å‰å­å¥çš„ç»“æŸä½ç½®
                    next_start = float('inf')
                    for other_kw in ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT', 'HAVING']:
                        if other_kw in keywords_found and keywords_found[other_kw]['start'] > kw_end:
                            next_start = min(next_start, keywords_found[other_kw]['start'])
                    # å¦‚æœæ²¡æœ‰ä¸‹ä¸€ä¸ªå…³é”®å­—ï¼Œä½¿ç”¨åˆ†å·ä½ç½®æˆ–å­—ç¬¦ä¸²ç»“å°¾
                    if next_start == float('inf'):
                        semicolon_pos = sql.rfind(';')
                        if semicolon_pos > kw_end:
                            next_start = semicolon_pos
                        else:
                            next_start = len(sql)
                    clauses[kw] = sql[start:next_start].strip()

            # é‡æ–°æ„å»º SQLï¼ˆæ­£ç¡®é¡ºåºï¼‰
            new_sql_parts = []
            order = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT']
            for kw in order:
                if kw in clauses:
                    new_sql_parts.append(clauses[kw])

            # ç»„åˆæˆæ–°çš„ SQL
            if new_sql_parts:
                sql = ' '.join(new_sql_parts)
                # ç¡®ä¿ä»¥åˆ†å·ç»“å°¾
                if not sql.endswith(';'):
                    sql += ';'
                logger.info(f"Rebuilt SQL with correct clause order: {sql[:80]}...")

    # ä¿®å¤ 3: ç§»é™¤ LIMIT åé¢çš„ä»»ä½•å†…å®¹ï¼ˆLLM å¸¸è§é”™è¯¯ï¼‰
    # åŒ¹é… LIMIT å­å¥ï¼Œç„¶åæˆªæ–­ï¼Œç§»é™¤åé¢çš„ WHERE, AND, OR ç­‰
    # è¿™ä¸ªæ­£åˆ™åŒ¹é… LIMIT æ•°å­—ï¼Œç„¶ååé¢ä¸èƒ½æœ‰ WHERE/AND/OR/GROUP/ORDER/HAVING
    limit_pattern = r'\bLIMIT\s+(\d+)'
    match = re.search(limit_pattern, sql, re.IGNORECASE)
    if match:
        # æ‰¾åˆ° LIMIT å­å¥ï¼Œæˆªæ–­åˆ°æ•°å­—ç»“æŸçš„åœ°æ–¹
        # éœ€è¦æ‰¾åˆ° LIMIT åé¢çš„æ•°å­—
        limit_end = match.end()
        # æ£€æŸ¥ LIMIT åé¢æ˜¯å¦è¿˜æœ‰å…¶ä»–å­å¥ï¼ˆWHERE, AND, OR, GROUP BY, HAVING ç­‰ï¼‰
        # å¦‚æœæœ‰ï¼Œæˆªæ–­å®ƒä»¬
        remaining_sql = sql[limit_end:].strip()
        # å¦‚æœå‰©ä½™å†…å®¹ä»¥ WHERE, AND, OR, GROUP, HAVING å¼€å¤´ï¼Œè¯´æ˜æ˜¯é”™è¯¯çš„
        if re.match(r'^(WHERE|AND|OR|GROUP BY|HAVING)', remaining_sql, re.IGNORECASE):
            # æˆªæ–­åˆ° LIMIT æ•°å­—ç»“æŸçš„åœ°æ–¹
            sql = sql[:limit_end].rstrip()
            logger.info(f"Removed content after LIMIT: {remaining_sql[:50]}...")

    # ä¿®å¤ 4: ç§»é™¤æœ«å°¾çš„åˆ†å·ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
    sql = re.sub(r';+$', '', sql)

    # ä¿®å¤ 5: ç¡®ä¿æŸ¥è¯¢ä»¥åˆ†å·ç»“å°¾ï¼ˆå¯¹äºå•æ¡æŸ¥è¯¢ï¼‰
    if not sql.endswith(';'):
        sql += ';'

    # ä¿®å¤ 6: ç§»é™¤æ³¨é‡Šåçš„å±é™©å‘½ä»¤ï¼ˆé¢å¤–å®‰å…¨æ£€æŸ¥ï¼‰
    # ç§»é™¤ -- åé¢çš„å†…å®¹åˆ°è¡Œå°¾
    sql = re.sub(r'--.*$', '', sql, flags=re.MULTILINE)
    # ç§»é™¤ /* */ å—æ³¨é‡Š
    sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)

    # ä¿®å¤ 7: æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    sql = ' '.join(sql.split())

    return sql


def _extract_table_name_from_query(query: str) -> Optional[str]:
    """
    ä» SQL æŸ¥è¯¢ä¸­æå– FROM å­å¥çš„è¡¨å

    æ”¯æŒç®€å•çš„ SELECT æŸ¥è¯¢ï¼Œæå–ç¬¬ä¸€ä¸ª FROM åé¢çš„è¡¨åã€‚
    æ”¯æŒå¸¦emojiã€ä¸­æ–‡ã€ç‰¹æ®Šå­—ç¬¦çš„è¡¨åã€‚

    Args:
        query: SQL æŸ¥è¯¢è¯­å¥

    Returns:
        æå–çš„è¡¨åï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› None
    """
    import re

    try:
        # ç§»é™¤æ³¨é‡Šå’Œæ¢è¡Œç¬¦ï¼Œç®€åŒ–è§£æ
        cleaned = re.sub(r'--.*$', '', query, flags=re.MULTILINE)
        cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)
        cleaned = ' '.join(cleaned.split())

        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆåŒ¹é…åŒå¼•å·å†…çš„è¡¨åï¼ˆæ”¯æŒemojiã€ä¸­æ–‡ã€ç©ºæ ¼ï¼‰
        # æ¨¡å¼ï¼šFROM "table name" æˆ– FROM "è¡¨å"
        double_quote_pattern = r'\bFROM\s+"([^"]+)"'
        match = re.search(double_quote_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (double-quoted) from query: '{table_name}'")
            return table_name

        # ğŸ”§ åŒ¹é…å•å¼•å·å†…çš„è¡¨å
        single_quote_pattern = r"\bFROM\s+'([^']+)'"
        match = re.search(single_quote_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (single-quoted) from query: '{table_name}'")
            return table_name

        # ğŸ”§ æœ€åï¼šåŒ¹é…ä¸å¸¦å¼•å·çš„è¡¨åï¼ˆæ”¯æŒemojiå’Œä¸­æ–‡ï¼Œä½†ä¸èƒ½æœ‰ç©ºæ ¼ï¼‰
        # ä½¿ç”¨ Unicode å±æ€§åŒ¹é…ä»»ä½•å•è¯å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€emojiç­‰ï¼‰
        unquoted_pattern = r'\bFROM\s+([^\s;]+?)(?:\s+WHERE|\s+ORDER|\s+GROUP|\s+LIMIT|\s+HAVING|;|$)'
        match = re.search(unquoted_pattern, cleaned, re.IGNORECASE)
        if match:
            table_name = match.group(1)
            logger.info(f"Extracted table name (unquoted) from query: '{table_name}'")
            return table_name

        logger.warning(f"Could not extract table name from query: {query[:100]}...")
        return None
    except Exception as e:
        logger.warning(f"Error extracting table name: {e}")
        return None


def get_query_suggestion(error_msg: str, query: str) -> Optional[str]:
    """
    æ ¹æ®é”™è¯¯ä¿¡æ¯æä¾›æŸ¥è¯¢å»ºè®®

    Args:
        error_msg: é”™è¯¯ä¿¡æ¯
        query: åŸå§‹æŸ¥è¯¢

    Returns:
        å»ºè®®ä¿¡æ¯
    """
    import re

    error_lower = error_msg.lower()

    if 'limit' in error_lower and 'and' in error_lower:
        return "LIMIT å­å¥åä¸åº”æœ‰ AND æ¡ä»¶ã€‚è¯·å°† LIMIT æ”¾åœ¨æŸ¥è¯¢æœ€åã€‚"

    if 'column' in error_lower and 'does not exist' in error_lower:
        # æå–ä¸å­˜åœ¨çš„åˆ—å
        match = re.search(r'column "(.*?)" does not exist', error_msg)
        if match:
            col = match.group(1)
            return f"åˆ— '{col}' ä¸å­˜åœ¨ã€‚è¯·ä½¿ç”¨ list_tables å’Œ get_schema æŸ¥çœ‹å¯ç”¨çš„è¡¨å’Œåˆ—ã€‚"

    if 'relation' in error_lower and 'does not exist' in error_lower:
        match = re.search(r'relation "(.*?)" does not exist', error_msg)
        if match:
            table = match.group(1)
            return (f"ğŸš¨ è¡¨ '{table}' ä¸å­˜åœ¨ï¼\n\n"
                    f"ğŸ”´ å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤é‡è¯•ï¼š\n"
                    f"1. ç«‹å³è°ƒç”¨ list_tables() æŸ¥çœ‹æ‰€æœ‰å¯ç”¨è¡¨\n"
                    f"2. æ ¹æ®ä¸šåŠ¡è¯­ä¹‰é€‰æ‹©ç›¸å…³è¡¨ï¼ˆä¾‹å¦‚ï¼šsalesâ†’è®¢å•è¡¨/ğŸ“Šæœˆåº¦é”€å”®æ±‡æ€»ï¼‰\n"
                    f"3. ä½¿ç”¨ list_tables() è¿”å›çš„ç¡®åˆ‡è¡¨åé‡æ–°æŸ¥è¯¢\n\n"
                    f"ğŸ“‹ ä¸šåŠ¡æœ¯è¯­æ˜ å°„ï¼š\n"
                    f"â€¢ é”€å”®/é”€å”®é¢ â†’ è®¢å•è¡¨ã€è®¢å•æ˜ç»†ã€æœˆåº¦é”€å”®æ±‡æ€»ã€ğŸ“Šæœˆåº¦é”€å”®æ±‡æ€»\n"
                    f"â€¢ å®¢æˆ·/ç”¨æˆ· â†’ ç”¨æˆ·è¡¨ã€å®¢æˆ·è¡¨ã€å®¢æˆ·æ¶ˆè´¹æ’è¡Œ\n"
                    f"â€¢ äº§å“/å•†å“ â†’ äº§å“è¡¨ã€å•†å“è¡¨\n"
                    f"â€¢ è®¢å• â†’ è®¢å•è¡¨ã€è®¢å•æ˜ç»†")

    if 'syntax error' in error_lower:
        return "SQL è¯­æ³•é”™è¯¯ã€‚è¯·ç¡®ä¿æŸ¥è¯¢æ ¼å¼æ­£ç¡®ï¼Œå»ºè®®ä½¿ç”¨ç®€å•çš„ SELECT è¯­å¥ã€‚"

    return None


# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def list_tables(connection_id: Optional[str] = None) -> str:
    """
    åˆ—å‡ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨æˆ– Excel æ–‡ä»¶ä¸­çš„æ‰€æœ‰å·¥ä½œè¡¨

    Args:
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        è¡¨åˆ—è¡¨çš„ JSON å­—ç¬¦ä¸²

    Example:
        >>> list_tables()
        '{"tables": ["users", "orders", "products"], "table_count": 3}'
    """
    import json

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    logger.info(f"list_tables: connection_id={connection_id}")

    # æ¸…é™¤æ—§ç¼“å­˜ä»¥ç¡®ä¿ä½¿ç”¨æ–°çš„è¿æ¥ä¿¡æ¯
    if connection_id:
        _schema_cache.clear()

    # æ£€æŸ¥ç¼“å­˜
    cache_key = _make_cache_key("list_tables", connection_id)
    cached = _schema_cache.get(cache_key)
    if cached is not None:
        logger.info("list_tables: è¿”å›ç¼“å­˜ç»“æœ")
        return cached

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œè¿”å›å·¥ä½œè¡¨åˆ—è¡¨
    if _is_excel_connection(database_url):
        logger.info("list_tables: Detected Excel data source, listing sheets")
        try:
            file_path = _get_excel_file_path(database_url)
            import pandas as pd

            excel_file = pd.ExcelFile(file_path, engine='openpyxl')
            sheets = excel_file.sheet_names

            result = {
                "tables": sheets,
                "table_count": len(sheets),
                "success": True,
                "data_source": "excel",
                "file_path": file_path
            }

            result_str = json.dumps(result, ensure_ascii=False)
            _schema_cache.set(cache_key, result_str)
            logger.info(f"list_tables: Excel æ–‡ä»¶ï¼Œ{len(sheets)} ä¸ªå·¥ä½œè¡¨: {sheets}")

            # ğŸ”§ è®¾ç½®æ ‡å¿—ï¼Œè¡¨ç¤º list_tables å·²è¢«è°ƒç”¨
            _set_list_tables_called(True)

            return result_str

        except Exception as e:
            logger.error(f"List Excel sheets error: {e}")
            error_str = json.dumps({
                "error": str(e),
                "error_type": "execution_error"
            }, ensure_ascii=False)
            return error_str

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        conn = create_db_connection(database_url)
        cursor = conn.cursor()

        # PostgreSQL æŸ¥è¯¢æ‰€æœ‰è¡¨
        cursor.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_type = 'BASE TABLE'
            ORDER BY table_name
        """)

        tables = [row[0] for row in cursor.fetchall()]

        cursor.close()
        conn.close()

        result = {
            "tables": tables,
            "table_count": len(tables),
            "success": True
        }

        result_str = json.dumps(result, ensure_ascii=False)
        # å­˜å…¥ç¼“å­˜
        _schema_cache.set(cache_key, result_str)
        logger.info(f"list_tables: æŸ¥è¯¢æˆåŠŸï¼Œç¼“å­˜ç»“æœ ({len(tables)} ä¸ªè¡¨)")

        # ğŸ”§ è®¾ç½®æ ‡å¿—ï¼Œè¡¨ç¤º list_tables å·²è¢«è°ƒç”¨
        _set_list_tables_called(True)

        return result_str

    except Exception as e:
        logger.error(f"List tables error: {e}")
        error_str = json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)
        return error_str


# æ³¨æ„ï¼šè¿™äº›å‡½æ•°ä¸å†ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œè€Œæ˜¯åœ¨ get_database_tools ä¸­æ‰‹åŠ¨åˆ›å»º StructuredTool
def get_schema(table_name: str, connection_id: Optional[str] = None) -> str:
    """
    è·å–è¡¨çš„ç»“æ„ä¿¡æ¯æˆ– Excel å·¥ä½œè¡¨çš„åˆ—ä¿¡æ¯

    Args:
        table_name: è¡¨åæˆ–å·¥ä½œè¡¨å
        connection_id: æ•°æ®æºè¿æ¥ ID (å¯é€‰)

    Returns:
        è¡¨ç»“æ„çš„ JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«åˆ—åã€ç±»å‹ã€æ˜¯å¦å¯ç©ºç­‰ä¿¡æ¯

    Example:
        >>> get_schema("users")
        '{"table_name": "users", "columns": [{"name": "id", "type": "integer", "nullable": false}], "column_count": 1}'
    """
    import json

    # ä» thread-local è·å– connection_idï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
    if connection_id is None:
        connection_id, _, _ = _get_connection_context()

    # æ£€æŸ¥ç¼“å­˜
    cache_key = _make_cache_key("get_schema", table_name, connection_id)
    cached = _schema_cache.get(cache_key)
    if cached is not None:
        logger.info(f"get_schema({table_name}): è¿”å›ç¼“å­˜ç»“æœ")
        return cached

    # è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
    database_url, connection_info = get_database_url(connection_id)

    # å¦‚æœæ˜¯ Excel è¿æ¥ï¼Œè¿”å›åˆ—ä¿¡æ¯
    if _is_excel_connection(database_url):
        logger.info(f"get_schema({table_name}): Detected Excel data source, getting columns")
        try:
            file_path = _get_excel_file_path(database_url)
            import pandas as pd

            # è¯»å– Excel å·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=table_name, engine='openpyxl', nrows=0)

            # è·å–åˆ—ä¿¡æ¯
            columns = []
            for col in df.columns:
                # æ¨æ–­æ•°æ®ç±»å‹
                dtype_str = str(df[col].dtype)
                # ç®€åŒ–ç±»å‹åç§°
                if dtype_str.startswith('int'):
                    col_type = 'integer'
                elif dtype_str.startswith('float'):
                    col_type = 'float'
                elif dtype_str == 'bool':
                    col_type = 'boolean'
                elif dtype_str.startswith('datetime'):
                    col_type = 'datetime'
                else:
                    col_type = 'text'

                columns.append({
                    "name": col,
                    "type": col_type,
                    "nullable": True  # Excel ä¸å¼ºåˆ¶ NOT NULL
                })

            result = {
                "table_name": table_name,
                "columns": columns,
                "column_count": len(columns),
                "success": True,
                "data_source": "excel"
            }

            result_str = json.dumps(result, ensure_ascii=False)
            _schema_cache.set(cache_key, result_str)
            logger.info(f"get_schema({table_name}): Excel å·¥ä½œè¡¨ï¼Œ{len(columns)} åˆ—")

            return result_str

        except Exception as e:
            logger.error(f"Get Excel schema error: {e}")
            error_str = json.dumps({
                "error": str(e),
                "error_type": "execution_error"
            }, ensure_ascii=False)
            return error_str

    # æ•°æ®åº“æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        conn = create_db_connection(database_url)
        cursor = conn.cursor()

        # PostgreSQL æŸ¥è¯¢è¡¨ç»“æ„
        cursor.execute("""
            SELECT
                column_name,
                data_type,
                is_nullable,
                column_default
            FROM information_schema.columns
            WHERE table_name = %s
            AND table_schema = 'public'
            ORDER BY ordinal_position
        """, (table_name,))

        columns = []
        for row in cursor.fetchall():
            columns.append({
                "name": row[0],
                "type": row[1],
                "nullable": row[2] == "YES",
                "default": row[3]
            })

        cursor.close()
        conn.close()

        result = {
            "table_name": table_name,
            "columns": columns,
            "column_count": len(columns),
            "success": True
        }

        result_str = json.dumps(result, ensure_ascii=False)
        # å­˜å…¥ç¼“å­˜
        _schema_cache.set(cache_key, result_str)
        logger.info(f"get_schema({table_name}): æŸ¥è¯¢æˆåŠŸï¼Œç¼“å­˜ç»“æœ ({len(columns)} åˆ—)")

        return result_str

    except Exception as e:
        logger.error(f"Get schema error: {e}")
        error_str = json.dumps({
            "error": str(e),
            "error_type": "execution_error"
        }, ensure_ascii=False)
        return error_str


# ============================================================================
# è¯­ä¹‰å±‚æ–‡ä»¶ç³»ç»Ÿå·¥å…· (Schema FS Tools)
# ============================================================================

from pathlib import Path

class SchemaFSValidator:
    """cube_schema æ–‡ä»¶ç³»ç»Ÿè®¿é—®éªŒè¯å™¨"""
    
    # æŒ‡å‘ c:\data_agent\cube_schema
    # è®¡ç®—æ–¹å¼ï¼šå½“å‰æ–‡ä»¶ (c:\data_agent\AgentV2\tools\database_tools.py) çš„ä¸Šçº§(tools)çš„ä¸Šçº§(AgentV2)çš„ä¸Šçº§(data_agent) / "cube_schema"
    ALLOWED_BASE_PATH = Path(__file__).parent.parent.parent / "cube_schema"

    @classmethod
    def validate_path(cls, path: Path) -> bool:
        """ä¸¥æ ¼éªŒè¯è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»"""
        try:
            resolved = path.resolve()
            base = cls.ALLOWED_BASE_PATH.resolve()
            # æ£€æŸ¥ resolved æ˜¯å¦ä»¥ base å¼€å¤´
            return str(resolved).startswith(str(base))
        except (ValueError, RuntimeError, Exception) as e:
            logger.warning(f"Path validation failed: {e}")
            return False

    @classmethod
    def sanitize_content(cls, content: str, max_length: int = 5000) -> str:
        """é™åˆ¶è¿”å›å†…å®¹å¤§å°ï¼Œé¿å… Token çˆ†ç‚¸"""
        if len(content) > max_length:
            return content[:max_length] + "\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
        return content

def list_schema_files() -> str:
    """
    åˆ—å‡º cube_schema ç›®å½•ä¸‹å¯ç”¨çš„è¯­ä¹‰å±‚æ–‡æ¡£

    è¿”å›æ ¼å¼ï¼šJSON æ•°ç»„ï¼ŒåŒ…å« filename, description, measures, dimensions

    ä½¿ç”¨åœºæ™¯ï¼š
    - å›ç­”"æ•°æ®åº“æœ‰å“ªäº›è¡¨ï¼Ÿ"
    - äº†è§£æ•°æ®ç»“æ„æ¦‚è§ˆ
    """
    base_path = SchemaFSValidator.ALLOWED_BASE_PATH

    if not base_path.exists():
        logger.warning(f"Schema directory not found: {base_path}")
        return json.dumps({"error": "Schema directory not found"}, ensure_ascii=False)

    files_info = []
    try:
        # æŸ¥æ‰¾ yaml æ–‡ä»¶
        for yaml_file in sorted(base_path.glob("*.yaml")):
            files_info.append({
                "filename": yaml_file.name,
                "size": yaml_file.stat().st_size,
                "modified": time.ctime(yaml_file.stat().st_mtime)
            })
        
        # æŸ¥æ‰¾ markdown æ–‡æ¡£
        for md_file in sorted(base_path.glob("*.md")):
            files_info.append({
                "filename": md_file.name,
                "size": md_file.stat().st_size,
                "modified": time.ctime(md_file.stat().st_mtime)
            })
            
    except Exception as e:
        logger.error(f"Error listing schema files: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)

    return json.dumps(files_info, ensure_ascii=False, indent=2)


def read_schema_file(filename: str, section: Optional[str] = None) -> str:
    """
    è¯»å– cube_schema ä¸­æŒ‡å®šæ–‡ä»¶çš„å†…å®¹

    å‚æ•°ï¼š
    - filename: æ–‡ä»¶åï¼ˆå¦‚ "Orders.yaml"ï¼‰
    - section: å¯é€‰ï¼Œåªè¯»å–ç‰¹å®šéƒ¨åˆ†ï¼ˆmeasures/dimensions/description/sql_tableï¼‰

    è¿”å›ï¼šæ–‡ä»¶å†…å®¹ï¼ˆå¯è¢«æˆªæ–­ä»¥æ§åˆ¶ Tokenï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - æŸ¥çœ‹ Orders è¡¨æœ‰å“ªäº›åº¦é‡
    - æŸ¥çœ‹æŸä¸ªå­—æ®µçš„æ•°æ®ç±»å‹
    - äº†è§£è¡¨ä¹‹é—´çš„å…³è”å…³ç³»
    """
    base_path = SchemaFSValidator.ALLOWED_BASE_PATH
    file_path = base_path / filename

    # å®‰å…¨éªŒè¯
    if not SchemaFSValidator.validate_path(file_path):
        return "é”™è¯¯ï¼šä¸å…è®¸è®¿é—®è¯¥æ–‡ä»¶ï¼ˆè·¯å¾„éæ³•ï¼‰"

    if not file_path.exists():
        return f"é”™è¯¯ï¼šæ–‡ä»¶ {filename} ä¸å­˜åœ¨"

    try:
        # è¯»å–å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # å¯é€‰ï¼šåªè¿”å›ç‰¹å®šéƒ¨åˆ† (é’ˆå¯¹ YAML æ–‡ä»¶)
        if section and filename.endswith('.yaml'):
            lines = content.split('\n')
            in_section = False
            filtered_lines = []
            
            # ç®€å•çš„æ–‡æœ¬åˆ†å—é€»è¾‘ (æ ¹æ®ç¼©è¿›å’Œå†’å·åˆ¤æ–­)
            found_section = False
            
            # æ‰¾åˆ°é¡¶çº§é”®çš„ç¼©è¿›æ¨¡å¼
            for line in lines:
                stripped = line.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¦éƒ¨åˆ†æ ‡é¢˜ (å¦‚ "measures:", "dimensions:")
                if stripped.startswith(f"{section}:"):
                    in_section = True
                    found_section = True
                    filtered_lines.append(line)
                    continue
                
                # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªä¸»è¦éƒ¨åˆ†ï¼ˆé¡¶çº§é”®ï¼‰ï¼Œåˆ™åœæ­¢
                # å‡è®¾é¡¶çº§é”®å¯èƒ½æ²¡æœ‰ç¼©è¿›æˆ–ç¼©è¿›å¾ˆå°‘ï¼Œè¿™é‡Œä½¿ç”¨ç®€å•çš„å¯å‘å¼
                if in_section and line and ':' in line and not line.strip().startswith('-') and not line.strip().startswith(' '):
                     # å¦‚æœè¿™è¡Œçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªæ–°çš„é¡¶çº§keyï¼ˆä¾‹å¦‚ "dimensions:"ï¼‰ï¼Œä¸”ä¸æ˜¯å½“å‰section
                     if not stripped.startswith(f"{section}:") and not line.strip().startswith('#'):
                         # è¿™æ˜¯ä¸€ä¸ªæ–°çš„é¡¶çº§ sectionï¼Œç»“æŸå½“å‰ section
                         in_section = False
                
                if in_section:
                    filtered_lines.append(line)
            
            if found_section:
                content = '\n'.join(filtered_lines)
            else:
                 # æœªæ‰¾åˆ° sectionï¼Œå¦‚æœè¯·æ±‚çš„æ˜¯å¸¸è§éƒ¨åˆ†ï¼Œæç¤ºæœªæ‰¾åˆ°
                 if section in ["measures", "dimensions", "sql_table", "joins", "description"]:
                     content = f"æœªåœ¨æ–‡ä»¶ {filename} ä¸­æ‰¾åˆ° '{section}' éƒ¨åˆ†ã€‚"

        # é™åˆ¶å¤§å°
        return SchemaFSValidator.sanitize_content(content)
        
    except Exception as e:
        logger.error(f"Error reading schema file {filename}: {e}")
        return f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"


def search_schema(keyword: str) -> str:
    """
    åœ¨æ‰€æœ‰ cube_schema æ–‡ä»¶ä¸­æœç´¢å…³é”®è¯

    å‚æ•°ï¼š
    - keyword: æœç´¢å…³é”®è¯ï¼ˆå¦‚è¡¨åã€å­—æ®µåã€åº¦é‡åï¼‰

    è¿”å›ï¼šåŒ¹é…çš„æ–‡ä»¶å’Œå†…å®¹ç‰‡æ®µ

    ä½¿ç”¨åœºæ™¯ï¼š
    - æŸ¥æ‰¾åŒ…å«"æ”¶å…¥"çš„æ‰€æœ‰åº¦é‡
    - æœç´¢"customer_id"å­—æ®µåœ¨å“ªäº›è¡¨ä¸­
    - æŸ¥æ‰¾æ‰€æœ‰ä¸"åº“å­˜"ç›¸å…³çš„ç»´åº¦
    """
    base_path = SchemaFSValidator.ALLOWED_BASE_PATH

    if not base_path.exists():
        return "é”™è¯¯ï¼šcube_schema ç›®å½•ä¸å­˜åœ¨"

    results = []
    keyword_lower = keyword.lower()

    try:
        # æœç´¢ yaml å’Œ md æ–‡ä»¶
        files = list(base_path.glob("*.yaml")) + list(base_path.glob("*.md"))
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if keyword_lower in content.lower():
                    # æå–åŒ¹é…çš„è¡Œï¼ˆä¸Šä¸‹æ–‡ï¼‰
                    lines = content.split('\n')
                    matches = []
                    for i, line in enumerate(lines):
                        if keyword_lower in line.lower():
                            # ä¸Šä¸‹æ–‡ï¼šå‰åå„2è¡Œ
                            start = max(0, i-2)
                            end = min(len(lines), i+3)
                            
                            # æ„å»ºä¸Šä¸‹æ–‡ç‰‡æ®µ
                            context_lines = []
                            for j in range(start, end):
                                prefix = "> " if j == i else "  "
                                context_lines.append(f"{prefix}{lines[j]}")
                                
                            matches.append('\n'.join(context_lines))
                            
                            # é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„åŒ¹é…æ•°ï¼Œé¿å…è¿‡å¤š
                            if len(matches) >= 3:
                                break
                    
                    if matches:
                        results.append({
                            "file": file_path.name,
                            "matches": matches
                        })
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")

        if not results:
            return f"æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„å†…å®¹"

        return json.dumps(results, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"æœç´¢å‡ºé”™: {str(e)}"


# ============================================================================
# å·¥å…·é›†åˆ
# ============================================================================

def get_database_tools(
    connection_id: Optional[str] = None,
    db_session: Optional[Any] = None,
    tenant_id: Optional[str] = None
) -> List:
    """
    è·å–æ‰€æœ‰æ•°æ®åº“å·¥å…·ï¼ˆæ”¯æŒ Excel å’Œæ•°æ®åº“ï¼‰

    Args:
        connection_id: æ•°æ®æºè¿æ¥ IDï¼ˆå¯é€‰ï¼‰
        db_session: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºæŸ¥è¯¢æ•°æ®æºé…ç½®ï¼‰
        tenant_id: ç§Ÿæˆ· ID

    Returns:
        LangChain Tool åˆ—è¡¨
    """
    # è®¾ç½®è¿æ¥ä¸Šä¸‹æ–‡ä¾›å·¥å…·ä½¿ç”¨ï¼ˆè™½ç„¶ä¹‹åä¼šä½¿ç”¨é—­åŒ…ï¼Œä½†ä¿ç•™è®¾ç½®ä»¥å…¼å®¹ï¼‰
    _set_connection_context(connection_id, db_session, tenant_id)

    # ğŸ”§ æ–°æ–¹æ¡ˆï¼šä½¿ç”¨é—­åŒ…é¢„ç»‘å®š connection_id
    # åŒæ—¶åœ¨åŒ…è£…å‡½æ•°ä¸­è®¾ç½® context å˜é‡ï¼Œç¡®ä¿ _get_connection_context() èƒ½å¤Ÿè·å–åˆ°æ­£ç¡®çš„å€¼

    from langchain_core.tools import StructuredTool

    # åˆ›å»ºåŒ…è£…å‡½æ•°ï¼Œé¢„ç»‘å®š connection_id å¹¶è®¾ç½® context
    def make_execute_query(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped(query: str) -> str:
            # è®¾ç½® context å˜é‡ï¼Œç¡®ä¿ get_database_url èƒ½å¤Ÿè·å–åˆ°æ­£ç¡®çš„å€¼
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return execute_query(query, fixed_connection_id)
        wrapped.__name__ = "execute_query"
        wrapped.__doc__ = execute_query.__doc__
        return wrapped

    def make_list_tables(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped() -> str:
            # è®¾ç½® context å˜é‡
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return list_tables(fixed_connection_id)
        wrapped.__name__ = "list_tables"
        wrapped.__doc__ = list_tables.__doc__
        return wrapped

    def make_get_schema(fixed_connection_id, fixed_db_session, fixed_tenant_id):
        def wrapped(table_name: str) -> str:
            # è®¾ç½® context å˜é‡
            _set_connection_context(fixed_connection_id, fixed_db_session, fixed_tenant_id)
            return get_schema(table_name, fixed_connection_id)
        wrapped.__name__ = "get_schema"
        wrapped.__doc__ = get_schema.__doc__
        return wrapped

    # åˆ›å»ºå¸¦é¢„ç»‘å®š connection_id çš„åŒ…è£…å‡½æ•°
    bound_execute_query = make_execute_query(connection_id, db_session, tenant_id)
    bound_list_tables = make_list_tables(connection_id, db_session, tenant_id)
    bound_get_schema = make_get_schema(connection_id, db_session, tenant_id)

    # åˆ›å»º StructuredTool å¯¹è±¡ - æ•°æ®åº“å·¥å…·
    tools = [
        StructuredTool.from_function(
            func=bound_execute_query,
            name="execute_query",
            description=(
                "âœ… Execute SQL to get ACTUAL DATA from tables (NOT table names). "
                ""
                "ğŸ”´ CRITICAL: You MUST call list_tables() FIRST to get the actual table names! "
                "Use the EXACT table names returned by list_tables() - do not guess or translate. "
                ""
                "Table Name Rules: "
                "- If list_tables() returns Chinese names (é”€å”®è®¢å•è¡¨), use Chinese "
                "- If list_tables() returns English names (orders), use English "
                "- Do NOT assume table names - always check with list_tables() first "
                ""
                "Examples of questions that need THIS tool (after calling list_tables): "
                "- 'æœ‰å“ªäº›åœ°åŒºï¼Ÿ' â†’ execute_query('SELECT * FROM åœ°åŒºè¡¨') "
                "- 'what users exist' â†’ execute_query('SELECT * FROM ç”¨æˆ·è¡¨ LIMIT 100') "
                ""
                "Returns results in JSON format with columns and rows. "
                "Args: query (str): The SQL SELECT query to execute"
            )
        ),
        StructuredTool.from_function(
            func=bound_list_tables,
            name="list_tables",
            description=(
                "ğŸ”´ CRITICAL: List all TABLE NAMES in the database (NOT the data within tables). "
                ""
                "ğŸ“‹ MANDATORY: You MUST call this tool BEFORE any execute_query call to get the correct table names! "
                ""
                "Usage Rules: "
                "- ALWAYS call list_tables() first when you need to query data "
                "- Use the EXACT table names returned by list_tables() in your SQL "
                "- Do NOT guess table names - they may be Chinese (é”€å”®è®¢å•è¡¨) or English (orders) "
                ""
                "Returns meta-information like ['é”€å”®è®¢å•è¡¨', 'ç”¨æˆ·è¡¨', 'orders'] (actual table names). "
                ""
                "Args: None"
            )
        ),
        StructuredTool.from_function(
            func=bound_get_schema,
            name="get_schema",
            description=(
                "Get the schema (column information) for a specific table or sheet. "
                "Args: table_name (str): The exact table/sheet name from list_tables()"
            )
        )
    ]

    # ğŸ”¥ æ–°å¢ï¼šè¯­ä¹‰å±‚æ–‡ä»¶ç³»ç»Ÿå·¥å…·ï¼ˆSchema FS Toolsï¼‰
    # è¿™äº›å·¥å…·ä¸éœ€è¦ connection_idï¼Œç›´æ¥è¯»å– cube_schema ç›®å½•
    tools.extend([
        StructuredTool.from_function(
            func=list_schema_files,
            name="list_schema_files",
            description=(
                "List all available semantic layer documents (YAML/MD files) in the cube_schema directory. "
                "Use this to answer questions like 'What tables are documented?' or 'What semantic layers are available?'. "
                "Returns a JSON array with filename, size, and modified time. "
                "Args: None"
            )
        ),
        StructuredTool.from_function(
            func=read_schema_file,
            name="read_schema_file",
            description=(
                "Read the content of a specific semantic layer document (YAML/MD file) from cube_schema. "
                "Use this to get detailed information about table structure, measures, dimensions, and business logic. "
                "Args: filename (str): The name of the file (e.g., 'Orders.yaml'); "
                "section (str, optional): Filter to specific section like 'measures', 'dimensions', 'sql_table'"
            )
        ),
        StructuredTool.from_function(
            func=search_schema,
            name="search_schema",
            description=(
                "Search for a keyword across all semantic layer documents in cube_schema. "
                "Use this to find which tables contain specific measures, dimensions, or business concepts. "
                "Args: keyword (str): The search keyword (e.g., 'revenue', 'customer_id')"
            )
        )
    ])

    logger.info(f"[get_database_tools] Created {len(tools)} tools with connection_id={connection_id}")
    return tools


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    import asyncio

    print("=" * 60)
    print("Database Tools æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯• 1: åˆ—å‡ºè¡¨
    print("\n[TEST 1] åˆ—å‡ºæ•°æ®åº“è¡¨")
    result = list_tables.invoke({})
    print(f"ç»“æœ: {result}")

    # æµ‹è¯• 2: è·å–è¡¨ç»“æ„
    print("\n[TEST 2] è·å–è¡¨ç»“æ„")
    result = get_schema.invoke({"table_name": "tenants"})
    print(f"ç»“æœ: {result}")

    # æµ‹è¯• 3: æ‰§è¡ŒæŸ¥è¯¢
    print("\n[TEST 3] æ‰§è¡ŒæŸ¥è¯¢")
    result = execute_query.invoke({"query": "SELECT * FROM tenants LIMIT 1"})
    print(f"ç»“æœ: {result}")
