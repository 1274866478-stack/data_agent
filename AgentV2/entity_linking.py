# -*- coding: utf-8 -*-
"""
å®ä½“é“¾æ¥æœåŠ¡ - åŸºäºå‘é‡æ£€ç´¢çš„æ™ºèƒ½å®ä½“åŒ¹é…

è¿™ä¸ªæ¨¡å—æä¾›åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„å®ä½“é“¾æ¥åŠŸèƒ½ï¼Œè§£å†³æ¨¡ç³ŠåŒ¹é…é—®é¢˜ï¼š
- äº§å“åæ¨¡ç³ŠåŒ¹é…: "P40" â†’ "Huawei P40 Pro"
- åŸå¸‚åˆ«åæ‰©å±•: "é­”éƒ½" â†’ "ä¸Šæµ·" (ä¸ä¸šåŠ¡æœ¯è¯­è¡¨ååŒ)
- ä¸šåŠ¡æŒ‡æ ‡ç¼©å†™: "GMV" â†’ "Gross Merchandise Volume"
- å±‚çº§å®ä½“åŒ¹é…: "iPhone" â†’ "Apple iPhone 15 Pro Max"

æ ¸å¿ƒä¼˜åŠ¿ï¼š
1. åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œè€Œéç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
2. æ”¯æŒå¤šè¯­è¨€ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
3. å¯æ‰©å±•çš„å®ä½“ç±»å‹
4. ä¸ ChromaDB é›†æˆï¼Œæ”¯æŒå¤§è§„æ¨¡å®ä½“åº“

ä½œè€…: Data Agent Team
ç‰ˆæœ¬: 2.0.0
"""

import json
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from functools import lru_cache

import numpy as np

logger = logging.getLogger(__name__)


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

class EntityType(Enum):
    """å®ä½“ç±»å‹æšä¸¾"""
    PRODUCT = "product"           # äº§å“/å•†å“
    CUSTOMER = "customer"         # å®¢æˆ·
    LOCATION = "location"         # åœ°ç†ä½ç½®
    CATEGORY = "category"         # åˆ†ç±»
    METRIC = "metric"             # ä¸šåŠ¡æŒ‡æ ‡
    DIMENSION = "dimension"       # ç»´åº¦
    ORGANIZATION = "organization" # ç»„ç»‡
    PERSON = "person"             # äººå‘˜
    CUSTOM = "custom"             # è‡ªå®šä¹‰


@dataclass
class Entity:
    """å®ä½“å®šä¹‰

    Attributes:
        id: å®ä½“å”¯ä¸€æ ‡è¯†
        name: å®ä½“æ ‡å‡†åç§°
        aliases: åˆ«ååˆ—è¡¨
        entity_type: å®ä½“ç±»å‹
        description: æè¿°
        metadata: é¢å¤–çš„å…ƒæ•°æ®
        embedding: é¢„è®¡ç®—çš„å‘é‡ï¼ˆå¯é€‰ï¼‰
        tenant_id: ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
        created_at: åˆ›å»ºæ—¶é—´
    """
    id: str
    name: str
    entity_type: EntityType
    aliases: List[str] = field(default_factory=list)
    description: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None
    tenant_id: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "name": self.name,
            "entity_type": self.entity_type.value,
            "aliases": self.aliases,
            "description": self.description,
            "metadata": self.metadata,
            "tenant_id": self.tenant_id,
            "created_at": self.created_at.isoformat()
        }

    def get_search_texts(self) -> List[str]:
        """è·å–ç”¨äºæœç´¢çš„æ–‡æœ¬åˆ—è¡¨"""
        texts = [self.name]
        texts.extend(self.aliases)
        if self.description:
            texts.append(self.description)
        return texts


@dataclass
class LinkingResult:
    """å®ä½“é“¾æ¥ç»“æœ

    Attributes:
        query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
        matched_entity: åŒ¹é…åˆ°çš„å®ä½“
        confidence: åŒ¹é…ç½®ä¿¡åº¦ (0-1)
        match_type: åŒ¹é…ç±»å‹ (exact/fuzzy/semantic)
        explanation: åŒ¹é…è§£é‡Š
    """
    query: str
    matched_entity: Optional[Entity]
    confidence: float
    match_type: str  # exact, fuzzy, semantic
    explanation: str

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query": self.query,
            "matched_entity": self.matched_entity.to_dict() if self.matched_entity else None,
            "confidence": self.confidence,
            "match_type": self.match_type,
            "explanation": self.explanation
        }


# ============================================================================
# å‘é‡åµŒå…¥æœåŠ¡æ¥å£
# ============================================================================

class EmbeddingService:
    """å‘é‡åµŒå…¥æœåŠ¡åŸºç±»

    æä¾›æ–‡æœ¬åˆ°å‘é‡çš„è½¬æ¢åŠŸèƒ½ã€‚æ”¯æŒå¤šç§åç«¯ï¼š
    - SentenceTransformers (æœ¬åœ°)
    - OpenAI Embeddings API
    - æ™ºè°± AI Embeddings
    - DeepSeek Embeddings
    """

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """åˆå§‹åŒ–åµŒå…¥æœåŠ¡

        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self._model = None

    def encode(self, texts: Union[str, List[str]]) -> Union[np.ndarray, List[np.ndarray]]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨

        Returns:
            å‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° encode æ–¹æ³•")

    def encode_single(self, text: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        result = self.encode(text)
        if isinstance(result, list):
            return result[0]
        return result

    @lru_cache(maxsize=1000)
    def get_cached_embedding(self, text: str) -> np.ndarray:
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        return self.encode_single(text)


class SentenceTransformerEmbedding(EmbeddingService):
    """åŸºäº SentenceTransformers çš„æœ¬åœ°åµŒå…¥æœåŠ¡"""

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """åˆå§‹åŒ–

        Args:
            model_name: SentenceTransformers æ¨¡å‹åç§°
                       æ¨èçš„å¤šè¯­è¨€æ¨¡å‹:
                       - paraphrase-multilingual-MiniLM-L12-v2 (å¿«é€Ÿ, å¤šè¯­è¨€)
                       - distiluse-base-multilingual-cased-v2 (ä¸­ç­‰, å¤šè¯­è¨€)
                       - paraphrase-multilingual-mpnet-base-v2 (é«˜ç²¾åº¦, å¤šè¯­è¨€)
        """
        super().__init__(model_name)
        self._load_model()

    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            self._model = SentenceTransformer(self.model_name)
            logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        except ImportError:
            logger.warning("sentence_transformers æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è¯é¢‘åµŒå…¥")
            self._model = None
        except Exception as e:
            logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            self._model = None

    def encode(self, texts: Union[str, List[str]]) -> Union[np.ndarray, List[np.ndarray]]:
        """ç¼–ç æ–‡æœ¬"""
        if self._model is None:
            # å›é€€åˆ°ç®€å•è¯é¢‘åµŒå…¥
            return self._simple_encode(texts)

        if isinstance(texts, str):
            texts = [texts]

        embeddings = self._model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=False
        )

        if len(texts) == 1:
            return embeddings[0]
        return embeddings

    def _simple_encode(self, texts: Union[str, List[str]]) -> Union[np.ndarray, List[np.ndarray]]:
        """ç®€å•çš„è¯é¢‘ç¼–ç ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        if isinstance(texts, str):
            texts = [texts]

        def char_to_vector(text: str, dim: int = 384) -> np.ndarray:
            """å­—ç¬¦çº§åˆ«çš„ç®€å•ç¼–ç """
            # ä½¿ç”¨å­—ç¬¦é¢‘ç‡å’Œä½ç½®ä¿¡æ¯
            vector = np.zeros(dim, dtype=np.float32)
            for i, char in enumerate(text[:dim]):
                # ç®€å•çš„å­—ç¬¦å“ˆå¸Œ
                idx = (ord(char) * (i + 1)) % dim
                vector[idx] += 1.0 / (i + 1)
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            return vector

        embeddings = [char_to_vector(text) for text in texts]

        if len(texts) == 1:
            return embeddings[0]
        return embeddings


# ============================================================================
# å®ä½“å­˜å‚¨åç«¯
# ============================================================================

class EntityStore:
    """å®ä½“å­˜å‚¨åç«¯åŸºç±»"""

    def add(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“"""
        raise NotImplementedError

    def add_batch(self, entities: List[Entity]) -> int:
        """æ‰¹é‡æ·»åŠ å®ä½“"""
        count = 0
        for entity in entities:
            if self.add(entity):
                count += 1
        return count

    def search(
        self,
        query: str,
        top_k: int = 5,
        entity_type: Optional[EntityType] = None,
        tenant_id: Optional[str] = None,
        min_score: float = 0.0
    ) -> List[Tuple[Entity, float]]:
        """æœç´¢å®ä½“

        Returns:
            (å®ä½“, ç›¸ä¼¼åº¦åˆ†æ•°) åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        raise NotImplementedError

    def get(self, entity_id: str) -> Optional[Entity]:
        """æ ¹æ®IDè·å–å®ä½“"""
        raise NotImplementedError

    def delete(self, entity_id: str) -> bool:
        """åˆ é™¤å®ä½“"""
        raise NotImplementedError

    def clear(self, tenant_id: Optional[str] = None):
        """æ¸…ç©ºå­˜å‚¨"""
        raise NotImplementedError

    def count(self, tenant_id: Optional[str] = None) -> int:
        """ç»Ÿè®¡å®ä½“æ•°é‡"""
        raise NotImplementedError


class InMemoryEntityStore(EntityStore):
    """å†…å­˜å®ä½“å­˜å‚¨ï¼ˆç”¨äºæµ‹è¯•å’Œå°è§„æ¨¡åœºæ™¯ï¼‰"""

    def __init__(self, embedding_service: EmbeddingService):
        """åˆå§‹åŒ–

        Args:
            embedding_service: å‘é‡åµŒå…¥æœåŠ¡
        """
        self.embedding_service = embedding_service
        self._entities: Dict[str, Entity] = {}
        self._embeddings: Dict[str, np.ndarray] = {}

    def add(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“"""
        if entity.id in self._entities:
            return False

        self._entities[entity.id] = entity

        # è®¡ç®—å¹¶ç¼“å­˜åµŒå…¥å‘é‡
        search_texts = " ".join(entity.get_search_texts())
        embedding = self.embedding_service.encode_single(search_texts)
        self._embeddings[entity.id] = embedding

        return True

    def search(
        self,
        query: str,
        top_k: int = 5,
        entity_type: Optional[EntityType] = None,
        tenant_id: Optional[str] = None,
        min_score: float = 0.0
    ) -> List[Tuple[Entity, float]]:
        """æœç´¢å®ä½“"""
        if not self._entities:
            return []

        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedding_service.encode_single(query)

        # è®¡ç®—ç›¸ä¼¼åº¦
        results = []
        for entity_id, entity in self._entities.items():
            # è¿‡æ»¤å®ä½“ç±»å‹
            if entity_type and entity.entity_type != entity_type:
                continue
            # è¿‡æ»¤ç§Ÿæˆ·
            if tenant_id and entity.tenant_id != tenant_id:
                continue

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            entity_embedding = self._embeddings.get(entity_id)
            if entity_embedding is not None:
                similarity = self._cosine_similarity(query_embedding, entity_embedding)
                if similarity >= min_score:
                    results.append((entity, similarity))

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def get(self, entity_id: str) -> Optional[Entity]:
        """æ ¹æ®IDè·å–å®ä½“"""
        return self._entities.get(entity_id)

    def delete(self, entity_id: str) -> bool:
        """åˆ é™¤å®ä½“"""
        if entity_id in self._entities:
            del self._entities[entity_id]
            self._embeddings.pop(entity_id, None)
            return True
        return False

    def clear(self, tenant_id: Optional[str] = None):
        """æ¸…ç©ºå­˜å‚¨"""
        if tenant_id:
            to_delete = [
                eid for eid, entity in self._entities.items()
                if entity.tenant_id == tenant_id
            ]
            for eid in to_delete:
                self.delete(eid)
        else:
            self._entities.clear()
            self._embeddings.clear()

    def count(self, tenant_id: Optional[str] = None) -> int:
        """ç»Ÿè®¡å®ä½“æ•°é‡"""
        if tenant_id:
            return sum(
                1 for entity in self._entities.values()
                if entity.tenant_id == tenant_id
            )
        return len(self._entities)

    @staticmethod
    def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(dot_product / (norm_a * norm_b))


class ChromaDBEntityStore(EntityStore):
    """åŸºäº ChromaDB çš„å®ä½“å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰"""

    def __init__(
        self,
        embedding_service: EmbeddingService,
        collection_name: str = "entity_linking",
        persist_directory: Optional[Path] = None
    ):
        """åˆå§‹åŒ–

        Args:
            embedding_service: å‘é‡åµŒå…¥æœåŠ¡
            collection_name: ChromaDB é›†åˆåç§°
            persist_directory: æŒä¹…åŒ–ç›®å½•
        """
        self.embedding_service = embedding_service
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self._client = None
        self._collection = None
        self._init_chromadb()

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB"""
        try:
            import chromadb

            if self.persist_directory:
                self.persist_directory.mkdir(parents=True, exist_ok=True)
                self._client = chromadb.PersistentClient(
                    path=str(self.persist_directory)
                )
            else:
                self._client = chromadb.Client()

            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )

            logger.info(f"ChromaDB é›†åˆåˆå§‹åŒ–å®Œæˆ: {self.collection_name}")

        except ImportError:
            logger.warning("chromadb æœªå®‰è£…ï¼Œå›é€€åˆ°å†…å­˜å­˜å‚¨")
            raise
        except Exception as e:
            logger.error(f"ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def add(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“"""
        # å‡†å¤‡æœç´¢æ–‡æœ¬
        search_texts = " ".join(entity.get_search_texts())

        # ç”ŸæˆåµŒå…¥
        embedding = self.embedding_service.encode_single(search_texts)

        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "name": entity.name,
            "entity_type": entity.entity_type.value,
            "description": entity.description or "",
            "aliases": json.dumps(entity.aliases, ensure_ascii=False),
            **entity.metadata
        }
        if entity.tenant_id:
            metadata["tenant_id"] = entity.tenant_id

        # æ·»åŠ åˆ° ChromaDB
        try:
            self._collection.add(
                ids=[entity.id],
                embeddings=[embedding.tolist()],
                metadatas=[metadata],
                documents=[search_texts]
            )
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ å®ä½“å¤±è´¥: {e}")
            return False

    def search(
        self,
        query: str,
        top_k: int = 5,
        entity_type: Optional[EntityType] = None,
        tenant_id: Optional[str] = None,
        min_score: float = 0.0
    ) -> List[Tuple[Entity, float]]:
        """æœç´¢å®ä½“"""
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedding_service.encode_single(query)

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if entity_type:
            where["entity_type"] = entity_type.value
        if tenant_id:
            where["tenant_id"] = tenant_id

        # æŸ¥è¯¢ ChromaDB
        try:
            results = self._collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k * 2,  # è·å–æ›´å¤šç»“æœä»¥æ”¯æŒè¿‡æ»¤
                where=where if where else None
            )

            if not results or not results["ids"][0]:
                return []

            # è½¬æ¢ç»“æœ
            entities_with_scores = []
            for i, entity_id in enumerate(results["ids"][0]):
                distance = 1 - results["distances"][0][i]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                if distance < min_score:
                    continue

                metadata = results["metadatas"][0][i]
                entity = Entity(
                    id=entity_id,
                    name=metadata.get("name", ""),
                    entity_type=EntityType(metadata.get("entity_type", "custom")),
                    aliases=json.loads(metadata.get("aliases", "[]")),
                    description=metadata.get("description", ""),
                    metadata={k: v for k, v in metadata.items()
                             if k not in ["name", "entity_type", "description", "aliases", "tenant_id"]},
                    tenant_id=metadata.get("tenant_id")
                )
                entities_with_scores.append((entity, distance))

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            entities_with_scores.sort(key=lambda x: x[1], reverse=True)
            return entities_with_scores[:top_k]

        except Exception as e:
            logger.error(f"æœç´¢å®ä½“å¤±è´¥: {e}")
            return []

    def get(self, entity_id: str) -> Optional[Entity]:
        """æ ¹æ®IDè·å–å®ä½“"""
        try:
            results = self._collection.get(ids=[entity_id])
            if not results or not results["ids"]:
                return None

            metadata = results["metadatas"][0]
            return Entity(
                id=entity_id,
                name=metadata.get("name", ""),
                entity_type=EntityType(metadata.get("entity_type", "custom")),
                aliases=json.loads(metadata.get("aliases", "[]")),
                description=metadata.get("description", ""),
                metadata={k: v for k, v in metadata.items()
                         if k not in ["name", "entity_type", "description", "aliases", "tenant_id"]},
                tenant_id=metadata.get("tenant_id")
            )
        except Exception as e:
            logger.error(f"è·å–å®ä½“å¤±è´¥: {e}")
            return None

    def delete(self, entity_id: str) -> bool:
        """åˆ é™¤å®ä½“"""
        try:
            self._collection.delete(ids=[entity_id])
            return True
        except Exception as e:
            logger.error(f"åˆ é™¤å®ä½“å¤±è´¥: {e}")
            return False

    def clear(self, tenant_id: Optional[str] = None):
        """æ¸…ç©ºå­˜å‚¨"""
        try:
            if tenant_id:
                # è·å–è¯¥ç§Ÿæˆ·çš„æ‰€æœ‰å®ä½“ID
                results = self._collection.get(where={"tenant_id": tenant_id})
                if results and results["ids"]:
                    self._collection.delete(ids=results["ids"])
            else:
                # åˆ é™¤å¹¶é‡å»ºé›†åˆ
                self._client.delete_collection(self.collection_name)
                self._collection = self._client.create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"}
                )
        except Exception as e:
            logger.error(f"æ¸…ç©ºå­˜å‚¨å¤±è´¥: {e}")

    def count(self, tenant_id: Optional[str] = None) -> int:
        """ç»Ÿè®¡å®ä½“æ•°é‡"""
        try:
            if tenant_id:
                results = self._collection.get(where={"tenant_id": tenant_id})
                return len(results["ids"]) if results else 0
            else:
                return self._collection.count()
        except Exception as e:
            logger.error(f"ç»Ÿè®¡å®ä½“æ•°é‡å¤±è´¥: {e}")
            return 0


# ============================================================================
# å®ä½“é“¾æ¥æœåŠ¡
# ============================================================================

class EntityLinkingService:
    """å®ä½“é“¾æ¥æœåŠ¡

    æä¾›æ™ºèƒ½å®ä½“åŒ¹é…åŠŸèƒ½ï¼š
    1. ç²¾ç¡®åŒ¹é… - åç§°æˆ–åˆ«åå®Œå…¨åŒ¹é…
    2. æ¨¡ç³ŠåŒ¹é… - åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦åŒ¹é…
    3. è¯­ä¹‰åŒ¹é… - åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰åŒ¹é…
    """

    def __init__(
        self,
        entity_store: Optional[EntityStore] = None,
        embedding_service: Optional[EmbeddingService] = None,
        enable_exact_match: bool = True,
        enable_fuzzy_match: bool = True,
        enable_semantic_match: bool = True,
        fuzzy_threshold: float = 0.6,
        semantic_threshold: float = 0.5
    ):
        """åˆå§‹åŒ–å®ä½“é“¾æ¥æœåŠ¡

        Args:
            entity_store: å®ä½“å­˜å‚¨åç«¯
            embedding_service: å‘é‡åµŒå…¥æœåŠ¡
            enable_exact_match: æ˜¯å¦å¯ç”¨ç²¾ç¡®åŒ¹é…
            enable_fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            enable_semantic_match: æ˜¯å¦å¯ç”¨è¯­ä¹‰åŒ¹é…
            fuzzy_threshold: æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
            semantic_threshold: è¯­ä¹‰åŒ¹é…é˜ˆå€¼
        """
        # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        self.embedding_service = embedding_service or SentenceTransformerEmbedding()

        # åˆå§‹åŒ–å­˜å‚¨åç«¯
        if entity_store is None:
            try:
                entity_store = ChromaDBEntityStore(
                    embedding_service=self.embedding_service,
                    persist_directory=Path("./data/entity_linking")
                )
            except Exception:
                entity_store = InMemoryEntityStore(
                    embedding_service=self.embedding_service
                )

        self.entity_store = entity_store

        # é…ç½®
        self.enable_exact_match = enable_exact_match
        self.enable_fuzzy_match = enable_fuzzy_match
        self.enable_semantic_match = enable_semantic_match
        self.fuzzy_threshold = fuzzy_threshold
        self.semantic_threshold = semantic_threshold

        # åŠ è½½å†…ç½®å®ä½“
        self._load_builtin_entities()

    def _load_builtin_entities(self):
        """åŠ è½½å†…ç½®å®ä½“"""
        builtin_entities = self._get_builtin_entities()
        count = self.entity_store.add_batch(builtin_entities)
        logger.info(f"åŠ è½½ {count} ä¸ªå†…ç½®å®ä½“")

    def _get_builtin_entities(self) -> List[Entity]:
        """è·å–å†…ç½®å®ä½“åˆ—è¡¨"""
        entities = []

        # äº§å“å®ä½“ç¤ºä¾‹
        products = [
            {
                "id": "prod_huawei_p40_pro",
                "name": "Huawei P40 Pro",
                "aliases": ["P40 Pro", "P40", "åä¸ºP40", "åä¸º P40 Pro", "åä¸ºP40 Pro"],
                "description": "åä¸ºæ——èˆ°æ™ºèƒ½æ‰‹æœºï¼Œæ­è½½éº’éºŸ990 5GèŠ¯ç‰‡",
                "metadata": {"brand": "Huawei", "category": "Smartphone"}
            },
            {
                "id": "prod_iphone_15_pro",
                "name": "Apple iPhone 15 Pro",
                "aliases": ["iPhone 15 Pro", "iPhone15 Pro", "è‹¹æœ 15 Pro", "è‹¹æœ15 Pro"],
                "description": "è‹¹æœæ——èˆ°æ™ºèƒ½æ‰‹æœºï¼Œæ­è½½A17 ProèŠ¯ç‰‡",
                "metadata": {"brand": "Apple", "category": "Smartphone"}
            },
            {
                "id": "prod_xiaomi_mi14",
                "name": "Xiaomi Mi 14",
                "aliases": ["å°ç±³14", "Mi 14", "å°ç±³Mi14", "å°ç±³ Mi 14"],
                "description": "å°ç±³æ——èˆ°æ™ºèƒ½æ‰‹æœºï¼Œæ­è½½éªé¾™8 Gen3èŠ¯ç‰‡",
                "metadata": {"brand": "Xiaomi", "category": "Smartphone"}
            },
        ]

        for p in products:
            entities.append(Entity(
                id=p["id"],
                name=p["name"],
                entity_type=EntityType.PRODUCT,
                aliases=p["aliases"],
                description=p.get("description", ""),
                metadata=p.get("metadata", {})
            ))

        # ä½ç½®å®ä½“ç¤ºä¾‹
        locations = [
            {
                "id": "loc_beijing",
                "name": "åŒ—äº¬",
                "aliases": ["Beijing", "Peking", "é¦–éƒ½", "å¸éƒ½"],
                "description": "ä¸­å›½é¦–éƒ½ï¼Œç›´è¾–å¸‚",
                "metadata": {"level": "municipality", "region": "North China"}
            },
            {
                "id": "loc_shanghai",
                "name": "ä¸Šæµ·",
                "aliases": ["Shanghai", "ç”³åŸ", "é­”éƒ½"],
                "description": "ä¸­å›½ç›´è¾–å¸‚ï¼Œç»æµä¸­å¿ƒ",
                "metadata": {"level": "municipality", "region": "East China"}
            },
            {
                "id": "loc_shenzhen",
                "name": "æ·±åœ³",
                "aliases": ["Shenzhen", "é¹åŸ"],
                "description": "ä¸­å›½å¹¿ä¸œçœå‰¯çœçº§å¸‚ï¼Œç§‘æŠ€ä¸­å¿ƒ",
                "metadata": {"level": "city", "province": "Guangdong", "region": "South China"}
            },
        ]

        for loc in locations:
            entities.append(Entity(
                id=loc["id"],
                name=loc["name"],
                entity_type=EntityType.LOCATION,
                aliases=loc["aliases"],
                description=loc.get("description", ""),
                metadata=loc.get("metadata", {})
            ))

        # ä¸šåŠ¡æŒ‡æ ‡å®ä½“
        metrics = [
            {
                "id": "metric_gmv",
                "name": "GMV",
                "aliases": ["Gross Merchandise Volume", "å•†å“äº¤æ˜“æ€»é¢", "æˆäº¤æ€»é¢"],
                "description": "ä¸€å®šæ—¶é—´æ®µå†…çš„æˆäº¤å•†å“é‡‘é¢æ€»å’Œ",
                "metadata": {"category": "financial"}
            },
            {
                "id": "metric_arpu",
                "name": "ARPU",
                "aliases": ["Average Revenue Per User", "æ¯ç”¨æˆ·å¹³å‡æ”¶å…¥"],
                "description": "å¹³å‡æ¯ç”¨æˆ·æ”¶å…¥",
                "metadata": {"category": "financial"}
            },
        ]

        for m in metrics:
            entities.append(Entity(
                id=m["id"],
                name=m["name"],
                entity_type=EntityType.METRIC,
                aliases=m["aliases"],
                description=m.get("description", ""),
                metadata=m.get("metadata", {})
            ))

        return entities

    def link(
        self,
        query: str,
        entity_type: Optional[EntityType] = None,
        tenant_id: Optional[str] = None,
        top_k: int = 3
    ) -> List[LinkingResult]:
        """æ‰§è¡Œå®ä½“é“¾æ¥

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            entity_type: é™åˆ¶å®ä½“ç±»å‹
            tenant_id: ç§Ÿæˆ·ID
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            é“¾æ¥ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç½®ä¿¡åº¦é™åºæ’åˆ—
        """
        results = []

        # 1. ç²¾ç¡®åŒ¹é…
        if self.enable_exact_match:
            exact_results = self._exact_match(query, entity_type, tenant_id)
            results.extend(exact_results)

        # 2. æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœç²¾ç¡®åŒ¹é…æœªæ‰¾åˆ°è¶³å¤Ÿç»“æœï¼‰
        if self.enable_fuzzy_match and len(results) < top_k:
            fuzzy_results = self._fuzzy_match(
                query,
                entity_type,
                tenant_id,
                min_score=self.fuzzy_threshold
            )
            # å»é‡
            for fr in fuzzy_results:
                if not any(r.matched_entity.id == fr.matched_entity.id for r in results):
                    results.append(fr)

        # 3. è¯­ä¹‰åŒ¹é…ï¼ˆå¦‚æœä»æœªæ‰¾åˆ°è¶³å¤Ÿç»“æœï¼‰
        if self.enable_semantic_match and len(results) < top_k:
            semantic_results = self._semantic_match(
                query,
                entity_type,
                tenant_id,
                top_k=top_k,
                min_score=self.semantic_threshold
            )
            # å»é‡
            for sr in semantic_results:
                if not any(r.matched_entity.id == sr.matched_entity.id for r in results):
                    results.append(sr)

        # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        results.sort(key=lambda x: x.confidence, reverse=True)
        return results[:top_k]

    def _exact_match(
        self,
        query: str,
        entity_type: Optional[EntityType],
        tenant_id: Optional[str]
    ) -> List[LinkingResult]:
        """ç²¾ç¡®åŒ¹é…"""
        results = []

        # è¿™é‡Œéœ€è¦éå†æ‰€æœ‰å®ä½“è¿›è¡Œç²¾ç¡®åŒ¹é…
        # å¯¹äºå¤§è§„æ¨¡åœºæ™¯ï¼Œåº”è¯¥å»ºç«‹åç§°ç´¢å¼•
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨è¯­ä¹‰æœç´¢è·å–å€™é€‰ï¼Œç„¶åéªŒè¯ç²¾ç¡®åŒ¹é…

        search_results = self.entity_store.search(
            query,
            top_k=10,
            entity_type=entity_type,
            tenant_id=tenant_id,
            min_score=0.8  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼ä½œä¸ºå€™é€‰
        )

        query_lower = query.lower().strip()

        for entity, similarity in search_results:
            # æ£€æŸ¥åç§°ç²¾ç¡®åŒ¹é…
            if entity.name.lower() == query_lower:
                results.append(LinkingResult(
                    query=query,
                    matched_entity=entity,
                    confidence=1.0,
                    match_type="exact",
                    explanation=f"åç§°å®Œå…¨åŒ¹é…: {entity.name}"
                ))
                continue

            # æ£€æŸ¥åˆ«åç²¾ç¡®åŒ¹é…
            for alias in entity.aliases:
                if alias.lower() == query_lower:
                    results.append(LinkingResult(
                        query=query,
                        matched_entity=entity,
                        confidence=0.95,
                        match_type="exact",
                        explanation=f"åˆ«å '{alias}' å®Œå…¨åŒ¹é…: {entity.name}"
                    ))
                    break

        return results

    def _fuzzy_match(
        self,
        query: str,
        entity_type: Optional[EntityType],
        tenant_id: Optional[str],
        min_score: float
    ) -> List[LinkingResult]:
        """æ¨¡ç³ŠåŒ¹é…"""
        results = []

        # è·å–å€™é€‰å®ä½“
        search_results = self.entity_store.search(
            query,
            top_k=20,
            entity_type=entity_type,
            tenant_id=tenant_id,
            min_score=min_score
        )

        for entity, similarity in search_results:
            # è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
            name_score = self._levenshtein_similarity(query, entity.name)
            max_alias_score = name_score
            best_alias = entity.name

            for alias in entity.aliases:
                alias_score = self._levenshtein_similarity(query, alias)
                if alias_score > max_alias_score:
                    max_alias_score = alias_score
                    best_alias = alias

            if max_alias_score >= min_score:
                # ç»¼åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
                combined_score = (similarity + max_alias_score) / 2
                results.append(LinkingResult(
                    query=query,
                    matched_entity=entity,
                    confidence=combined_score,
                    match_type="fuzzy",
                    explanation=f"æ¨¡ç³ŠåŒ¹é…: '{best_alias}' â†’ {entity.name} "
                              f"(ç›¸ä¼¼åº¦: {combined_score:.2f})"
                ))

        return results

    def _semantic_match(
        self,
        query: str,
        entity_type: Optional[EntityType],
        tenant_id: Optional[str],
        top_k: int,
        min_score: float
    ) -> List[LinkingResult]:
        """è¯­ä¹‰åŒ¹é…"""
        results = []

        search_results = self.entity_store.search(
            query,
            top_k=top_k,
            entity_type=entity_type,
            tenant_id=tenant_id,
            min_score=min_score
        )

        for entity, similarity in search_results:
            results.append(LinkingResult(
                query=query,
                matched_entity=entity,
                confidence=similarity,
                match_type="semantic",
                explanation=f"è¯­ä¹‰åŒ¹é…: {entity.name} (ç›¸ä¼¼åº¦: {similarity:.2f})"
            ))

        return results

    @staticmethod
    def _levenshtein_similarity(s1: str, s2: str) -> float:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦"""
        s1_lower = s1.lower()
        s2_lower = s2.lower()

        if s1_lower == s2_lower:
            return 1.0

        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—
        len1, len2 = len(s1_lower), len(s2_lower)
        max_len = max(len1, len2)

        if max_len == 0:
            return 0.0

        # ä½¿ç”¨åŒ…å«å…³ç³»å¿«é€Ÿåˆ¤æ–­
        if s1_lower in s2_lower or s2_lower in s1_lower:
            return min(len1, len2) / max_len

        # åŠ¨æ€è§„åˆ’è®¡ç®—ç¼–è¾‘è·ç¦»
        dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]

        for i in range(len1 + 1):
            dp[i][0] = i
        for j in range(len2 + 1):
            dp[0][j] = j

        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                if s1_lower[i - 1] == s2_lower[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1]
                else:
                    dp[i][j] = 1 + min(
                        dp[i - 1][j],
                        dp[i][j - 1],
                        dp[i - 1][j - 1]
                    )

        distance = dp[len1][len2]
        return 1.0 - (distance / max_len)

    def add_entity(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“"""
        return self.entity_store.add(entity)

    def add_entities(self, entities: List[Entity]) -> int:
        """æ‰¹é‡æ·»åŠ å®ä½“"""
        return self.entity_store.add_batch(entities)

    def add_entity_from_dict(self, entity_dict: Dict[str, Any]) -> bool:
        """ä»å­—å…¸æ·»åŠ å®ä½“"""
        try:
            entity = Entity(
                id=entity_dict["id"],
                name=entity_dict["name"],
                entity_type=EntityType(entity_dict.get("entity_type", "custom")),
                aliases=entity_dict.get("aliases", []),
                description=entity_dict.get("description", ""),
                metadata=entity_dict.get("metadata", {}),
                tenant_id=entity_dict.get("tenant_id")
            )
            return self.add_entity(entity)
        except Exception as e:
            logger.error(f"ä»å­—å…¸æ·»åŠ å®ä½“å¤±è´¥: {e}")
            return False

    def import_from_json(self, json_path: Union[str, Path], tenant_id: Optional[str] = None) -> int:
        """ä» JSON æ–‡ä»¶å¯¼å…¥å®ä½“

        JSON æ ¼å¼:
        {
            "entities": [
                {
                    "id": "unique_id",
                    "name": "Standard Name",
                    "entity_type": "product|location|...",
                    "aliases": ["alias1", "alias2"],
                    "description": "Description",
                    "metadata": {"key": "value"}
                }
            ]
        }
        """
        json_file = Path(json_path)
        if not json_file.exists():
            logger.warning(f"å®ä½“æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return 0

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            entities = []
            for entity_dict in data.get("entities", []):
                if tenant_id:
                    entity_dict["tenant_id"] = tenant_id
                entity = Entity(
                    id=entity_dict["id"],
                    name=entity_dict["name"],
                    entity_type=EntityType(entity_dict.get("entity_type", "custom")),
                    aliases=entity_dict.get("aliases", []),
                    description=entity_dict.get("description", ""),
                    metadata=entity_dict.get("metadata", {}),
                    tenant_id=entity_dict.get("tenant_id")
                )
                entities.append(entity)

            return self.add_entities(entities)

        except Exception as e:
            logger.error(f"å¯¼å…¥å®ä½“å¤±è´¥: {e}")
            return 0


# ============================================================================
# ä¸­é—´ä»¶é›†æˆ
# ============================================================================

class EntityLinkingMiddleware:
    """å®ä½“é“¾æ¥ä¸­é—´ä»¶

    åœ¨ Agent æ‰§è¡Œå‰æ‹¦æˆªæŸ¥è¯¢ï¼Œè¿›è¡Œå®ä½“é“¾æ¥ï¼Œ
    å¹¶å°†é“¾æ¥ç»“æœæ³¨å…¥åˆ° Agent ä¸Šä¸‹æ–‡ä¸­ã€‚
    """

    def __init__(
        self,
        linking_service: Optional[EntityLinkingService] = None,
        enable_auto_link: bool = True,
        injection_mode: str = "prompt"  # prompt, context, both
    ):
        """åˆå§‹åŒ–

        Args:
            linking_service: å®ä½“é“¾æ¥æœåŠ¡
            enable_auto_link: æ˜¯å¦å¯ç”¨è‡ªåŠ¨é“¾æ¥
            injection_mode: æ³¨å…¥æ¨¡å¼
                - prompt: æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯
                - context: æ³¨å…¥åˆ°ç”¨æˆ·ä¸Šä¸‹æ–‡
                - both: åŒæ—¶æ³¨å…¥
        """
        self.linking_service = linking_service or EntityLinkingService()
        self.enable_auto_link = enable_auto_link
        self.injection_mode = injection_mode

    def before_agent_execution(
        self,
        agent_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åœ¨ Agent æ‰§è¡Œå‰å¤„ç†

        Args:
            agent_input: Agent è¾“å…¥

        Returns:
            å¢å¼ºåçš„ Agent è¾“å…¥
        """
        if not self.enable_auto_link:
            return agent_input

        query = agent_input.get("query", "")
        if not query:
            return agent_input

        tenant_id = agent_input.get("tenant_id")

        # æ‰§è¡Œå®ä½“é“¾æ¥
        linking_results = self.linking_service.link(query, tenant_id=tenant_id)

        if not linking_results:
            return agent_input

        # ç”Ÿæˆé“¾æ¥æ–‡æœ¬
        injection_text = self._generate_injection_text(linking_results)

        # æ³¨å…¥åˆ° Agent è¾“å…¥
        if self.injection_mode in ("prompt", "both"):
            agent_input["__entity_linking_prompt__"] = injection_text

        if self.injection_mode in ("context", "both"):
            agent_input["__entity_linking_context__"] = {
                "linked_entities": [r.to_dict() for r in linking_results],
                "query": query
            }

        agent_input["__linked_entities__"] = linking_results

        return agent_input

    def _generate_injection_text(self, results: List[LinkingResult]) -> str:
        """ç”Ÿæˆæ³¨å…¥æ–‡æœ¬"""
        if not results:
            return ""

        lines = [
            "## å®ä½“é“¾æ¥ç»“æœ",
            f"æ£€æµ‹åˆ° {len(results)} ä¸ªç›¸å…³å®ä½“ï¼š",
            ""
        ]

        for i, result in enumerate(results, 1):
            entity = result.matched_entity
            lines.append(f"{i}. **{entity.name}** (ç½®ä¿¡åº¦: {result.confidence:.1%})")
            if entity.description:
                lines.append(f"   - æè¿°: {entity.description}")
            if entity.aliases:
                lines.append(f"   - åˆ«å: {', '.join(entity.aliases[:5])}")
            if entity.metadata:
                lines.append(f"   - å…ƒæ•°æ®: {json.dumps(entity.metadata, ensure_ascii=False)}")
            lines.append("")

        return "\n".join(lines)

    def enhance_system_prompt(self, base_prompt: str, query: str, tenant_id: Optional[str] = None) -> str:
        """å¢å¼ºç³»ç»Ÿæç¤ºè¯"""
        if not self.enable_auto_link:
            return base_prompt

        linking_results = self.linking_service.link(query, tenant_id=tenant_id)

        if not linking_results:
            return base_prompt

        injection_text = self._generate_injection_text(linking_results)

        return f"""{base_prompt}

{injection_text}

ğŸ’¡ æç¤ºï¼šç”¨æˆ·æŸ¥è¯¢ä¸­çš„å®ä½“å¯èƒ½æŒ‡å‘ä¸Šé¢çš„æ ‡å‡†åç§°ï¼Œè¯·ä½¿ç”¨æ ‡å‡†åç§°è¿›è¡ŒæŸ¥è¯¢å’Œå±•ç¤ºã€‚
"""


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def link_entities(
    query: str,
    entity_type: Optional[str] = None,
    tenant_id: Optional[str] = None,
    top_k: int = 3
) -> str:
    """é“¾æ¥å®ä½“ - ä¾› LLM è°ƒç”¨

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        entity_type: é™åˆ¶å®ä½“ç±»å‹
        tenant_id: ç§Ÿæˆ·ID
        top_k: è¿”å›ç»“æœæ•°é‡

    Returns:
        JSON æ ¼å¼çš„é“¾æ¥ç»“æœ
    """
    service = EntityLinkingService()

    et = EntityType(entity_type) if entity_type else None
    results = service.link(query, entity_type=et, tenant_id=tenant_id, top_k=top_k)

    return json.dumps({
        "query": query,
        "results": [r.to_dict() for r in results],
        "count": len(results)
    }, ensure_ascii=False, indent=2)


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    import sys

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("=" * 60)
    print("å®ä½“é“¾æ¥æœåŠ¡æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæœåŠ¡
    service = EntityLinkingService()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "P40",
        "åä¸º P40 Pro",
        "é­”éƒ½",
        "iPhone",
        "å°ç±³æ‰‹æœº",
        "GMV æ˜¯å¤šå°‘",
    ]

    for query in test_queries:
        print(f"\n[æµ‹è¯•] æŸ¥è¯¢: {query}")

        results = service.link(query, top_k=3)

        if results:
            print(f"  æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ:")
            for i, result in enumerate(results, 1):
                entity = result.matched_entity
                print(f"    {i}. {entity.name}")
                print(f"       ç±»å‹: {result.match_type}, ç½®ä¿¡åº¦: {result.confidence:.2f}")
                print(f"       è¯´æ˜: {result.explanation}")
        else:
            print("  æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)
