# Change: 启用AI助手流式响应

## Why
当前AI助手的回答需要等待完整生成后才显示，用户体验不够流畅。虽然后端已经支持流式响应，但前端尚未使用。实现流式响应可以让用户实时看到AI生成的内容，提升交互体验和响应感知，减少等待焦虑。

## What Changes
- 修改前端聊天API客户端，支持流式调用 `/llm/chat/completions` 端点，并支持 `AbortSignal` 中断控制
- 更新 `chatStore`，实现流式响应的实时更新逻辑，支持中断控制和状态管理
- 修改聊天UI组件，支持实时显示流式内容，包括SQL代码和图表配置的流式渲染
- 增强Markdown渲染器，处理流式传输中不完整的语法，防止布局跳动
- 实现流式JSON解析器，支持不完整JSON的渐进式渲染
- 添加"停止生成"按钮，允许用户中途中断流式响应
- 优化缓存策略，仅在流结束后保存完整消息
- 保持向后兼容，支持非流式模式作为降级方案
- 确保流式响应与现有功能（SQL执行、图表生成等）兼容

## Impact
- 受影响的功能：AI助手聊天界面、聊天状态管理
- 受影响的代码：
  - `frontend/src/lib/api-client.ts` - 添加流式API调用方法
  - `frontend/src/store/chatStore.ts` - 实现流式响应处理逻辑
  - `frontend/src/components/chat/` - 更新UI组件支持实时显示
  - `frontend/src/app/(app)/ai-assistant/page.tsx` - 集成流式响应
- 用户体验：显著提升，响应更及时，减少等待时间感知；支持中断控制，避免死循环或错误SQL导致的长时间等待
- 性能：可能略微增加服务器资源使用（保持连接），但用户体验提升明显；需要监控长时间流式对话的内存占用
- 兼容性：保持向后兼容，支持降级到非流式模式
- 技术挑战：需要处理流式传输中的不完整Markdown和JSON，确保渲染稳定性

