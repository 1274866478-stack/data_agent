# AI助手编造数据问题诊断测试计划

## 问题描述

AI助手在回答用户问题时，经常编造数据、产生幻觉，没有真正根据数据库来回答问题。需要系统性地诊断问题根源。

## 可能的原因分析

### 1. 工具调用失败但AI仍然生成答案
- **症状**：工具调用失败或返回空数据，但AI仍然生成看似合理的答案
- **可能位置**：
  - `backend/src/app/services/agent/agent_service.py` - 工具调用结果处理
  - `backend/src/app/services/agent/prompts.py` - System Prompt中的错误处理规则

### 2. AI没有真正执行SQL查询
- **症状**：AI只生成了SQL代码但没有通过工具执行
- **可能位置**：
  - `backend/src/app/services/agent/agent_service.py` - SQL执行逻辑
  - `backend/src/app/services/agent/tools.py` - SQL工具封装

### 3. 工具返回的数据没有被正确使用
- **症状**：工具返回了真实数据，但AI忽略了这些数据，使用训练数据生成答案
- **可能位置**：
  - `backend/src/app/services/agent/agent_service.py` - 消息处理和响应生成
  - LLM的上下文传递机制

### 4. Prompt不够强制
- **症状**：虽然prompt中有警告，但AI仍然违反规则
- **可能位置**：
  - `backend/src/app/services/agent/prompts.py` - System Prompt内容

### 5. 日志和验证机制不足
- **症状**：无法验证AI是否真正使用了数据库数据
- **可能位置**：
  - 所有相关文件 - 缺少详细的验证日志

## 测试计划

### 阶段1：基础验证测试

#### 测试1.1：验证数据库连接和工具可用性
**目标**：确认数据库连接正常，工具可以正常调用

**步骤**：
1. 检查数据库连接字符串是否正确
2. 手动调用 `list_tables` 工具，验证返回结果
3. 手动调用 `get_schema` 工具，验证返回结果
4. 手动调用 `query_database` 工具执行简单SQL，验证返回结果

**验证点**：
- [ ] 数据库连接成功
- [ ] `list_tables` 返回表列表
- [ ] `get_schema` 返回表结构
- [ ] `query_database` 返回查询结果

**日志要求**：
- 记录每个工具调用的输入和输出
- 记录工具调用的耗时
- 记录任何错误信息

#### 测试1.2：验证Agent初始化
**目标**：确认Agent可以正确初始化并加载工具

**步骤**：
1. 检查Agent是否成功初始化
2. 检查工具是否正确加载
3. 检查System Prompt是否正确注入

**验证点**：
- [ ] Agent初始化成功
- [ ] 所有必需工具已加载
- [ ] System Prompt包含防编造数据的规则

**日志要求**：
- 记录Agent初始化过程
- 记录加载的工具列表
- 记录System Prompt的关键部分

### 阶段2：工具调用流程测试

#### 测试2.1：简单查询测试
**目标**：验证AI是否会调用工具执行简单查询

**测试问题**：
- "数据库中有哪些表？"
- "用户表的结构是什么？"
- "查询前5条用户记录"

**验证点**：
- [ ] AI调用了 `list_tables` 工具（对于第一个问题）
- [ ] AI调用了 `get_schema` 工具（对于第二个问题）
- [ ] AI调用了 `query_database` 工具（对于第三个问题）
- [ ] AI的回答基于工具返回的真实数据

**日志要求**：
- 记录每个工具调用的时间戳
- 记录工具调用的参数
- 记录工具返回的结果
- 记录AI的最终回答
- **关键**：对比工具返回的数据和AI回答中的数据，检查是否一致

#### 测试2.2：工具调用失败场景测试
**目标**：验证当工具调用失败时，AI是否会编造数据

**测试场景**：
1. 模拟 `list_tables` 工具调用失败
2. 模拟 `query_database` 工具调用失败（返回错误）
3. 模拟 `query_database` 工具返回空结果

**测试问题**：
- "查询所有用户信息"（在工具失败的情况下）

**验证点**：
- [ ] AI是否明确告知工具调用失败
- [ ] AI是否编造了数据来替代失败的工具调用结果
- [ ] AI的回答是否包含错误信息

**日志要求**：
- 记录工具调用失败的错误信息
- 记录AI的完整回答
- **关键**：检查AI回答中是否包含编造的数据

#### 测试2.3：数据一致性验证测试
**目标**：验证AI回答中的数据是否与工具返回的数据一致

**测试问题**：
- "查询用户表中所有用户的姓名和邮箱"
- "统计每个部门的员工数量"
- "查询销售额最高的前10个产品"

**验证步骤**：
1. 手动执行相同的SQL查询，获取真实数据
2. 通过AI助手执行相同的查询
3. 对比AI回答中的数据与真实数据

**验证点**：
- [ ] AI回答中的数据与真实数据完全一致
- [ ] AI没有使用示例数据（如John Doe, Jane Smith等）
- [ ] AI没有修改或替换真实数据

**日志要求**：
- 记录工具返回的原始数据（JSON格式）
- 记录AI回答中的具体数据值
- **关键**：创建数据对比报告，标记不一致的地方

### 阶段3：Prompt和响应生成测试

#### 测试3.1：Prompt有效性测试
**目标**：验证System Prompt中的防编造数据规则是否有效

**测试方法**：
1. 检查当前System Prompt内容
2. 测试不同强度的Prompt版本
3. 对比AI的行为差异

**验证点**：
- [ ] Prompt中是否包含明确的"禁止编造数据"规则
- [ ] Prompt中是否包含"必须使用工具返回的数据"规则
- [ ] Prompt中是否包含"工具失败时必须明确告知"规则

**日志要求**：
- 记录使用的Prompt版本
- 记录AI的行为表现

#### 测试3.2：响应生成流程测试
**目标**：验证AI生成回答时是否正确使用了工具返回的数据

**测试方法**：
1. 在Agent执行过程中，记录所有消息
2. 分析消息流，检查工具返回的数据是否被正确传递
3. 检查AI的最终回答是否引用了工具返回的数据

**验证点**：
- [ ] 工具返回的数据被正确传递到LLM
- [ ] AI的最终回答引用了工具返回的数据
- [ ] AI没有忽略工具返回的数据

**日志要求**：
- 记录完整的消息流（包括所有HumanMessage、AIMessage、ToolMessage）
- 记录每个消息的内容
- **关键**：标记工具返回的数据在消息流中的位置

### 阶段4：端到端集成测试

#### 测试4.1：完整查询流程测试
**目标**：验证从用户提问到AI回答的完整流程

**测试问题**（使用真实数据库）：
1. "数据库中有哪些表？"
2. "用户表的结构是什么？"
3. "查询前10条用户记录"
4. "统计每个部门的员工数量"
5. "查询销售额最高的前5个产品"

**验证点**：
- [ ] 每个问题都触发了正确的工具调用
- [ ] 工具调用成功并返回数据
- [ ] AI的回答基于真实数据
- [ ] AI的回答中没有编造的数据

**日志要求**：
- 记录完整的请求-响应流程
- 记录所有工具调用
- 记录AI的最终回答
- **关键**：创建详细的执行报告，包括每个步骤的输入输出

#### 测试4.2：边界情况测试
**目标**：测试各种边界情况下的AI行为

**测试场景**：
1. 数据库为空（没有表）
2. 查询结果为空（没有匹配的记录）
3. 查询结果包含特殊字符（中文、emoji等）
4. 查询结果包含大量数据（超过1000条）
5. 数据库连接超时

**验证点**：
- [ ] AI正确处理空结果
- [ ] AI正确处理特殊字符
- [ ] AI正确处理大量数据
- [ ] AI正确处理连接错误

**日志要求**：
- 记录每个边界情况的处理过程
- 记录AI的回答
- **关键**：检查AI是否在边界情况下编造数据

### 阶段5：日志分析和诊断

#### 测试5.1：日志收集和分析
**目标**：收集所有相关日志，分析问题模式

**收集的日志**：
1. Agent初始化日志
2. 工具调用日志（输入、输出、耗时）
3. LLM请求日志（输入、输出）
4. 消息流日志（所有消息）
5. 最终响应日志

**分析内容**：
- 工具调用成功率
- 工具调用失败的原因
- AI回答与工具返回数据的一致性
- 编造数据的模式（何时、何种情况下）

**输出**：
- 诊断报告，包含问题根源分析
- 数据一致性对比报告
- 工具调用统计报告

#### 测试5.2：问题根源定位
**目标**：根据测试结果，定位问题根源

**分析方法**：
1. 对比正常情况和异常情况
2. 分析日志中的关键事件
3. 检查代码中的关键逻辑

**可能的问题根源**：
- [ ] 工具调用失败但错误处理不当
- [ ] LLM没有正确理解工具返回的数据
- [ ] Prompt不够强制，无法阻止AI编造数据
- [ ] 消息传递机制有问题，工具返回的数据没有正确传递
- [ ] LLM模型本身的问题（倾向于生成看似合理的答案）

## 测试工具和脚本

### 需要创建的测试脚本

1. **工具调用测试脚本** (`scripts/test_tool_calls.py`)
   - 测试每个工具的独立调用
   - 验证工具返回的数据格式

2. **Agent执行测试脚本** (`scripts/test_agent_execution.py`)
   - 测试Agent的完整执行流程
   - 记录所有消息和工具调用

3. **数据一致性验证脚本** (`scripts/validate_data_consistency.py`)
   - 对比工具返回的数据和AI回答中的数据
   - 生成一致性报告

4. **日志分析脚本** (`scripts/analyze_logs.py`)
   - 分析收集的日志
   - 生成诊断报告

## 测试数据准备

### 测试数据库

1. **标准测试数据库**
   - 包含多个表（用户表、订单表、产品表等）
   - 包含各种数据类型（文本、数字、日期等）
   - 包含中文数据

2. **边界测试数据**
   - 空表
   - 包含特殊字符的数据
   - 大量数据（1000+条记录）

### 测试问题集

创建标准测试问题集，包括：
- 简单查询问题
- 复杂查询问题
- 统计查询问题
- 可视化查询问题
- 边界情况问题

## 预期输出

### 测试报告

每个测试阶段完成后，生成测试报告，包含：
1. 测试结果摘要
2. 发现的问题
3. 数据一致性分析
4. 工具调用统计
5. 建议的修复方案

### 诊断报告

最终生成综合诊断报告，包含：
1. 问题根源分析
2. 问题影响范围
3. 修复优先级
4. 修复方案建议

## 下一步行动

根据测试结果，制定修复计划：
1. 如果是工具调用问题 → 修复工具调用逻辑
2. 如果是Prompt问题 → 优化System Prompt
3. 如果是消息传递问题 → 修复消息传递机制
4. 如果是LLM模型问题 → 考虑更换模型或调整参数

## 测试执行时间表

- **阶段1**：1-2天（基础验证）
- **阶段2**：2-3天（工具调用流程）
- **阶段3**：1-2天（Prompt和响应生成）
- **阶段4**：2-3天（端到端测试）
- **阶段5**：1-2天（日志分析和诊断）

**总计**：7-12天

## 注意事项

1. **测试环境隔离**：使用独立的测试数据库，避免影响生产数据
2. **日志详细程度**：确保日志足够详细，能够追踪问题
3. **数据对比**：手动验证AI回答中的数据，确保准确性
4. **多次测试**：每个测试场景执行多次，确保结果可重现
5. **问题记录**：详细记录每个发现的问题，包括复现步骤

